{"/dev/os/windows/1_windows10/": {
    "title": "1. Windows 10",
    "keywords": "windows",
    "url": "/dev/os/windows/1_windows10/",
    "body": "Windows 10 is the latest operating system developed by Microsoft, succeeding Windows 8. It combines the familiar features of Windows 7 with the modern design elements of Windows 8, offering a more cohesive and user-friendly experience. Windows 10 introduces a range of enhancements, including the return of the Start Menu, integration of virtual assistant Cortana, improved security features such as Windows Defender, enhanced multitasking capabilities with virtual desktops, and compatibility with a wide range of devices including PCs, tablets, and smartphones. It also provides regular updates and support, ensuring users have access to the latest features and security patches. Overall, Windows 10 aims to provide a seamless and intuitive computing experience for both consumers and enterprise users. 1. Key Highlights 1. Start Menu Revival: Windows 10 brings back the Start Menu with a blend of the classic Windows 7 layout and Windows 8’s Live Tiles. This provides users with a familiar yet modern interface. 2. Cortana Integration: Microsoft’s virtual assistant, Cortana, is integrated into Windows 10, offering voice commands, reminders, and search functionalities to enhance productivity. 3. Microsoft Edge Browser: Windows 10 introduces Edge, a modern web browser designed to replace Internet Explorer, featuring enhanced performance, security, and compatibility with modern web standards. 4. Virtual Desktops: Users can create multiple virtual desktops to manage and organize open applications, improving workflow and multitasking capabilities. 5. Action Center: This feature provides a centralized location for notifications and quick access to settings, improving user awareness and control over their system. 6. Universal Apps: Windows 10 supports universal apps that run across all devices in the Windows ecosystem, including PCs, tablets, and smartphones, ensuring a consistent user experience. 7. Continuum Mode: This feature allows devices to switch between desktop and tablet modes seamlessly, optimizing the interface based on the type of device and its usage. 8. Enhanced Security: Windows 10 includes Windows Hello, a biometric authentication system, and improved security features such as Device Guard and Secure Boot to protect against modern threats. 9. DirectX 12: For gamers, Windows 10 includes DirectX 12, which offers improved performance and graphics capabilities, making it a significant upgrade for gaming. 10. Windows Update for Business: This feature provides IT administrators with more control over the deployment of updates, allowing for phased rollouts and scheduled maintenance. 2. Technical Aspects Windows 10 offers a range of features and improvements that enhance user experience, security, and productivity. For IT professionals, understanding the deployment, security, and management capabilities of Windows 10 is crucial for effectively supporting and optimizing enterprise environments. By leveraging these technical aspects, IT teams can ensure a smooth transition to Windows 10 and maintain robust, secure, and efficient IT infrastructure. 1. Deployment and Migration: Windows Imaging and Configuration Designer (ICD): Used to create and deploy custom images of Windows 10. Windows Autopilot: A set of technologies to simplify and automate device setup and configuration. 2. Group Policy Management: Advanced Group Policy settings: Manage security, applications, and user settings. Administrative Templates: Configure various settings for Windows components. 3. Windows Update Management: Windows Update for Business: Control update deployment, defer feature updates, and manage update rollouts. WSUS (Windows Server Update Services): Centralized update management for enterprise environments. 4. Security Features: Windows Defender Advanced Threat Protection (ATP): Provides robust threat detection and response capabilities. BitLocker: Full disk encryption to protect data on lost or stolen devices. Credential Guard: Uses virtualization-based security to protect credential information. 5. Networking: DirectAccess and VPN: Simplified remote access solutions. Wi-Fi Sense: Facilitates easier connection to Wi-Fi networks but requires careful configuration to avoid security risks. 6. Device Management: Microsoft Intune: Cloud-based service for managing devices and applications. Mobile Device Management (MDM): Manage a range of devices from a unified interface. 7. Hyper-V: Virtualization Platform: Allows running virtual machines on Windows 10. Containers: Support for Windows containers and Docker for modern application deployment. 8. PowerShell: Automation: Use PowerShell scripts to automate administrative tasks. Remoting: Manage remote systems through PowerShell, providing powerful management capabilities. 9. Windows Subsystem for Linux (WSL): Run Linux Distros: Allows IT professionals to run native Linux command-line tools directly on Windows. Development Environment: Provides a seamless development environment for cross-platform applications. 10. Compatibility and Legacy Support: - Application Compatibility Toolkit (ACT): Helps to identify and mitigate application compatibility issues. - Enterprise Mode: Allows legacy web apps to run in Internet Explorer mode within Edge. 3. Sharing Internet This guide demonstrates how to share an internet connection from one computer to another using an Ethernet or LAN cable. It covers two methods using built-in features in Windows. 3.1 Method 1: Using Bridge Connections 1. Connect Host PC to Wi-Fi: Ensure the host computer is connected to a Wi-Fi network. 2. Open Network Connections: Access the network connections page on the host PC. 3. Setup Network Bridge: Select both the Wi-Fi and Ethernet network adapters. Right-click on the Wi-Fi adapter and select “Bridge Connections”. Right-click the newly created bridge adapter and select “Properties”. Ensure both Ethernet and Wi-Fi options are checked and click “OK”. 4. Connect Client PC: Use an Ethernet or LAN cable to connect the host PC to the client PC. 5. Verify Connection: The client PC should show an Ethernet icon in the taskbar, indicating a successful connection to the internet. 3.2 Method 2: Using Internet Connection Sharing (ICS) 1. Disconnect Network Bridge: If previously set, remove the network adapters from the bridge connection. 2. Connect Host and Client PCs: Ensure the host and client computers are connected via an Ethernet cable. 3. Configure Wi-Fi Adapter on Host PC: Right-click the Wi-Fi adapter and select “Properties”. Navigate to the “Sharing” tab. Check the box next to “Allow other network users to connect through this computer’s internet connection”. Under “Home networking connection”, select “Ethernet”. 4. Set Static IP Addresses: Due to potential issues with DHCP, it’s advisable to set private static IP addresses. On the host computer, right-click the Ethernet adapter and assign static IP addresses. Assign corresponding static IP addresses on the client computer. 5. Verify Internet Access: The client device should now have access to the internet. Software Purchase Grey Market https://consogame.com/software/windows/microsoft-windows-10-professional https://www.g2a.com/ https://www.scdkey.com/ https://www.cdkeys.com/ https://www.vip-scdkey.com/key/p202103081052146250.html https://www.bobkeys.com/software/p201609081840431227.html"
  },"/dev/os/windows/2_windows_server2019/": {
    "title": "2. Windows Server 2019",
    "keywords": "windows",
    "url": "/dev/os/windows/2_windows_server2019/",
    "body": "Content is Coming Soon… 1. Active Directory Active Directory (AD) is a directory service developed by Microsoft for Windows domain networks. It is included in most Windows Server operating systems. Active Directory stores information about objects on a network and makes this information available to users and network administrators. Here are the key components and functions of Active Directory: Directory Services: Active Directory stores information about users, computers, groups, and other resources on the network in a hierarchical, centralized database called the Active Directory Domain Services (AD DS) database. Domain Controller: A domain controller (DC) is a server that runs the Active Directory Domain Services role. It authenticates and authorizes all users and computers in a Windows domain, assigning and enforcing security policies for all computers and installing or updating software. Domains and Domain Trees: Active Directory organizes objects in a hierarchical structure called a domain. Domains can be grouped together to form domain trees, which can further be grouped into forests. Each domain in a forest maintains a two-way transitive trust relationship with every other domain in the forest, allowing users in one domain to access resources in another. User Authentication and Authorization: Active Directory authenticates and authorizes users and computers when they log in to the network. It verifies the identity of users and ensures they have the necessary permissions to access network resources. Group Policy: Active Directory uses Group Policy to define and enforce security settings and configurations for users and computers. Group Policy allows administrators to centrally manage and configure settings for all computers and users in a domain. LDAP Directory Services: Active Directory is based on the Lightweight Directory Access Protocol (LDAP), which provides a standardized way to access directory services. This allows applications and services to query and update information stored in Active Directory. DNS Integration: Active Directory relies heavily on Domain Name System (DNS) for name resolution. DNS is used to locate domain controllers, locate other services, and resolve domain names to IP addresses. Overall, Active Directory plays a critical role in managing and securing Windows-based networks, providing centralized authentication, authorization, and directory services to users and administrators. 2. Group Policy"
  },"/dev/os/windows/3.1_mingw/": {
    "title": "3.1. MinGW",
    "keywords": "windows",
    "url": "/dev/os/windows/3.1_mingw/",
    "body": "Content is Coming Soon…"
  },"/dev/os/windows/3.2_cygwin/": {
    "title": "3.2. Cygwin",
    "keywords": "windows",
    "url": "/dev/os/windows/3.2_cygwin/",
    "body": "Content is Coming Soon…"
  },"/dev/os/windows/3_wsl/": {
    "title": "3. Linux for Windows (WSL)",
    "keywords": "windows",
    "url": "/dev/os/windows/3_wsl/",
    "body": "Content is Coming Soon…"
  },"/dev/os/linux/1_ubuntu/": {
    "title": "1. Ubuntu",
    "keywords": "linux",
    "url": "/dev/os/linux/1_ubuntu/",
    "body": "Content is Coming Soon…"
  },"/dev/os/linux/2_kali_linux/": {
    "title": "2. Kali Linux",
    "keywords": "linux",
    "url": "/dev/os/linux/2_kali_linux/",
    "body": "Content is Coming Soon…"
  },"/dev/os/vmmachine/1_vmworkstation/": {
    "title": "1. VM Workstation",
    "keywords": "vmmachine",
    "url": "/dev/os/vmmachine/1_vmworkstation/",
    "body": "Content is Coming Soon…"
  },"/dev/os/vmmachine/2_windows_hyper_v/": {
    "title": "2. Windows Hyper-V",
    "keywords": "vmmachine",
    "url": "/dev/os/vmmachine/2_windows_hyper_v/",
    "body": "1 VMware overview VMware is a company that offers a range of virtualization products, such as VMware Workstation, VMware Fusion, VMware Server, and VMware ESXi. VMware Workstation and Fusion are desktop applications that let you create and run virtual machines on your Windows or Mac OS. VMware Server and ESXi are server applications that let you host multiple virtual machines on a single physical server. VMware is known for its reliability, performance, and compatibility with various operating systems and hardware. 2 Hyper-V overview Hyper-V is a virtualization feature that is built into Windows Server and Windows 10. Hyper-V lets you create and manage virtual machines using a graphical interface or a command-line tool. Hyper-V is designed to integrate well with other Windows features, such as Active Directory, PowerShell, and Remote Desktop. Hyper-V is also compatible with Linux and other operating systems, but may require some additional configuration. 3 Pros of VMware One of the main advantages of VMware is its wide support for different operating systems, including Windows, Linux, Mac OS, Solaris, FreeBSD, and more. VMware also has a large and active community of users and developers, who provide helpful resources, documentation, and support. VMware also offers advanced features, such as snapshots, cloning, live migration, and high availability, that can enhance your virtualization experience. VMware is also considered to be more stable and secure than Hyper-V, as it has fewer bugs and vulnerabilities. 4 Cons of VMware One of the main drawbacks of VMware is its cost. VMware products are not free, and you may need to purchase licenses, subscriptions, or support plans to use them. VMware also requires more resources than Hyper-V, as it runs on top of an existing operating system, rather than being part of it. VMware also has some compatibility issues with certain hardware and drivers, which may affect your performance or functionality. 5 Pros of Hyper-V One of the main benefits of Hyper-V is its affordability. Hyper-V is free for Windows Server and Windows 10 users, and you do not need to pay any extra fees to use it. Hyper-V also has a lower overhead than VMware, as it runs as a part of the Windows kernel, rather than as a separate application. Hyper-V also has some advantages over VMware in terms of integration with other Windows features, such as networking, security, and management. 6 Cons of Hyper-V One of the main disadvantages of Hyper-V is its limited support for non-Windows operating systems. Hyper-V may not work well with some Linux distributions or other operating systems, and you may need to install additional drivers or tools to make them run smoothly. Hyper-V also has a smaller and less active community than VMware, which means you may have less access to resources, documentation, and support. Hyper-V also lacks some of the advanced features that VMware offers, such as snapshots, cloning, and live migration."
  },"/dev/os/docker/3_docker_0_images/": {
    "title": "0. Images to Download",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_0_images/",
    "body": "OneDrive URL (Ubuntu VM - Docker Ready!) Download Here Ubuntu VM Credentials UN: Comp6017 PW: Comp6017"
  },"/dev/os/docker/3_docker_1_fundamentals/": {
    "title": "1. Fundamentals",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_1_fundamentals/",
    "body": "1. Docker vs Virtual Machines? Docker and virtual machines (VMs) are both technologies used to create isolated environments for running applications, but they have fundamental differences in terms of architecture, performance, and usage. Here’s a detailed comparison: 1.1 Architecture Virtual Machines: VMs run on a hypervisor, which can be either Type 1 (bare-metal) or Type 2 (hosted). Each VM includes a full operating system (OS) instance, which means that every VM has its own kernel and set of system libraries. VMs are more heavyweight due to the need for separate OS instances and the overhead of the hypervisor. Docker: Docker uses containerization technology, which leverages the host OS kernel. Containers share the host OS kernel and use isolated user spaces, making them much lighter than VMs. Docker containers package applications and their dependencies but do not include a full OS, just the necessary libraries and binaries. 1.2 Performance Virtual Machines: VMs are more resource-intensive because they require more CPU, memory, and storage to run multiple full OS instances. The hypervisor adds a layer of overhead, which can impact performance. Docker: Containers are more lightweight and efficient because they share the host OS kernel. Startup times for containers are typically much faster compared to VMs. Docker can achieve higher density of applications on the same hardware compared to VMs. 1.3 Isolation Virtual Machines: VMs provide strong isolation because each VM runs a completely separate OS instance. This isolation is more secure but at the cost of higher resource usage. Docker: Containers provide process-level isolation using namespaces and control groups (cgroups) in the Linux kernel. While still secure, container isolation is generally considered to be less strong than VM isolation since they share the same kernel. 1.4 Portability Virtual Machines: VMs are portable across different physical machines and can be migrated easily using tools provided by hypervisor vendors. Portability can be limited by differences in the underlying hardware and hypervisor features. Docker: Docker containers are highly portable and can run on any system that supports Docker, regardless of the underlying hardware or OS. Docker images can be easily shared and deployed across different environments using Docker Hub or private registries. 1.5 Use Cases Virtual Machines: Running multiple different operating systems on a single physical machine. Providing strong isolation for applications that require high security. Legacy applications that require a specific OS environment. Docker: Microservices architecture and modern application development. Continuous Integration/Continuous Deployment (CI/CD) pipelines. Applications that need to be lightweight, scalable, and portable. Summary Aspect Virtual Machines Docker Containers Architecture Full OS instance per VM Shared OS kernel Resource Usage High Low Performance Slower startup, higher overhead Fast startup, low overhead Isolation Stronger isolation Process-level isolation Portability Limited by hypervisor Highly portable Use Cases Multi-OS environments, strong security Microservices, CI/CD, scalability Both Docker and virtual machines have their own strengths and are suitable for different scenarios. Understanding these differences can help in choosing the right technology for a given use case. 2. What is a hypervisor? A hypervisor, also known as a virtual machine monitor (VMM), is software, firmware, or hardware that creates and manages virtual machines (VMs) by allowing multiple operating systems to share a single hardware host. Each operating system or virtual machine appears to have the host’s processor, memory, and other resources all to itself, but the hypervisor is actually controlling the host processor and resources, allocating what is needed to each operating system, and making sure that the guest operating systems (guest VMs) cannot disrupt each other. 2.1 Types of Hypervisors There are two main types of hypervisors: Type 1 Hypervisors (Bare-Metal Hypervisors): These hypervisors run directly on the host’s hardware to control the hardware and to manage guest operating systems. They do not require a separate underlying operating system. Examples include VMware ESXi, Microsoft Hyper-V, and Xen. Type 2 Hypervisors (Hosted Hypervisors): These hypervisors run on a conventional operating system (OS) just as other computer programs do. The guest operating system runs as a process on the host. Examples include VMware Workstation, Oracle VirtualBox, and Parallels Desktop. 2.2 Functions of a Hypervisor Resource Allocation: Hypervisors allocate resources like CPU, memory, storage, and network to each VM. Isolation: They provide isolation between VMs to ensure that the operation of one VM does not affect others. Virtual Hardware Emulation: Hypervisors emulate virtual hardware devices for VMs, allowing VMs to use abstracted hardware resources. VM Management: They provide tools and interfaces for creating, managing, and monitoring VMs. Migration: Hypervisors allow for live migration of VMs between hosts without downtime, which is useful for load balancing and maintenance. 2.3 Advantages of Using Hypervisors Consolidation: Multiple VMs can run on a single physical machine, improving hardware utilization. Isolation: Each VM operates independently, providing strong isolation for different workloads or tenants. Scalability: New VMs can be quickly created and deployed to scale applications. Flexibility: Different operating systems can be run on the same hardware platform. Disaster Recovery: Hypervisors can help in creating snapshots and backups of VMs, aiding in disaster recovery. 2.4 Disadvantages of Using Hypervisors Performance Overhead: Running multiple VMs on the same hardware can introduce performance overhead due to resource sharing and virtualization. Complexity: Managing a virtualized environment can be complex and may require specialized skills and tools. Security Risks: While hypervisors provide isolation, vulnerabilities in the hypervisor itself can potentially lead to security risks affecting all VMs. 2.5 Examples of Hypervisors Type 1 (Bare-Metal): VMware ESXi Microsoft Hyper-V Xen KVM (Kernel-based Virtual Machine) Type 2 (Hosted): VMware Workstation Oracle VirtualBox Parallels Desktop QEMU (Quick Emulator) IMPORTANT Hypervisors are fundamental to modern cloud computing, enabling the efficient use of hardware resources and the flexible deployment of virtualized workloads. 3. How is Docker Working ? Docker operates on a client-server architecture. Docker Client: The Docker client is the primary way users interact with Docker. It provides a command-line interface (CLI) that sends commands to the Docker daemon. The client can run on the same host as the daemon or connect to a remote daemon. Docker Daemon: The Docker daemon (dockerd) is a server that runs on the host machine. It is responsible for building, running, and managing Docker containers. The daemon listens for Docker API requests and manages Docker objects, such as images, containers, networks, and volumes. Docker REST API: The Docker API is used by the Docker client to communicate with the daemon. It can also be used by other applications to interact with the daemon. Docker Registry: This is a repository for Docker images. The Docker client can pull images from the registry to create containers and push images to the registry. Docker Hub is a popular public registry, but there are also private registries. In this architecture, the Docker client can interact with the Docker daemon over various protocols, such as UNIX sockets or network interfaces, enabling flexible deployment and management of containers. 4. Docker on Windows (Installation) Docker Desktop is a crucial tool for developers looking to containerize applications. The video from the Docker Mastery course offers a detailed walkthrough, now shared on YouTube for broader accessibility. Below are the steps and recommendations from the course for setting up Docker Desktop on Windows 10 or 11. 1. Downloading and Installing Docker Desktop Visit the Docker Desktop download page and download the installer. Run the executable to start the installation process. 2. Enabling WSL 2 Docker Desktop now uses WSL 2, a more efficient way to run Linux on Windows compared to the older Hyper-V setup. During installation, enable WSL 2 if it isn’t already enabled. The installer will guide you through this, including installing the necessary Linux kernel update. 3. Post-Installation Configuration After installation, launch Docker Desktop and follow the setup wizard. Agree to the End User License Agreement (EULA). Docker Desktop is free for learning and personal use, though some enterprise features may require a paid license. 4. Setting Up Visual Studio Code Download Visual Studio Code from its official website. Install Docker and Kubernetes extensions within Visual Studio Code for enhanced functionality. 5. Adjusting Docker Desktop Settings Access Docker Desktop settings by right-clicking the Docker icon in the system tray and selecting “Settings”. Configure your preferred settings, especially under the WSL integration section. Ensure your Linux distributions (e.g., Ubuntu) are enabled for Docker. 6. Creating a Docker ID Create a free Docker ID at hub.docker.com. Log in to Docker Desktop with your Docker ID to increase your pull rate limit from Docker Hub. 7. Cloning the Course Repository Clone the course repository into your WSL file system for better performance: git clone &lt;repository_url&gt; USING WINDOWS TERMINAL - NOT RECOMMENDED! Windows Terminal provides a modern interface for managing your command-line tools, supporting PowerShell, Command Prompt, and WSL distributions. Download it from the Microsoft Store if you’re on Windows 10, or use the pre-installed version on Windows 11. Customize the terminal to set your default profile to WSL for a seamless Docker experience. RECOMMENDED Use WSL2 distribution like Ubuntu to access docker instead of docker desktop or Windows terminal. Troubleshooting Tips Virtualization Errors: Ensure CPU virtualization features (VT-x) are enabled in your BIOS. Pull Rate Limits: Log in with your Docker ID to avoid hitting free-tier limits on Docker Hub. 5. Docker CLI Cheat Sheet Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Download Cheat Sheet"
  },"/dev/os/docker/3_docker_2_workflow/": {
    "title": "2. Workflow",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_2_workflow/",
    "body": "1. Creating a Docker Container Creating a Docker container that runs a Unix shell is straightforward and a common use case. Docker containers can run any process, including a shell, provided that the necessary binaries and libraries are included in the container image. Here’s a step-by-step guide on how to create and run a Docker container with a Unix shell: Step 1: Install Docker Ensure Docker is installed on your system. You can download and install Docker from the official Docker website. Step 2: Choose a Base Image Docker images are built from base images, which can include various Linux distributions. Common choices for Unix shell environments include: alpine: A minimal Docker image based on Alpine Linux. ubuntu: A more full-featured Docker image based on Ubuntu. Step 3: Create a Dockerfile A Dockerfile is a script that contains instructions on how to build a Docker image. Below are examples of Dockerfiles for both Alpine Linux and Ubuntu: Dockerfile for Alpine Linux # Use the official Alpine Linux image FROM alpine:latest # Install Bash shell RUN apk add --no-cache bash # Set the default command to run Bash CMD [\"bash\"] Dockerfile for Ubuntu # Use the official Ubuntu image FROM ubuntu:latest # Install Bash shell RUN apt-get update &amp;&amp; apt-get install -y bash # Set the default command to run Bash CMD [\"bash\"] Step 4: Build the Docker Image Navigate to the directory containing your Dockerfile and build the Docker image using the docker build command: # For Alpine Linux docker build -t alpine-bash . # For Ubuntu docker build -t ubuntu-bash . The -t flag tags the image with a name (alpine-bash or ubuntu-bash). Step 5: Run the Docker Container Once the image is built, you can run a container from the image using the docker run command: # For Alpine Linux docker run -it alpine-bash # For Ubuntu docker run -it ubuntu-bash The -it flags make the container interactive and allocate a pseudo-TTY, allowing you to interact with the shell. 1.1 Example: Running an Alpine Linux Shell Here’s a complete example of creating and running an Alpine Linux shell in a Docker container: Create a directory and a Dockerfile: mkdir alpine-shell cd alpine-shell nano Dockerfile Add the following content to the Dockerfile: FROM alpine:latest RUN apk add --no-cache bash CMD [\"bash\"] Build the Docker image: docker build -t alpine-bash . Run the Docker container: docker run -it alpine-bash TIP When starting the container, it will run the startup command ‘bash’ as specified in the docker file above! You will now be inside a Bash shell running in an Alpine Linux container. Some of linux commands to observe what’s going on inside the container. root@&lt;container-id&gt;# ls -al (observe the file system) root@&lt;container-id&gt;# ps -elf (running processes) If one explores the filesystem, there are no other shell or GUI in the file system (docker container linux is lightweight unlike VM). The bash process is the main process with PID=1. IMPORTANT Docker makes it easy to package and distribute environments, including Unix shells, ensuring consistency and isolation from the host system. 1.2 Additional Tips Networking You can connect the container to a network using the --network flag: docker run -it --network=my-network alpine-bash TIP Docker makes it easy to package and distribute environments, including Unix shells, ensuring consistency and isolation from the host system. 1.3 Dokerize an App A guide to put your application to docker and distribute. 1.4 Working with Existing Docker images 1.4.1 Creating / Running / Starting Container Docker hub provides docker images to start with. docker pull ubuntu # download the Ubuntu image docker run -it ubuntu # The command `run` # 1. creates a container with a random name # 2. starts the Ubuntu container with the default startup command specified in docker file (i.e bash) #- `-i`: Keeps STDIN open even if not attached. #- `-t`: Allocates a pseudo-TTY, which allows you to interact with the container. can run a custom startup command when running a container. docker run -it ubuntu sh # run Ubuntu container with a custom startup command that you want to run inside the container (i.e `sh`) docker run -it ubuntu top -b # The command that you want to run inside the container here is `top -b` docker run -it alpine-bash sh -c \"echo 'Hello, World!'\" # You can override the default command to run custom scripts or commands Start an existing container If you want to start an already existing container in interactive mode, you can use: docker start -i &lt;container_name_or_id&gt; This will attach to the container and allow you to interact with it. IMPORTANT Cannot run a command (like in run ... &lt;command to run inside container&gt;) when starting a container Start an existing container in detach mode To start an existing Docker container in detached mode: docker start &lt;container_name_or_id&gt; By default, Docker containers will run in the background unless they are designed to be interactive (e.g., by running a shell). If you want to reattach or interact with the running container, you can use: docker attach &lt;container_name_or_id&gt; # This will attach you to the PID=1 main process Run a command in a running container The docker exec -it command is used to run a command in a running Docker container interactively. This is often used to open a new shell session inside a container that is already running. To start a bash session inside a running container: docker exec -it &lt;container_name_or_id&gt; /bin/bash # Creates a new shell with bash and attach a pseudo terminal in the interactive mode docker exec: This part of the command is used to execute a command inside a running container. -i: This flag keeps STDIN open, allowing you to interact with the container. -t: This flag allocates a pseudo-TTY, which makes the command interactive (usually used for commands like /bin/bash or /bin/sh). IMPORTANT This command is useful when you need to inspect the container, troubleshoot, or manually run commands inside it. Enter into a running container You can enter a running container with: docker exec -it my_new_container /bin/bash you can replace bash with sh if bash is not available in the container. To attach to a running container later, use -a / –attach option: docker start -a my_new_container If you need to explicitly use a UID , like root = UID 0, you can specify this: docker exec -it -u 0 my_new_container /bin/bash # will log you as root 1.4.2 Exiting Container Lets run a container docker run -it ubuntu bash Exit without Stopping the container To exit the bash shell in the container one can, use the Ctrl+PQ option which will return the host system prompt but keeps the container running in the background. This operation detaches the container and allows you to return to your system’s shell without exiting from the only process (i.e bash) which is running inside the container. Hence the container will stay Running. Alternative Once inside the container, if you exit from the only process running inside the container (in this example, bash process), it will return to your system’s shell, and the container will be stopped. root@container_id# exit IMPORTANT A container must have a running process to stay alive! A container cannot exist if the main process is terminated. 2. Docker with Ubuntu vs Ubuntu VM? Docker provides the functionality of different operating systems, like Ubuntu, without running a full guest OS by leveraging a few key concepts and technologies. Here’s how Docker manages to provide the same functionalities using an Ubuntu image without installing the entire Ubuntu operating system: 2.1 Key Concepts and Technologies Containers vs. Virtual Machines: Virtual Machines (VMs): Each VM includes a full operating system along with the application and its dependencies, running on a hypervisor. This means each VM has its own kernel and OS resources. Containers: Containers, on the other hand, share the host system’s kernel and only include the application and its dependencies. Containers run as isolated processes on the host OS, using its kernel but maintaining their own filesystem, network, and process space. Union File Systems and Docker Layers: Docker images are built in layers. Each instruction in a Dockerfile creates a new layer. These layers are stacked and form a union filesystem, which means that the image only contains the necessary parts of the OS to run the application. For an Ubuntu image, this includes the necessary binaries, libraries, and tools that are part of the Ubuntu userland, but not the kernel. Namespaces and Cgroups: Namespaces: Provide isolated environments within the same OS instance. This includes process isolation (PID namespace), user isolation (user namespace), file system isolation (mount namespace), etc. Control Groups (Cgroups): Manage and limit resource usage (CPU, memory, disk I/O, etc.) of the containerized applications. Docker Images: When you pull an Ubuntu Docker image from Docker Hub, it includes the Ubuntu filesystem (binaries, libraries, etc.) but without the Ubuntu kernel. It relies on the host’s kernel to function. This means the image has everything needed to provide the “Ubuntu experience” (such as the apt package manager, bash shell, common utilities, etc.) without the overhead of a full OS. 2.2 Example of How an Ubuntu Image Works in Docker When you run an Ubuntu container, Docker uses the host OS kernel and provides the container with the Ubuntu filesystem: Download the Ubuntu Image: docker pull ubuntu Run the Ubuntu Container: docker run -it ubuntu Inside the container, you can run commands just like you would on a full Ubuntu system: root@container_id:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var You can use apt-get to install software, navigate the filesystem, and perform other tasks as if you were on an Ubuntu machine. How Docker Achieves This File System: The Ubuntu Docker image contains a minimal root filesystem that replicates the Ubuntu environment. This includes directories like /bin, /etc, /lib, /usr, etc., filled with the usual tools and libraries. Process Management: Docker containers run as isolated processes on the host system. The host’s kernel manages these processes, providing the necessary system calls and resource management. User Space Tools: The tools and applications you use within the container are from the Ubuntu user space. These tools are packaged in the Docker image, enabling you to interact with the container as if it were a standalone Ubuntu system. IMPORTANT Docker containers simulate a full OS environment by packaging the userland components (binaries, libraries, tools) of that OS while relying on the host’s kernel. This approach allows Docker to provide lightweight, efficient, and isolated environments that mimic the functionalities of a complete OS without the overhead of a full VM. 3. Docker Basics 3.1 Docker Crash Course Pulled docker images are stored in? Here is a list of the storage locations of the docker images on different operating systems: Ubuntu: /var/lib/docker/ Fedora: /var/lib/docker/ Debian: /var/lib/docker/ Windows: C:\\ProgramData\\DockerDesktop. MacOS: ~/Library/Containers/com. docker. docker/Data/vms/0/ 3.3 Docker Advance Docker Basic Commands 0. Docker Related docker ps running containers docker run –name debian-container-always -it –restart always debian:latest container restarts automatically if stopped due to exit from the main process. To end the session, clean up all the containers and images. docker container rm -f (sudo docker ps -aq) docker image rmi -f $(sudo docker images – aq) 1. Docker run -dit flag In Docker, the -dit flags are used in combination to run a container in the background (detached mode), while still providing interactive terminal capabilities and allocating a pseudo-TTY. Let’s break down the purpose of each flag and why they might be used together: 1. -d (detached mode): This flag runs the container in the background, returning control of the terminal to the user. The container continues to run even after the user logs out or closes the terminal session. 2. -i (interactive mode): This flag keeps the STDIN stream open, allowing the user to provide input to the container interactively. It is useful for cases where the container process requires user input or when running a process that reads from the terminal. 3. -t (pseudo-TTY): This flag allocates a pseudo-TTY, which provides an interface for terminal input and output. It makes the command prompt more user-friendly, with features like line editing. Why Use -dit Together? While it may seem counterintuitive to use -d (detached mode) and -it (interactive and TTY) together, there are specific scenarios where this combination is useful: 1. Starting Interactive Services in the Background: Sometimes, you want to start a service or an interactive application (like a shell or a REPL) in the background, but still be able to attach to it later if needed. For example, running a debugging tool or an interactive shell session in a container that you can later attach to using docker attach or docker exec. 2. Log and Debug: By using -it, you can ensure that the container has a proper terminal interface, which can be useful for logging and debugging. Even if the container is running in detached mode (-d), you can inspect logs or attach to the container to interact with it. 3. Daemon Processes with Interactive Control: Some daemon processes may provide an interactive control interface that can be accessed through a TTY. By using -dit, the container runs in the background, but you can still interact with the daemon if necessary. Example Scenario Suppose you’re running a container with a shell that needs to start and continue running in the background, but you also want the ability to interact with it later: docker run -dit --name my-shell-container ubuntu bash In this case: -d allows the container to run in the background. -i keeps the input open, which is necessary for interactive shells. -t provides a terminal interface, making the shell prompt user-friendly. You can later attach to this container using docker attach my-shell-container or execute a command interactively using docker exec -it my-shell-container bash. IMPORTANT Using -dit is not a common scenario, but it offers flexibility when you want a containerized process to run in the background while retaining interactive capabilities. Important Example Following docker container will immediately stop as soon it is started. You will not be able to attach or exec any command on the container again. To attach, you need to have a interactive pseudo terminal created initianlly when you create a container To exec, you need a running container docker run --name stranded_container ubuntu # or docker run --name stranded_container ubuntu bash # Creating a container without a interactive peusdo terminal (-it). # `bash` process will simply run in ubuntu and exit as soon as the container starts. Therefore resulting in stopping the container. 3. Docker commit The docker commit command is used to create a new Docker image from an existing container. This allows you to capture the current state of a container, including any changes made to it, and save it as a new image that can be used to create other containers. docker commit [OPTIONS] &lt;container_id_or_name&gt; &lt;new_image_name&gt; &lt;container_id_or_name&gt;: The ID or name of the container you want to commit. &lt;new_image_name&gt;: The name you want to give to the new image. Common Options: -m \"message\": Adds a commit message, similar to a Git commit message, describing what changes were made. -a \"author\": Specifies the author of the image (e.g., -a \"John Doe\"). -p: Pauses the container during the commit to ensure that the filesystem is in a consistent state. Example: Assume you have a running container with ID abc123 that you’ve made changes to, and you want to save those changes as a new image called my_custom_httpd: docker commit -m \"Customized Apache HTTPD configuration\" -a \"John Doe\" abc123 my_custom_httpd This command will create a new image named my_custom_httpd based on the current state of the abc123 container. You can then use this image to run new containers with the changes preserved. Verifying the New Image: To verify that the new image has been created, you can list your Docker images: docker images You should see my_custom_httpd in the list of available images. 3.1 Docker commit vs dockerfiles docker commit and Dockerfiles serve different purposes in Docker workflows, and each has its own advantages and use cases. Here’s a comparison of the advantages of using docker commit versus Dockerfiles: Advantages of docker commit 1. Quick Prototyping and Experimentation: Immediate Changes: docker commit allows you to quickly capture the current state of a running container, including changes made interactively (e.g., installing packages, modifying files). This can be useful for rapid prototyping or experimentation without needing to create a Dockerfile. Ease of Use: For developers who are experimenting with a new setup or configuration, docker commit can be a fast way to save the state of a container without writing a Dockerfile. 2. Snapshot of Current Container State: Capture Complex States: It’s useful when you have manually configured a container in ways that are hard to describe in a Dockerfile, such as complex runtime configurations or custom setup scripts. Preserve Manual Changes: If you’ve made manual changes inside a container (e.g., modifications to files or configurations) and want to save those changes as an image, docker commit captures those changes. 3. No Need for Dockerfile Syntax Knowledge: Simplicity for Non-Developers: Users who are not familiar with Dockerfile syntax or who prefer not to write Dockerfiles can use docker commit to create images from their containers without needing to learn Dockerfile commands. Advantages of Dockerfiles 1. Reproducibility: Consistent Builds: Dockerfiles provide a clear and repeatable method to build Docker images. This ensures that anyone building the image from the Dockerfile will get the same result, which is crucial for debugging and maintaining consistency across different environments. Version Control: Dockerfiles can be stored in version control systems (e.g., Git), allowing you to track changes over time and collaborate with others. 2. Documentation: Readable and Maintainable: Dockerfiles serve as documentation of how an image is built. They provide a human-readable and maintainable record of the installation and configuration steps. Best Practices: Dockerfiles encourage best practices by using well-defined instructions and following conventions that ensure better image layering and efficiency. 3. Automation and CI/CD Integration: Automated Builds: Dockerfiles can be used in continuous integration and deployment pipelines to automate image builds and deployments. Consistent Environments: Automated builds using Dockerfiles help maintain consistent environments across development, staging, and production. 4. Flexibility and Control: Custom Builds: Dockerfiles offer fine-grained control over the build process, allowing for optimization and customization that docker commit cannot provide. Complex Configurations: They enable the creation of complex images with multi-stage builds, conditional logic, and advanced configuration that docker commit does not support. In Summary Use docker commit when you need a quick snapshot of a running container’s state or when you are making interactive changes and need to preserve them without initially defining a Dockerfile. Use Dockerfiles for reproducibility, automation, documentation, and when you require a structured, maintainable way to build Docker images. IMPORTANT In most production scenarios, Dockerfiles are preferred due to their reproducibility and automation capabilities, while docker commit is more suited for ad-hoc tasks and experimentation. 4. Docker inspect The docker inspect command is used to obtain detailed information about Docker objects, such as containers, images, networks, or volumes. However, docker inspect doesn’t directly show the commit history of a Docker image or container in the way that, for example, Git shows commit history. Inspecting a Docker Image: If you want to see detailed information about a Docker image, including its layers and configuration, you can use: docker inspect &lt;image_name_or_id&gt; This command will display a JSON-formatted output with various details, such as the image’s ID, creation date, environment variables, command history, and more. Example: docker inspect my_custom_httpd Inspecting a Container: Similarly, to inspect a container, you can run: docker inspect &lt;container_name_or_id&gt; Example: docker inspect my_running_container 5. Docker history Viewing the Layers (Image History): If you want to see the layer-by-layer history of an image (which can give you insight into changes made via commits), you can use the docker history command: docker history &lt;image_name_or_id&gt; This command shows a list of layers, including the command that created each layer, the size of each layer, and when it was created. Example: docker history my_custom_httpd The output will show each layer of the image, which can help you understand what commands were run and what changes were made over time. However, it won’t show detailed commit messages like a version control system. To summarize, while docker inspect is useful for viewing detailed metadata and configuration, docker history is the command you would use to inspect the “commits” or layers that make up a Docker image. TODO: In progress Redis with Docker https://www.docker.com/blog/how-to-use-the-redis-docker-official-image/ Docker Compose https://youtu.be/DM65_JyGxCo What is docker-compose? Let’s come back to docker-compose. Docker Compose is a tool you can use to define and share multi-container applications. This means you can run a project with multiple containers using a single source. For example, assume you’re building a project with NodeJS and MongoDB together. You can create a single image that starts both containers as a service – you don’t need to start each separately. Interesting right? And this solves the problem which I called out at the very beginning of this article. To achieve this we need to define a docker-compose.yml."
  },"/dev/os/docker/3_docker_3_volume/": {
    "title": "3. Volumes",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_3_volume/",
    "body": "1.1 Creation of Volume Following methods are available to persist data in docker 1. Mount a folder If you need to persist data, you can mount a volume using the -v flag: You can mount a folder in a host PC to the container as a volume docker run -it -v /host/path:/container/path alpine-bash 2. Named Volume First create the volume and then mount it. docker volume create named-volume docker run -dit --name ubuntu-named-vol --volume named-volume:/volume-mounted-from-host ubuntu:latest # or docker run -dit --name ubuntu-named-vol -v named-volume:/volume-mounted-from-host ubuntu:latest IMPORTANT The named volume created will stay in docker (not on the host PC). You can import content to the name volume created via docker and access it inside the container (vice versa). 3. Create a named volume on the fly Docker creates the named volume on the fly and mount it. How sudo docker run -dit --name ubuntu-named-vol –mount source=myvolume target=/volume-mounted-from-host ubuntu:latest # or docker run -dit --name ubuntu-named-vol -v named-volume:/volume-mounted-from-host ubuntu:latest 4. Additional Tips Mount a volume onto an existing directory You can also mount a volume for an existing data volumn / folder inside the container. Following will create a container and add the data volume at /var, a directory that is found in the base image. docker run -dit --name ubuntu-named-vol -v named-volume:/var ubuntu:latest 1.2 Shared Volume To share a volume between two Docker containers, you can create a Docker volume and then mount it to both containers. This allows the containers to read from and write to the same storage location, enabling them to share data. 1. Method 01 1. Create a Docker Volume: First, create a Docker volume that will be shared between the containers. docker volume create shared_volume This creates a named volume called shared_volume. 2. Run the First Container with the Volume: Run the first container and mount the volume to a specific directory inside the container. docker run -d --name container1 -v shared_volume:/shared_data my_image -d: Runs the container in detached mode. --name container1: Names the container container1. -v shared_volume:/shared_data: Mounts the shared_volume to the /shared_data directory inside the container. my_image: The name of the Docker image to run. 3. Run the Second Container with the Same Volume: Now, run the second container and mount the same volume to a directory inside it. docker run -d --name container2 -v shared_volume:/shared_data my_image This mounts the same shared_volume to the /shared_data directory inside the second container. Explanation: Both containers (container1 and container2) now share the same shared_volume mounted to the /shared_data directory. Any data written to /shared_data by one container will be accessible to the other container, allowing them to share files and data. Example Scenario: If container1 writes a file to /shared_data/myfile.txt, container2 will be able to read that file from the same location. This is useful in various scenarios, such as when you want two services running in separate containers to share configuration files, logs, or any other data. 2. Method 02 # Create a volume on the fly docker run -it --name=Container1 -v named-volume:/shared-data-vol ubuntu # Now, create another container that mounts the same volume from Container1 (using --volumes-from) docker run -it --name=Container2 --volumes-from Container1 ubuntu # This will show the volume `named-volume` (from Container1) inside Container2 --volumes-from &lt;source_container&gt;: Specifies the container from which volumes should be mounted. This is the container that has the volumes you want to share. 3. Detach / Remove a Volume To detach a volume from a Docker container, you generally need to stop the container first and then remove the container. If you just want to detach the volume without removing it, you can do this by removing the container without deleting the associated volume. Steps are described below: 1. Stop the Container (Optional): If the container is running, stop it first: docker stop &lt;container_name_or_id&gt; 2. Remove the Container Without Removing the Volume: When removing the container, you can use the --volumes or -v flag to delete the associated volumes. To detach the volume and keep it intact, you should omit this flag: docker rm &lt;container_name_or_id&gt; This command will remove the container but will leave the volume intact. 3. Re-Attach the Volume to a New Container (Optional): If you want to use the same volume with another container, you can re-attach it when running a new container: docker run -d --name new_container -v shared_volume:/shared_data my_image Example: Let’s say you have a container named container1 using a volume called shared_volume: Stop the Container: docker stop container1 Remove the Container (Without Removing the Volume): docker rm container1 Re-Attach the Volume to Another Container: docker run -d --name container2 -v shared_volume:/shared_data my_image Note: If you use the docker rm -v &lt;container_name_or_id&gt; command, it will remove the container and any anonymous volumes associated with it, but named volumes like shared_volume will not be deleted unless explicitly removed with docker volume rm. The volume itself persists until you explicitly delete it with: docker volume rm shared_volume This approach allows you to detach a volume from one container and re-attach it to another, keeping the data intact and available. 4. Attach a Volume IMPORTANT Attaching a volume to an existing Docker container directly is not possible. Docker containers have a fixed configuration once they are created, which includes their volumes. However, you can achieve the desired result by creating a new container with the same configuration as the old one, including the new volume. 1. Create a New Container with the Additional Volume To simulate adding a volume to an existing container, you can create a new container based on the old one’s configuration and include the new volume mount. 1. Retrieve the Configuration of the Existing Container: First, get the details of the existing containe**r. You’ll need the image name and any necessary configuration details like environment variables, port mappings, etc. Use the following command to get the configuration: docker inspect &lt;existing_container_name_or_id&gt; Look for relevant details such as environment variables, ports, and volume mounts in the output. 2. Run a New Container with the Same Configuration and the New Volume: Use the information obtained from the inspect c**ommd to create a new container with the additional volume. docker run -d \\ --name &lt;new_container_name&gt; \\ --env &lt;env_vars&gt; \\ -p &lt;host_port&gt;:&lt;container_port&gt; \\ -v &lt;new_host_volume_path&gt;:&lt;new_container_volume_path&gt; \\ &lt;image_name&gt; Replace: &lt;new_container_name&gt;: a name for the new container. &lt;env_vars&gt;: environment variables if needed (format: VAR=value). &lt;host_port&gt;:&lt;container_port&gt;: port mappings if needed. &lt;new_host_volume_path&gt;: the new volume path on the host machine. &lt;new_container_volume_path&gt;: the path inside the container where the new volume will be mounted. &lt;image_name&gt;: the Docker image used by the existing container. Make sure to include any other configurations that were present in the old container. 3. Copy Data (if necessary): If you need to transfer data from the old container to the new one, you might need to manually cop**y fes. You can use docker cp to copy files between containers or between the host and containers. docker cp &lt;existing_container_name_or_id&gt;:&lt;path_in_container&gt; &lt;path_on_host&gt; docker cp &lt;path_on_host&gt; &lt;new_container_name&gt;:&lt;path_in_container&gt; 4. Verify the New Container: Check if the new container is running correctly and verify the volume mount: docker inspect &lt;new_container_name&gt; | grep Mounts -A 10 5. Remove the Old Container (optional): Once you’ve confirmed that the new container is working as expected, you can remove the old container if it’s no longer needed: docker rm &lt;existing_container_name_or_id&gt; This method essentially involves replicating the existing container with the addition of the new volume. While it’s not a direct modification of the existing container, it achieves the same result by creating a new container with the desired configuration. 1.3 Additional Tips: You can list all your Docker volumes using: docker volume ls To get more details about the shared volume: docker volume inspect shared_volume This will show you where the volume is stored on your host system and other details. 1.4 Read Only Volumes To map a volume in Docker where changes to a host folder do not affect the corresponding folder inside the container, you need to use a read-only bind mount. This ensures that the container can read from the host folder but cannot modify it, and changes on the host do not reflect in the container. Here’s how to do it: docker run -v /host/folder:/container/folder:ro &lt;image_name&gt; /host/folder: The directory on your host machine. /container/folder: The directory inside the container. :ro: Stands for “read-only”. This flag makes the bind mount read-only inside the container. Example: docker run -v /home/user/data:/app/data:ro my-container In this example: /home/user/data is the folder on your host. /app/data is the folder inside the container. :ro makes the /app/data folder read-only in the container, preventing changes inside the container from affecting the host. This setup ensures that the container can read from the host folder, but changes to the host folder will not automatically sync to the container during runtime. However, any updates in the host folder before the container starts will be reflected when it starts. 1.5 Anonymous Volume In Docker, an anonymous volume is a type of volume that is created automatically by Docker without a specific name. These volumes are used primarily when you want to persist data outside the container’s filesystem but don’t need to reference the volume by name later. 1. Key Characteristics of Anonymous Volumes 1. No Explicit Name: Unlike named volumes, anonymous volumes are not given a specific name by the user. Docker generates a unique name for them automatically. 2. Persistence: The data stored in an anonymous volume persists even after the container is removed. However, since the volume doesn’t have a name, it can be difficult to locate or reuse manually. 3. Automatic Creation: Docker creates anonymous volumes when you use the VOLUME instruction in a Dockerfile or when you bind a directory inside a container without specifying a host path. 2. Usage Example docker run -v /container/data busybox In this example: -v /container/data creates an anonymous volume mounted to the /container/data directory inside the container. Docker creates and manages this volume automatically. 3. Listing Anonymous Volumes You can list all volumes (including anonymous ones) with: docker volume ls 4. Identifying Anonymous Volumes Anonymous volumes have randomly generated names like 5a2e85b1a5dfd10e2b9ef77d497. 5. Data Persistence Even if you remove the container, the data in this anonymous volume will persist until the volume itself is removed. However, since it’s anonymous, the volume is hard to reuse without its randomly generated name. 6. Clean-Up Since anonymous volumes can accumulate and consume disk space over time, you might want to clean them up using: docker rm -f &lt;container_id&gt; docker volume prune This command removes all unused volumes, including anonymous ones no longer referenced by any container. 7. Use Cases Anonymous volumes are handy when you need to persist data temporarily or for single-use containers, without worrying about volume management afterward. For long-term or shared data storage, named volumes are typically preferred."
  },"/dev/os/docker/3_docker_4_networking/": {
    "title": "4. Networking",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_4_networking/",
    "body": "1. Fundamentals of Networking 2. Linking Containers 2.1 Using a Bridge Network To link two Redis containers in Docker, where one acts as the database and the other as a command-line interface (CLI) client, you can use Docker’s network features to allow the containers to communicate with each other. Here’s a step-by-step guide: 1. Create a Docker Network: This will allow the containers to communicate with each other. 2. Run the Redis Database Container: Start the Redis database container and attach it to the network. 3. Run the Redis CLI Container: Start another container with the Redis CLI, also attached to the same network, and connect it to the Redis database. Step 1: Create a Docker Network First, create a custom bridge network: docker network create redis-net Step 2: Run the Redis Database Container Run the Redis database container on the created network: docker run -d --name redis-db --network redis-net redis Here, -d runs the container in detached mode, --name redis-db names the container, and --network redis-net attaches the container to the redis-net network. Step 3: Run the Redis CLI Container Run another container for the Redis CLI, connecting it to the database container: docker run -it --rm --network redis-net redis redis-cli -h redis-db In this command: -it makes the container interactive and attaches a terminal. --rm removes the container once it exits. --network redis-net attaches the container to the redis-net network. redis is the image name. redis-cli is the command to run the Redis CLI. -h redis-db specifies the hostname of the Redis server to connect to, which is the name of the database container. With this setup, the Redis CLI container will connect to the Redis database container. You can then run Redis commands interactively. Example Command Once inside the Redis CLI, you can run commands like: SET mykey \"Hello\" GET mykey This setup isolates the containers and ensures they can communicate securely within the same network. 2.2 Link without Creating Docker Network To link two Docker containers without using a Docker network, you can use the --link option. Following steps summarizes steps to link a Redis database container and a Redis CLI container using the --link option: DEPRECATED --link This feature has been deprecated in favor of using Docker networks, but it can still be used in some cases. Step 1: Run the Redis Database Container First, run the Redis database container: docker run -d --name redis-db redis Step 2: Run the Redis CLI Container with Link Next, run the Redis CLI container and link it to the Redis database container: docker run -it --rm --link redis-db:redis redis redis-cli -h redis In this command: -it makes the container interactive and attaches a terminal. --rm removes the container once it exits. --link redis-db:redis creates a link from the current container to the redis-db container and gives the alias redis for connecting to it. redis is the image name. redis-cli is the command to run the Redis CLI. -h redis specifies the hostname (alias) of the Redis server to connect to, which corresponds to the alias defined in the --link option. This setup will allow the Redis CLI container to connect to the Redis database container using the alias redis without needing to specify an IP address or custom network. Step 2 (Alternative) First create a redis container (for the CLI) with the link to redis-db, run bash command at the start. docker run --rm -it –-link redis-db:redis redis /bin/bash at the bash prompt (inside the container), run the command redis-cli to start the redis cli, connecting to the database running at port 6379 at the server host redis (alias which is given with the --link option) root@309-402356254:/data# redis-cli -h redis -p 6379 TIP During server container creation, you can connect to the Redis server using the specified Redis port (6379 is the usual default if otherwise specified)."
  },"/dev/os/docker/3_docker_5_compose/": {
    "title": "5. Docker Compose",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_5_compose/",
    "body": "5.1 Introduction Docker Compose is a tool that allows you to define and manage multi-container Docker applications. It uses a YAML file to configure the application’s services, networks, and volumes. With Docker Compose, you can easily manage and deploy complex applications that require multiple services working together, such as web servers, databases, caches, and more. 1. Key Features of Docker Compose: 1. Multi-Container Management: Define and run multiple containers as a single application. For example, you can set up a web server, database, and cache in one docker-compose.yml file. 2. Declarative Configuration: The docker-compose.yml file describes the entire application, including its services, networks, and volumes, in a clear and consistent manner. 3. Single Command Deployment: Start your entire application stack with a single command (docker-compose up), which builds, creates, and starts all the defined services. 4. Environment-Specific Configurations: Easily manage different environments (development, testing, production) using environment variables and overriding configurations. 5. Networking: Automatically sets up a network for your containers, allowing them to communicate with each other using service names as hostnames. 6. Scalability: You can scale services up or down with a simple command (docker-compose up --scale &lt;service&gt;=&lt;num&gt;), making it easy to handle more load or reduce resource usage. 2. How Docker Compose Works: 1. Define your application’s environment with a docker-compose.yml file. 2. Start your application by running docker-compose up. 3. Compose handles starting and linking your services, creating the network, and attaching any volumes defined. 3. Example Use Case: Imagine you have a web application that requires: A web server running Nginx. A backend API built with Flask. A Redis instance for caching. With Docker Compose, you can define all these services in one file and manage them together. This eliminates the need to manually link containers, manage networks, or define volumes separately. 4 Basic Workflow: 1. Create a docker-compose.yml file: This file contains the definitions for your services. 2. Run docker-compose up: This command builds the images (if needed), creates the containers, and starts all the services defined in the file. 3. Use docker-compose down to stop and remove all containers, networks, and volumes associated with the Compose file. IMPORTANT Docker Compose simplifies the process of running multi-container Docker applications. By defining everything in a single configuration file, it allows you to manage the lifecycle of your application as a whole, from development to production. This is a simple form of container orchestration. 5.2 Container Orchestration Docker Compose is often associated with container orchestration, but it is not a full-fledged container orchestration tool like Kubernetes or Docker Swarm. Instead, Docker Compose is a simpler tool focused on defining and managing multi-container Docker applications in a single host environment. 1. Orchestration Capabilities: Docker Compose provides basic orchestration features such as starting, stopping, and scaling containers. It allows you to manage the lifecycle of a group of containers (e.g., web server, database) on a single host. Kubernetes/Docker Swarm provide more advanced orchestration capabilities, including automated deployment, scaling across multiple hosts, self-healing, rolling updates, and more. 2. Use Case: Docker Compose is ideal for local development, testing environments, and small-scale deployments. It’s particularly useful when all services run on a single machine. Full Orchestration Tools (like Kubernetes) are designed for production environments with complex needs, such as deploying across clusters, managing multiple nodes, and handling distributed systems. 3. Single vs. Multi-Host: Docker Compose manages containers on a single host. It does not natively support multi-host deployments or sophisticated networking across multiple nodes. Kubernetes and Docker Swarm are designed for managing containers across multiple hosts, supporting large-scale, distributed applications. IMPORTANT While Docker Compose has basic orchestration features, it is not a complete orchestration tool like Kubernetes. Docker Compose is best suited for managing multi-container applications on a single host, making it a powerful tool for development and small-scale production environments. For more complex, distributed, and scalable deployments, a full orchestration system like Kubernetes is required. 5.3 Example A simple example of a docker-compose.yml file. This example sets up a basic web application with a Python Flask app, a Redis database, and a reverse proxy using Nginx: version: '3.8' services: web: image: python:3.8-slim container_name: flask-app working_dir: /app volumes: - ./app:/app # This is not a named-volume mapping ports: - \"5000:5000\" command: flask run --host=0.0.0.0 redis: image: redis:alpine container_name: redis-db ports: - \"6379:6379\" nginx: image: nginx:alpine container_name: nginx-proxy ports: - \"80:80\" volumes: - ./nginx.conf:/etc/nginx/nginx.conf # This is not a named-volume mapping Structure: 1. version: Defines the version of Docker Compose being used. 2. services: Specifies the different services that make up the application. web: Runs the Flask app using a Python image. redis: Runs the Redis database. nginx: Uses Nginx as a reverse proxy. 3. volumes: Mounts local directories or files to the container. 4. ports: Maps container ports to the host. How to Use: Place this docker-compose.yml in the root directory of your project. Run docker-compose up in the terminal to start all services. You can customize the docker-compose.yml to fit your specific application needs, such as adding databases, additional services, or scaling your application. 5.4 docker-compose up When you run docker-compose up, it starts all the services defined in your docker-compose.yml file, but these services are not entirely independent—they are designed to work together as part of a multi-container application. 1. How Services Work with docker-compose up Dependency Management: Docker Compose respects the dependencies between services. For example, if your web service depends on a database service, Docker Compose ensures that the database service is started before the web service. You can explicitly define dependencies using the depends_on key in the docker-compose.yml file. services: web: build: . depends_on: - database database: image: postgres Network Connectivity: By default, Docker Compose creates a network for all the services to communicate with each other. Services can reach one another by their service names (e.g., database in the example above). Even though the services run in separate containers, they are interconnected and can communicate over this internal network. Parallel Start: Services are typically started in parallel, but if one service depends on another, Docker Compose ensures that the dependent service is ready before proceeding. Shared Volumes: If services share a volume, they can read/write to the same data. This ensures that data is consistent across services that need to interact with the same files. 2. Independence vs. Interdependence Independent Execution: Each service runs in its own container with its own environment, which means they are isolated and can run independently. Interdependence: Despite their independent execution, services are often designed to work together as part of the same application (e.g., a web server connecting to a database). Example of docker-compose up Execution: Given a docker-compose.yml file with two services: version: '3.8' services: web: image: nginx ports: - \"80:80\" depends_on: - app app: image: python:3.8 command: python app.py docker-compose up will: Start the app service. Once app is running, start the web service. 5.5 Share Volumes in Compose In Docker Compose, you can share volumes between containers by defining a named volume and then mounting it in multiple services. This allows containers to share data, such as configuration files or persistent storage. 1. Example: Sharing a Volume Between Containers Let’s say you have two services: app and worker. Both need to access a shared directory where they read and write data. Here’s how you can define and use a shared volume in a docker-compose.yml file: version: '3.8' services: app: image: python:3.8-slim container_name: app-container volumes: - shared-data:/data # This is a named-volume command: python /data/app.py worker: image: python:3.8-slim container_name: worker-container volumes: - shared-data:/data # This is a named-volume command: python /data/worker.py volumes: shared-data: # named-volume definition 1. services: app and worker are two services that both mount the shared-data volume to the /data directory inside their containers. Both services can read and write to the /data directory, and any changes made by one service will be visible to the other. 2. volumes: The volumes section at the bottom defines the named volume shared-data. Docker manages this volume and ensures it persists across container restarts. How It Works: When you run docker-compose up, Docker will create the shared-data volume if it doesn’t exist, and mount it to the specified directories inside the app and worker containers. Both containers can access and modify files in the /data directory, enabling them to share data. This approach is useful for scenarios like sharing logs, configuration files, or temporary data between different services in your Docker Compose setup. 2. Is volumes section mandatory in compose? The volumes section in Docker Compose is not mandatory but is often used for named volumes. Named volumes are useful when you need persistent storage that is managed by Docker, shared between multiple containers, or reused across services. However, if you define a volume directly in a service without explicitly declaring it under the volumes section, Docker will automatically treat it as an anonymous volume. Here’s a comparison: 2.1 Without volumes Section (Anonymous Volume): version: '3.8' services: app: image: python:3.8-slim container_name: app-container volumes: - /data worker: image: python:3.8-slim container_name: worker-container volumes: - /data In this case, Docker will create an anonymous volume for each service. These volumes are not shared, and their lifecycle is tied to the container. 2.2 With volumes Section (Named Volume): version: '3.8' services: app: image: python:3.8-slim container_name: app-container volumes: - shared-data:/data worker: image: python:3.8-slim container_name: worker-container volumes: - shared-data:/data volumes: shared-data: Here, the volumes section defines a named volume shared-data (you can see this in docker desktop volumes tab). Both services can access this same volume, and it persists independently of the containers. 2.3 Key Differences: Named Volumes: Defined explicitly under the volumes section. These can be shared between containers and persist even if containers are removed. Anonymous Volumes: Created automatically if you omit the volumes section. They are container-specific and not shared unless explicitly defined. If you need to share data between containers, defining the volume under the volumes section is recommended. 5.6 Environment variables In Docker Compose, both the env_file and environment sections are used to set environment variables for services, but they serve slightly different purposes and are used in different ways. 1. env_file Section The env_file section specifies a file that contains environment variable definitions. Each variable is defined on a new line in the file in the format KEY=VALUE. Docker Compose reads this file and sets the environment variables in the service’s container. Usage: version: '3.8' services: web: image: nginx env_file: - .env - ./config/env.list .env and ./config/env.list are files containing environment variables. The environment variables from these files are loaded into the container. Example .env File: DATABASE_URL=postgres://user:password@db:5432/mydatabase SECRET_KEY=mysecretkey 2. environment Section The environment section allows you to define environment variables directly within the docker-compose.yml file. You can list individual variables and their values directly under this section. Usage: version: '3.8' services: web: image: nginx environment: - DATABASE_URL=postgres://user:password@db:5432/mydatabase - SECRET_KEY=mysecretkey Variables are specified directly in the docker-compose.yml file, which is useful for simple configurations or when you want to keep environment settings directly in the compose file. 3. Key Differences: 1. Source: env_file: Loads environment variables from external files. This is useful for managing variables outside the docker-compose.yml file, which can be useful for secrets, configuration files, or shared environment settings. environment: Defines environment variables directly in the docker-compose.yml file. This is useful for straightforward configurations or when the variables are not sensitive. 2. File Format: env_file: Uses a separate file (or files) where each line is a key-value pair. environment: Defines variables inline, using a list format. 3. Multiple Files: env_file: Can specify multiple files, and environment variables from these files are merged, with later files overriding variables from earlier files if there are conflicts. environment: Can only define variables directly in the compose file and does not support merging with external files. 4. Example of Using Both: You can use both sections together if needed: version: '3.8' services: web: image: busybox command: sh -c 'sleep 2 &amp;&amp; env' # print the environment variables env_file: - .env environment: - DEBUG=true In this example: Environment variables from .env file are loaded. The DEBUG variable is also set directly in the docker-compose.yml file, which could override any similar variable defined in the .env file. IMPORTANT env_file: Loads environment variables from external files. environment: Defines environment variables directly in the docker-compose.yml file. Choosing between these depends on your needs for managing configuration and secrets in your Docker Compose setup."
  },"/dev/os/docker/3_kube_1_fundamentals/": {
    "title": "1. Fundamentals",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_1_fundamentals/",
    "body": "1. Kubernetes and Docker Kubernetes and Docker are both crucial tools in the world of containerization, but they serve different purposes and are often used together. 1.1 Docker: A Containerization Platform Docker is a platform designed to simplify the process of building, deploying, and running applications using containers. Containers are lightweight, portable units that package an application and all of its dependencies, ensuring that it runs consistently across different environments—whether it’s on a developer’s local machine, in a testing environment, or in production. Key features of Docker: Containerization: Docker packages an application and its dependencies into a single container, which can run on any system with Docker installed. Portability: Since containers include everything needed to run an application, they can be easily moved across different environments. Isolation: Docker containers are isolated from each other, which helps in maintaining consistency and preventing conflicts between different applications or services. 1.2 Kubernetes: A Container Orchestration Tool Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate the deployment, scaling, and management of containerized applications. While Docker handles the creation and running of individual containers, Kubernetes takes things a step further by managing large numbers of containers across multiple hosts, ensuring that they work together effectively. Key features of Kubernetes: Orchestration: Kubernetes coordinates the scheduling and running of containers on a cluster of machines, making sure the right containers are running at the right time. Scaling: Kubernetes can automatically scale up or down the number of running containers based on demand. Load balancing: It distributes network traffic to ensure that no single container is overwhelmed with too many requests. Self-healing: Kubernetes can automatically restart containers that fail, replace them, and kill those that don’t respond to user-defined health checks. Rolling updates and rollbacks: Kubernetes allows you to update applications without downtime and roll back to previous versions if something goes wrong. 1.3 How Are Docker and Kubernetes Related? Docker and Kubernetes are closely related, but they serve different roles within the container ecosystem. Docker is used to create and manage individual containers, while Kubernetes is used to manage and orchestrate those containers at scale. Docker creates containers. Kubernetes manages those containers across a cluster of machines, handling tasks like load balancing, scaling, and ensuring high availability. Kubernetes often uses Docker as the container runtime. This means Docker is the tool that actually runs the containers, while Kubernetes orchestrates how and where those containers are deployed. In essence, while Docker is the engine that runs the containers, Kubernetes is the control system that manages many containers across multiple environments, ensuring they work together efficiently. 2. Getting Started In the world of containerization, Kubernetes and Docker are powerful tools that work together to simplify the process of deploying and managing applications. This guide walks through the steps required to set up Kubernetes on Docker for Windows, covering everything from prerequisites to initial configuration. 2.1 Prerequisites and Setup Before diving into Kubernetes, it’s essential to ensure that your system meets the necessary prerequisites. The first crucial step is to enable Hyper-V on your Windows machine. Hyper-V is the built-in hypervisor for Windows, and it’s required because Docker and Kubernetes, being Linux-based technologies, need to run within a Linux virtual machine on Windows. To enable Hyper-V: Open the Control Panel. Navigate to Programs and select Turn Windows Features on or off. Locate the Hyper-V section, check all related boxes, and click Apply and OK. If Hyper-V cannot be enabled through the control panel, it may be disabled in your system’s BIOS. This is particularly common in laptops, so you might need to enable it from the BIOS settings before proceeding. 2.2 Installing Docker for Windows Once Hyper-V is enabled, the next step is to install Docker for Windows. Begin by searching for “Docker for Windows” on Google, and follow the link to the official Docker website. The installation process involves creating a Docker Hub account, which is necessary for downloading Docker and managing containers. After the installation is complete, you can confirm that Docker is running by checking the system tray for the Docker whale icon. This icon provides quick access to Docker’s settings and status. One important setting to configure is the Shared Drive option, which allows Docker to access files on your local machine. 2.3 Enabling Kubernetes With Docker up and running, enabling Kubernetes is a straightforward process: Open Docker’s settings menu. Navigate to the Kubernetes tab. Check the box to enable Kubernetes and click Apply. This action will initiate the download of Kubernetes cluster components, which might take a few minutes depending on your internet connection. Once completed, Kubernetes will start running in the background, ready for use. 2.4 Installing and Configuring kubectl The next step involves installing kubectl, the command-line tool used for interacting with Kubernetes clusters. To do this: Search for “kubectl download” and follow the link to the official Kubernetes documentation. Download the kubectl executable and place it in a directory of your choice, such as C:\\kubectl. Add the path to this directory to your system’s Environment Variables so that you can run kubectl commands from any command prompt. After setting the path, restart any open command-line windows and type kubectl to verify that the installation was successful. You should see a list of commands available within kubectl. 2.5 Connecting kubectl to Your Kubernetes Cluster With kubectl installed, it’s time to connect it to your Kubernetes cluster. By default, kubectl is configured to point to the Kubernetes cluster created by Docker for Windows. You can verify this by running the command: kubectl config current-context This command should return “docker-for-desktop,” indicating that kubectl is configured to interact with your local Kubernetes cluster. To further verify the connection, you can check the status of the nodes and pods in your cluster with the following commands: kubectl get nodes kubectl get pods At this point, your Kubernetes cluster is fully operational, and you’re ready to start deploying applications."
  },"/dev/os/docker/3_kube_2_kubectl/": {
    "title": "2. kubectl",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_2_kubectl/",
    "body": "1. config we’ll explore how to use Kubectl to manage your Kubernetes clusters, focusing on configuration, retrieving resources, working with namespaces, describing resources, and checking versions. 1.1 Configuring Kubectl for Multiple Clusters Kubectl can point to multiple Kubernetes clusters, making it easy to switch between development, staging, and production environments. The configuration is managed through contexts, which are stored in a kubeconfig file. Here’s how you can configure and switch contexts: 1. Checking Your Current Context To find out which Kubernetes cluster your Kubectl is currently pointing to, use the following command: kubectl config current-context This command outputs the name of the current context, indicating the active cluster we are pointing at. In Kubernetes cluster: http end point context: the cluster and the user for the cluster the kubeconfig file can be found in C:\\Users\\&lt;username&gt;\\.kube 2. Listing All Available Contexts To see all the contexts available in your kubeconfig file, run: kubectl config get-contexts This command displays a table of all contexts, clusters, and users, with a * next to the currently active context. Output: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * docker-desktop docker-desktop docker-desktop 3. Switching to a Different Context To switch to another context, such as a production environment, use: kubectl config use-context [context-name] Replace [context-name] with the name of the context you want to switch to. This command changes the active cluster to the specified context. 4. Using Multiple Kubeconfig Files If you manage multiple clusters with different configuration files, you can temporarily set the KUBECONFIG environment variable: export KUBECONFIG=$HOME/.kube/config:$HOME/.kube/config2 This command merges the configurations from both files, allowing Kubectl to use them simultaneously. 1.2 Retrieving Kubernetes Resources One of the primary tasks in managing Kubernetes clusters is retrieving information about the resources. Kubectl provides several commands for this purpose: 1. Listing Pods To list all pods in the current namespace, use: kubectl get pods This command outputs a table with pod names, statuses, and other details. 2. Listing Deployments To list all deployments in the current namespace, run: kubectl get deployments This command provides information about all deployments, including the number of replicas and their current status. 3. Listing Services To see all services running in the current namespace, use: kubectl get services This command displays a table listing each service’s name, type, cluster IP, external IP, and ports. 4. Listing ConfigMaps To list all config maps, which store non-confidential data in key-value pairs, use: kubectl get configmaps This command shows a table with all config maps in the current namespace. 5. Listing Secrets To view all secrets, which store sensitive information like passwords and tokens, run: kubectl get secrets This command outputs a table of all secrets, showing their names and types. 1.3 Working with Namespaces Namespaces in Kubernetes allow you to create virtual clusters within a physical cluster, enabling better resource management and isolation. 1. Listing All Namespaces To see all namespaces in your cluster, use: kubectl get namespaces This command lists all namespaces, showing their names and statuses. 2. Creating a New Namespace To create a new namespace called test, run: kubectl create namespace test This command creates a new namespace that can be used to isolate resources. 3. Listing Pods in a Specific Namespace To list all pods within a specific namespace, use: kubectl get pods -n [namespace-name] Replace [namespace-name] with the name of the namespace you’re interested in. This command outputs a table of pod details for the specified namespace. Example PS&gt; kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-76f75df574-45v4c 1/1 Running 0 6d14h coredns-76f75df574-mqxf4 1/1 Running 0 6d14h etcd-docker-desktop 1/1 Running 0 6d14h kube-apiserver-docker-desktop 1/1 Running 0 6d14h kube-controller-manager-docker-desktop 1/1 Running 0 6d14h kube-proxy-6k46f 1/1 Running 0 6d14h kube-scheduler-docker-desktop 1/1 Running 0 6d14h storage-provisioner 1/1 Running 0 6d14h vpnkit-controller 1/1 Running 0 6d14h 1.4 Describing Resources For a more in-depth look at the state and events related to specific resources, Kubectl’s describe command is invaluable. 1. Describing a Pod To get detailed information about a specific pod, including its status, events, and resource usage, use: kubectl describe pod [pod-name] -n [namespace-name] Replace [pod-name] and [namespace-name] with the appropriate pod and namespace names. This command provides a wealth of information useful for debugging and monitoring. 2. Describing a Node To investigate the details of a specific node, especially when facing infrastructure issues, run: kubectl describe node [node-name] This command provides comprehensive information about the node, including its conditions, capacity, and recent events. 1.5 Checking Versions Knowing the versions of both your Kubectl client and Kubernetes server is crucial, especially when managing clusters with different versions. 1. Checking Kubectl and Kubernetes Versions To see the version of your Kubectl client and the Kubernetes server, use: kubectl version This command outputs the client version (Kubectl) and the server version (Kubernetes cluster)."
  },"/dev/os/docker/3_kube_3_deployments/": {
    "title": "3. Deployments",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_3_deployments/",
    "body": "3.1 Understanding Nodes and Pods in Kubernetes Kubernetes, often referred to as K8s, is a powerful system for automating the deployment, scaling, and management of containerized applications. Two of the most fundamental concepts in Kubernetes are nodes and pods. Let’s break these down to understand their roles and how they work together within a Kubernetes cluster. 1. What is a Node in Kubernetes? A node is a worker machine in Kubernetes. It can be either a physical server or a virtual machine, depending on the environment in which Kubernetes is running. Each node in a Kubernetes cluster runs the services necessary to manage the networking between the containers, the communication with the master, and the assigned workloads. Key components of a node include: Kubelet: An agent that runs on each node and ensures that the containers are running in a pod. Kube-proxy: A network proxy that runs on each node, maintaining network rules and enabling communication between different parts of the cluster. Container Runtime: The software responsible for running containers. Docker is the most commonly used runtime, but alternatives like containerd or CRI-O can also be used. Example: Imagine you have a Kubernetes cluster running on three virtual machines (VMs). Each VM represents a node in the cluster. When you deploy an application, Kubernetes schedules the application’s components (containers) to run on one or more of these nodes. 2. What is a Pod in Kubernetes? A pod is the smallest and simplest Kubernetes object. It represents a single instance of a running process in your cluster. Pods are the unit of deployment in Kubernetes and can contain one or more tightly coupled containers. All containers in a pod share the same network namespace (IP address and port space) and can communicate with each other directly using localhost. Key characteristics of a pod: Single or Multiple Containers: A pod can run a single container, but it can also run multiple containers that need to work together (e.g., a web server container and a logging container). Shared Resources: Containers within a pod share storage volumes and a network IP, allowing them to communicate easily and efficiently. Lifecycle Management: Pods are ephemeral, meaning they can be created and destroyed as needed. Kubernetes ensures that the desired number of pods are running at all times. Example: Consider an e-commerce application where you have a web server and a Redis cache. You might deploy both the web server and Redis in a single pod. Since they share the same IP address, the web server can easily connect to Redis using localhost. If the web server fails, Kubernetes can restart the pod to ensure that the application remains available. 3. How Nodes and Pods Work Together When you deploy an application in Kubernetes, the platform schedules pods to run on nodes. For example, if you have three nodes and deploy an application that requires five instances (pods), Kubernetes might place two pods on the first node, two on the second node, and one on the third node, depending on resource availability and other factors like load balancing. Example Scenario: Suppose you have a Kubernetes cluster with three nodes. You want to deploy a microservice-based application that has the following components: Frontend (web server) Backend (API server) Database (PostgreSQL) You define each of these components as a separate pod. Kubernetes schedules these pods across the available nodes. If one of the nodes goes down, Kubernetes can reschedule the pods to run on the remaining healthy nodes, ensuring the application remains up and running. 4. Keypoints Nodes are the worker machines in a Kubernetes cluster that run your applications. Pods are the smallest deployable units in Kubernetes, representing a single instance of a running process. Nodes and pods are the building blocks that Kubernetes uses to manage, scale, and ensure the high availability of applications. Understanding these concepts is crucial for anyone working with Kubernetes, as they are fundamental to how the platform operates and handles containerized workloads. 3.2 Kubernetes Deployment https://youtu.be/DMpEZEakYVc In the ever-evolving landscape of software development and deployment, Kubernetes has become a cornerstone for managing containerized applications at scale. In this guide, we will dive into the basics of Kubernetes deployments, exploring how they enable the management of desired states and how to effectively use Kubernetes tools to ensure smooth operations. 1. Introduction to Kubernetes Deployments Kubernetes is a powerful tool for automating the deployment, scaling, and management of containerized applications. In previous discussions, we explored setting up Kubernetes on Docker for Windows and using the command-line utility kubectl to interact with Kubernetes clusters. Now, with Kubernetes running and kubectl at our disposal, we’ll focus on the Kubernetes deployment object—a key component that allows us to configure and manage application deployments effectively. 2. What is a Kubernetes Deployment? A Kubernetes deployment is an object that defines the desired state of your application and the deployment strategy to achieve that state. The deployment controller in Kubernetes ensures that the actual state of your application aligns with the desired state, adjusting the infrastructure as needed. To understand the concept of deployment in Kubernetes, consider the traditional approach to managing production servers. Typically, this involves load-balanced servers running various scripts to maintain health and functionality. However, scripts are inherently fragile—they execute steps sequentially, and failure at any point can leave the system in an unstable state. 3. The Power of Desired State Kubernetes addresses the shortcomings of traditional scripting by leveraging the concept of desired state. Instead of relying on scripts to set up and manage servers, Kubernetes allows you to define the desired state of your application in a deployment file. For example, you might specify that version 1 of your application should always have at least two instances running for high availability and load balancing. Kubernetes then takes on the responsibility of ensuring that this desired state is maintained. This approach offers numerous advantages, including automatic handling of updates, seamless rollouts, and infrastructure resilience. Kubernetes can gracefully upgrade your application by draining traffic from the old version before introducing the new one. It can also automatically recover from infrastructure failures by redeploying your application on healthy nodes. Moreover, Kubernetes allows you to scale your application up or down to meet demand without worrying about port conflicts or underlying infrastructure. 4. Defining a Kubernetes Deployment A Kubernetes deployment is defined using a YAML file that specifies the desired state of your application. Key elements of a deployment YAML file include: Application Version: The version of your application to deploy. Replicas: The number of instances (pods) to run for high availability. Strategy: The deployment strategy, such as a rolling update, which ensures a smooth transition between application versions. Ports: The ports to expose for your application. Liveliness Probe: A health check that allows Kubernetes to monitor the application and restart it if necessary. Resource Limits: The CPU and memory limits that Kubernetes uses to make intelligent scheduling decisions and manage resources effectively. 5. Understanding Pods in Kubernetes At the core of Kubernetes is the concept of a pod. A pod is the smallest deployable unit in Kubernetes and can be thought of as a virtual machine hosting your application. A pod can contain one or more containers, which run the actual processes (applications) you want to deploy. For example, a pod might contain your main application along with a Redis cache and a monitoring agent, all working together in the same environment. 6. Deploying with kubectl To deploy your application using Kubernetes, you use the kubectl command-line tool. The basic process involves applying your deployment YAML file with the command: kubectl apply -f &lt;path-to-deployment-yaml&gt; This command sends your desired state to Kubernetes, which then manages the deployment process, ensuring that your application is running as specified. You can also monitor and troubleshoot deployments using commands like: kubectl get deployments to check the status of your deployment. kubectl describe deployment &lt;deployment-name&gt; to get detailed information about your deployment, including statuses, resource usage, and events. kubectl get pods to view the status of individual pods. kubectl logs &lt;pod-name&gt; to view logs for troubleshooting application errors. 3.3 Example Here’s an example of a simple Kubernetes Deployment YAML file that defines a deployment for an Nginx web server. This YAML file describes how Kubernetes should create and manage the pods running the Nginx container. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 # Number of desired pods selector: matchLabels: app: nginx # Label selector for pods template: metadata: labels: app: nginx # Label applied to pods spec: containers: - name: nginx image: nginx:1.21.6 # Image version ports: - containerPort: 80 # Exposing port 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 10 resources: limits: memory: \"256Mi\" cpu: \"500m\" requests: memory: \"128Mi\" cpu: \"250m\" Breakdown of the Deployment YAML File apiVersion: Specifies the API version of the Kubernetes object (in this case, apps/v1 is used for Deployments). kind: Defines the type of Kubernetes object (Deployment). metadata: Contains information about the deployment, including its name (nginx-deployment) and labels (app: nginx). spec: Describes the desired state of the deployment. replicas: The number of pod replicas (3) that should be running. selector: A label selector to identify the pods managed by this deployment. template: The pod template that describes the pods to be created. metadata: Contains labels for the pods. spec: Defines the containers to run inside the pod. containers: Specifies the container to run: name: The name of the container. image: The container image to use (nginx:1.21.6). ports: The ports to expose (containerPort: 80). livenessProbe: Defines a probe that checks if the container is healthy. If the probe fails, Kubernetes will restart the container. resources: Specifies resource requests and limits: limits: The maximum resources the container can use. requests: The minimum resources the container needs. How to Apply This Deployment To deploy this YAML file, save it as nginx-deployment.yaml and use the following command: kubectl apply -f nginx-deployment.yaml This command will create the deployment, and Kubernetes will start managing the pods as specified. 3.4 Example Redis Client Server To set up a Redis client and server using Kubernetes Deployments, you can define two separate Deployment YAML files: one for the Redis server and another for a simple Redis client. Here’s how to do it: 1. Redis Server Deployment This Deployment creates a Redis server that listens on port 6379. apiVersion: apps/v1 kind: Deployment metadata: name: redis-server labels: app: redis spec: replicas: 1 selector: matchLabels: app: redis tier: backend template: metadata: labels: app: redis tier: backend spec: containers: - name: redis image: redis:6.2.6 # Redis image version ports: - containerPort: 6379 # Exposing Redis port --- apiVersion: v1 kind: Service metadata: name: redis-service labels: app: redis spec: selector: app: redis tier: backend ports: - protocol: TCP port: 6379 targetPort: 6379 type: ClusterIP # Internal access only 2. Redis Client Deployment This Deployment creates a simple Redis client that can connect to the Redis server. apiVersion: apps/v1 kind: Deployment metadata: name: redis-client labels: app: redis-client spec: replicas: 1 selector: matchLabels: app: redis-client template: metadata: labels: app: redis-client spec: containers: - name: redis-client image: redis:6.2.6 # Using the Redis image to access the Redis CLI command: [ \"sh\", \"-c\", \"while true; do sleep 30; done;\" ] # Keeps the pod running 3. How to Connect the Client to the Server After deploying both the Redis server and client, you can connect to the Redis client pod and use the Redis CLI to connect to the Redis server. Here’s how: Get the name of the Redis client pod: kubectl get pods -l app=redis-client Use kubectl exec to open a shell in the Redis client pod: kubectl exec -it &lt;redis-client-pod-name&gt; -- /bin/sh From within the client pod, use the Redis CLI to connect to the Redis server: redis-cli -h redis-service The -h redis-service flag tells the Redis client to connect to the Redis server using the service name redis-service. Breakdown of the YAML Files Redis Server Deployment: The Redis server is deployed with one replica. The server is exposed internally within the cluster using a ClusterIP service. Redis Client Deployment: The Redis client is deployed with one replica. The client pod contains the Redis CLI, which can be used to connect to the Redis server. By running these Deployments, you’ll have a Redis server and a client running in your Kubernetes cluster, allowing you to test Redis interactions within the cluster."
  },"/dev/os/docker/3_kube_4_services/": {
    "title": "3. Deployments",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_4_services/",
    "body": "https://youtu.be/xhva6DeKqVU Linking Local PC Network with Kube Network To link a Kubernetes service network with your local PC’s network, allowing you to access services running inside the Kubernetes cluster, you have several options. The best approach depends on your environment (e.g., whether you’re running Kubernetes on your local machine, in a cloud, or on-premise). Here’s how you can do it: 1. Exposing Services via NodePort (Simple Approach) How it works: A NodePort exposes the service on a specific port of each node’s IP address. Your local PC can access the service using the node’s IP address and the NodePort. Limitations: The service is available on all nodes, but it’s exposed on a higher port (range 30000-32767). Steps: Create a NodePort service: apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 # Port on the service targetPort: 80 # Port on the container nodePort: 30007 # Exposed port on the node (optional) type: NodePort Apply the YAML: kubectl apply -f service.yaml Access the service: From your PC’s browser, go to http://&lt;NodeIP&gt;:30007, where &lt;NodeIP&gt; is the IP of one of the nodes in the cluster. 2. Using LoadBalancer (Cloud/Managed Kubernetes) How it works: If you’re running Kubernetes on a cloud platform, using a LoadBalancer service is the easiest way to expose the service externally. Limitations: Requires cloud provider support and may incur additional costs. Steps: Create a LoadBalancer service: apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancer Apply the YAML: kubectl apply -f service.yaml Access the service: Get the external IP using kubectl get services. Access it from your PC’s browser via http://&lt;External-IP&gt;. 3. Port Forwarding (Development/Testing) How it works: Port forwarding allows you to map a local port on your PC to a port on a Kubernetes service or pod. Limitations: Temporary and only works as long as the kubectl port-forward command is running. Steps: Run port forwarding: kubectl port-forward service/my-service 8080:80 This maps port 8080 on your local PC to port 80 on the service. Access the service: Open your browser and go to http://localhost:8080. 4. Using Ingress (Advanced with DNS) How it works: An Ingress resource provides external access to services in the cluster, often with HTTP/HTTPS and DNS support. Limitations: Requires an Ingress controller like NGINX, Traefik, or another cloud-provided controller. Steps: Deploy an Ingress controller (if not already available). Create an Ingress resource to expose your service. ```yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: host: myapp.example.com http: paths: path: / pathType: Prefix backend: service: name: my-service port: number: 80 ``` Apply the YAML: kubectl apply -f ingress.yaml Access the service: Ensure the DNS for myapp.example.com points to the ingress IP. Access via http://myapp.example.com. Keypoints For local development: Use kubectl port-forward. For simple access: Use a NodePort service. For production: Use a LoadBalancer service or Ingress. Each of these methods should enable your PC to communicate with services inside your Kubernetes cluster."
  },"/dev/os/docker/3_kube_5_config_secret/": {
    "title": "3. Config and Secret Management",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_5_config_secret/",
    "body": "Config Management https://youtu.be/o-gXx7r7Rz4 Secret Management https://youtu.be/o36yTfGDmZ0"
  },"/dev/os/iotplatform/1_tensorboard/": {
    "title": "1. Home Assistant",
    "keywords": "iotplatform",
    "url": "/dev/os/iotplatform/1_tensorboard/",
    "body": "Content is Coming Soon…"
  }}
