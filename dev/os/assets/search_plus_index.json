{"/dev/os/windows/1_windows10/": {
    "title": "1. Windows 10",
    "keywords": "windows",
    "url": "/dev/os/windows/1_windows10/",
    "body": "Windows 10 is the latest operating system developed by Microsoft, succeeding Windows 8. It combines the familiar features of Windows 7 with the modern design elements of Windows 8, offering a more cohesive and user-friendly experience. Windows 10 introduces a range of enhancements, including the return of the Start Menu, integration of virtual assistant Cortana, improved security features such as Windows Defender, enhanced multitasking capabilities with virtual desktops, and compatibility with a wide range of devices including PCs, tablets, and smartphones. It also provides regular updates and support, ensuring users have access to the latest features and security patches. Overall, Windows 10 aims to provide a seamless and intuitive computing experience for both consumers and enterprise users. 1. Key Highlights 1. Start Menu Revival: Windows 10 brings back the Start Menu with a blend of the classic Windows 7 layout and Windows 8’s Live Tiles. This provides users with a familiar yet modern interface. 2. Cortana Integration: Microsoft’s virtual assistant, Cortana, is integrated into Windows 10, offering voice commands, reminders, and search functionalities to enhance productivity. 3. Microsoft Edge Browser: Windows 10 introduces Edge, a modern web browser designed to replace Internet Explorer, featuring enhanced performance, security, and compatibility with modern web standards. 4. Virtual Desktops: Users can create multiple virtual desktops to manage and organize open applications, improving workflow and multitasking capabilities. 5. Action Center: This feature provides a centralized location for notifications and quick access to settings, improving user awareness and control over their system. 6. Universal Apps: Windows 10 supports universal apps that run across all devices in the Windows ecosystem, including PCs, tablets, and smartphones, ensuring a consistent user experience. 7. Continuum Mode: This feature allows devices to switch between desktop and tablet modes seamlessly, optimizing the interface based on the type of device and its usage. 8. Enhanced Security: Windows 10 includes Windows Hello, a biometric authentication system, and improved security features such as Device Guard and Secure Boot to protect against modern threats. 9. DirectX 12: For gamers, Windows 10 includes DirectX 12, which offers improved performance and graphics capabilities, making it a significant upgrade for gaming. 10. Windows Update for Business: This feature provides IT administrators with more control over the deployment of updates, allowing for phased rollouts and scheduled maintenance. 2. Technical Aspects Windows 10 offers a range of features and improvements that enhance user experience, security, and productivity. For IT professionals, understanding the deployment, security, and management capabilities of Windows 10 is crucial for effectively supporting and optimizing enterprise environments. By leveraging these technical aspects, IT teams can ensure a smooth transition to Windows 10 and maintain robust, secure, and efficient IT infrastructure. 1. Deployment and Migration: Windows Imaging and Configuration Designer (ICD): Used to create and deploy custom images of Windows 10. Windows Autopilot: A set of technologies to simplify and automate device setup and configuration. 2. Group Policy Management: Advanced Group Policy settings: Manage security, applications, and user settings. Administrative Templates: Configure various settings for Windows components. 3. Windows Update Management: Windows Update for Business: Control update deployment, defer feature updates, and manage update rollouts. WSUS (Windows Server Update Services): Centralized update management for enterprise environments. 4. Security Features: Windows Defender Advanced Threat Protection (ATP): Provides robust threat detection and response capabilities. BitLocker: Full disk encryption to protect data on lost or stolen devices. Credential Guard: Uses virtualization-based security to protect credential information. 5. Networking: DirectAccess and VPN: Simplified remote access solutions. Wi-Fi Sense: Facilitates easier connection to Wi-Fi networks but requires careful configuration to avoid security risks. 6. Device Management: Microsoft Intune: Cloud-based service for managing devices and applications. Mobile Device Management (MDM): Manage a range of devices from a unified interface. 7. Hyper-V: Virtualization Platform: Allows running virtual machines on Windows 10. Containers: Support for Windows containers and Docker for modern application deployment. 8. PowerShell: Automation: Use PowerShell scripts to automate administrative tasks. Remoting: Manage remote systems through PowerShell, providing powerful management capabilities. 9. Windows Subsystem for Linux (WSL): Run Linux Distros: Allows IT professionals to run native Linux command-line tools directly on Windows. Development Environment: Provides a seamless development environment for cross-platform applications. 10. Compatibility and Legacy Support: - Application Compatibility Toolkit (ACT): Helps to identify and mitigate application compatibility issues. - Enterprise Mode: Allows legacy web apps to run in Internet Explorer mode within Edge. 3. Sharing Internet This guide demonstrates how to share an internet connection from one computer to another using an Ethernet or LAN cable. It covers two methods using built-in features in Windows. 3.1 Method 1: Using Bridge Connections 1. Connect Host PC to Wi-Fi: Ensure the host computer is connected to a Wi-Fi network. 2. Open Network Connections: Access the network connections page on the host PC. 3. Setup Network Bridge: Select both the Wi-Fi and Ethernet network adapters. Right-click on the Wi-Fi adapter and select “Bridge Connections”. Right-click the newly created bridge adapter and select “Properties”. Ensure both Ethernet and Wi-Fi options are checked and click “OK”. 4. Connect Client PC: Use an Ethernet or LAN cable to connect the host PC to the client PC. 5. Verify Connection: The client PC should show an Ethernet icon in the taskbar, indicating a successful connection to the internet. 3.2 Method 2: Using Internet Connection Sharing (ICS) 1. Disconnect Network Bridge: If previously set, remove the network adapters from the bridge connection. 2. Connect Host and Client PCs: Ensure the host and client computers are connected via an Ethernet cable. 3. Configure Wi-Fi Adapter on Host PC: Right-click the Wi-Fi adapter and select “Properties”. Navigate to the “Sharing” tab. Check the box next to “Allow other network users to connect through this computer’s internet connection”. Under “Home networking connection”, select “Ethernet”. 4. Set Static IP Addresses: Due to potential issues with DHCP, it’s advisable to set private static IP addresses. On the host computer, right-click the Ethernet adapter and assign static IP addresses. Assign corresponding static IP addresses on the client computer. 5. Verify Internet Access: The client device should now have access to the internet. Software Purchase Grey Market https://consogame.com/software/windows/microsoft-windows-10-professional https://www.g2a.com/ https://www.scdkey.com/ https://www.cdkeys.com/ https://www.vip-scdkey.com/key/p202103081052146250.html https://www.bobkeys.com/software/p201609081840431227.html"
  },"/dev/os/windows/2_windows_server2019/": {
    "title": "2. Windows Server 2019",
    "keywords": "windows",
    "url": "/dev/os/windows/2_windows_server2019/",
    "body": "Content is Coming Soon… 1. Active Directory Active Directory (AD) is a directory service developed by Microsoft for Windows domain networks. It is included in most Windows Server operating systems. Active Directory stores information about objects on a network and makes this information available to users and network administrators. Here are the key components and functions of Active Directory: Directory Services: Active Directory stores information about users, computers, groups, and other resources on the network in a hierarchical, centralized database called the Active Directory Domain Services (AD DS) database. Domain Controller: A domain controller (DC) is a server that runs the Active Directory Domain Services role. It authenticates and authorizes all users and computers in a Windows domain, assigning and enforcing security policies for all computers and installing or updating software. Domains and Domain Trees: Active Directory organizes objects in a hierarchical structure called a domain. Domains can be grouped together to form domain trees, which can further be grouped into forests. Each domain in a forest maintains a two-way transitive trust relationship with every other domain in the forest, allowing users in one domain to access resources in another. User Authentication and Authorization: Active Directory authenticates and authorizes users and computers when they log in to the network. It verifies the identity of users and ensures they have the necessary permissions to access network resources. Group Policy: Active Directory uses Group Policy to define and enforce security settings and configurations for users and computers. Group Policy allows administrators to centrally manage and configure settings for all computers and users in a domain. LDAP Directory Services: Active Directory is based on the Lightweight Directory Access Protocol (LDAP), which provides a standardized way to access directory services. This allows applications and services to query and update information stored in Active Directory. DNS Integration: Active Directory relies heavily on Domain Name System (DNS) for name resolution. DNS is used to locate domain controllers, locate other services, and resolve domain names to IP addresses. Overall, Active Directory plays a critical role in managing and securing Windows-based networks, providing centralized authentication, authorization, and directory services to users and administrators. 2. Group Policy"
  },"/dev/os/windows/3.1_mingw/": {
    "title": "3.1. MinGW",
    "keywords": "windows",
    "url": "/dev/os/windows/3.1_mingw/",
    "body": "Content is Coming Soon…"
  },"/dev/os/windows/3.2_cygwin/": {
    "title": "3.2. Cygwin",
    "keywords": "windows",
    "url": "/dev/os/windows/3.2_cygwin/",
    "body": "Content is Coming Soon…"
  },"/dev/os/windows/3_wsl/": {
    "title": "3. Linux for Windows (WSL)",
    "keywords": "windows",
    "url": "/dev/os/windows/3_wsl/",
    "body": "Content is Coming Soon…"
  },"/dev/os/linux/1_ubuntu/": {
    "title": "1. Ubuntu",
    "keywords": "linux",
    "url": "/dev/os/linux/1_ubuntu/",
    "body": "Content is Coming Soon…"
  },"/dev/os/linux/2_kali_linux/": {
    "title": "2. Kali Linux",
    "keywords": "linux",
    "url": "/dev/os/linux/2_kali_linux/",
    "body": "Content is Coming Soon…"
  },"/dev/os/vmmachine/1_vmworkstation/": {
    "title": "1. VM Workstation",
    "keywords": "vmmachine",
    "url": "/dev/os/vmmachine/1_vmworkstation/",
    "body": "Content is Coming Soon…"
  },"/dev/os/vmmachine/2_windows_hyper_v/": {
    "title": "2. Windows Hyper-V",
    "keywords": "vmmachine",
    "url": "/dev/os/vmmachine/2_windows_hyper_v/",
    "body": "1 VMware overview VMware is a company that offers a range of virtualization products, such as VMware Workstation, VMware Fusion, VMware Server, and VMware ESXi. VMware Workstation and Fusion are desktop applications that let you create and run virtual machines on your Windows or Mac OS. VMware Server and ESXi are server applications that let you host multiple virtual machines on a single physical server. VMware is known for its reliability, performance, and compatibility with various operating systems and hardware. 2 Hyper-V overview Hyper-V is a virtualization feature that is built into Windows Server and Windows 10. Hyper-V lets you create and manage virtual machines using a graphical interface or a command-line tool. Hyper-V is designed to integrate well with other Windows features, such as Active Directory, PowerShell, and Remote Desktop. Hyper-V is also compatible with Linux and other operating systems, but may require some additional configuration. 3 Pros of VMware One of the main advantages of VMware is its wide support for different operating systems, including Windows, Linux, Mac OS, Solaris, FreeBSD, and more. VMware also has a large and active community of users and developers, who provide helpful resources, documentation, and support. VMware also offers advanced features, such as snapshots, cloning, live migration, and high availability, that can enhance your virtualization experience. VMware is also considered to be more stable and secure than Hyper-V, as it has fewer bugs and vulnerabilities. 4 Cons of VMware One of the main drawbacks of VMware is its cost. VMware products are not free, and you may need to purchase licenses, subscriptions, or support plans to use them. VMware also requires more resources than Hyper-V, as it runs on top of an existing operating system, rather than being part of it. VMware also has some compatibility issues with certain hardware and drivers, which may affect your performance or functionality. 5 Pros of Hyper-V One of the main benefits of Hyper-V is its affordability. Hyper-V is free for Windows Server and Windows 10 users, and you do not need to pay any extra fees to use it. Hyper-V also has a lower overhead than VMware, as it runs as a part of the Windows kernel, rather than as a separate application. Hyper-V also has some advantages over VMware in terms of integration with other Windows features, such as networking, security, and management. 6 Cons of Hyper-V One of the main disadvantages of Hyper-V is its limited support for non-Windows operating systems. Hyper-V may not work well with some Linux distributions or other operating systems, and you may need to install additional drivers or tools to make them run smoothly. Hyper-V also has a smaller and less active community than VMware, which means you may have less access to resources, documentation, and support. Hyper-V also lacks some of the advanced features that VMware offers, such as snapshots, cloning, and live migration."
  },"/dev/os/docker/3_docker_0_images/": {
    "title": "0. Images to Download",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_0_images/",
    "body": "OneDrive URL (Ubuntu VM - Docker Ready!) Download Here Ubuntu VM Credentials UN: Comp6017 PW: Comp6017"
  },"/dev/os/docker/3_docker_1_fundamentals/": {
    "title": "1. Fundamentals",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_1_fundamentals/",
    "body": "1. Docker vs Virtual Machines? Docker and virtual machines (VMs) are both technologies used to create isolated environments for running applications, but they have fundamental differences in terms of architecture, performance, and usage. Here’s a detailed comparison: 1.1 Architecture Virtual Machines: VMs run on a hypervisor, which can be either Type 1 (bare-metal) or Type 2 (hosted). Each VM includes a full operating system (OS) instance, which means that every VM has its own kernel and set of system libraries. VMs are more heavyweight due to the need for separate OS instances and the overhead of the hypervisor. Docker: Docker uses containerization technology, which leverages the host OS kernel. Containers share the host OS kernel and use isolated user spaces, making them much lighter than VMs. Docker containers package applications and their dependencies but do not include a full OS, just the necessary libraries and binaries. 1.2 Performance Virtual Machines: VMs are more resource-intensive because they require more CPU, memory, and storage to run multiple full OS instances. The hypervisor adds a layer of overhead, which can impact performance. Docker: Containers are more lightweight and efficient because they share the host OS kernel. Startup times for containers are typically much faster compared to VMs. Docker can achieve higher density of applications on the same hardware compared to VMs. 1.3 Isolation Virtual Machines: VMs provide strong isolation because each VM runs a completely separate OS instance. This isolation is more secure but at the cost of higher resource usage. Docker: Containers provide process-level isolation using namespaces and control groups (cgroups) in the Linux kernel. While still secure, container isolation is generally considered to be less strong than VM isolation since they share the same kernel. 1.4 Portability Virtual Machines: VMs are portable across different physical machines and can be migrated easily using tools provided by hypervisor vendors. Portability can be limited by differences in the underlying hardware and hypervisor features. Docker: Docker containers are highly portable and can run on any system that supports Docker, regardless of the underlying hardware or OS. Docker images can be easily shared and deployed across different environments using Docker Hub or private registries. 1.5 Use Cases Virtual Machines: Running multiple different operating systems on a single physical machine. Providing strong isolation for applications that require high security. Legacy applications that require a specific OS environment. Docker: Microservices architecture and modern application development. Continuous Integration/Continuous Deployment (CI/CD) pipelines. Applications that need to be lightweight, scalable, and portable. Summary Aspect Virtual Machines Docker Containers Architecture Full OS instance per VM Shared OS kernel Resource Usage High Low Performance Slower startup, higher overhead Fast startup, low overhead Isolation Stronger isolation Process-level isolation Portability Limited by hypervisor Highly portable Use Cases Multi-OS environments, strong security Microservices, CI/CD, scalability Both Docker and virtual machines have their own strengths and are suitable for different scenarios. Understanding these differences can help in choosing the right technology for a given use case. 2. What is a hypervisor? A hypervisor, also known as a virtual machine monitor (VMM), is software, firmware, or hardware that creates and manages virtual machines (VMs) by allowing multiple operating systems to share a single hardware host. Each operating system or virtual machine appears to have the host’s processor, memory, and other resources all to itself, but the hypervisor is actually controlling the host processor and resources, allocating what is needed to each operating system, and making sure that the guest operating systems (guest VMs) cannot disrupt each other. 2.1 Types of Hypervisors There are two main types of hypervisors: Type 1 Hypervisors (Bare-Metal Hypervisors): These hypervisors run directly on the host’s hardware to control the hardware and to manage guest operating systems. They do not require a separate underlying operating system. Examples include VMware ESXi, Microsoft Hyper-V, and Xen. Type 2 Hypervisors (Hosted Hypervisors): These hypervisors run on a conventional operating system (OS) just as other computer programs do. The guest operating system runs as a process on the host. Examples include VMware Workstation, Oracle VirtualBox, and Parallels Desktop. 2.2 Functions of a Hypervisor Resource Allocation: Hypervisors allocate resources like CPU, memory, storage, and network to each VM. Isolation: They provide isolation between VMs to ensure that the operation of one VM does not affect others. Virtual Hardware Emulation: Hypervisors emulate virtual hardware devices for VMs, allowing VMs to use abstracted hardware resources. VM Management: They provide tools and interfaces for creating, managing, and monitoring VMs. Migration: Hypervisors allow for live migration of VMs between hosts without downtime, which is useful for load balancing and maintenance. 2.3 Advantages of Using Hypervisors Consolidation: Multiple VMs can run on a single physical machine, improving hardware utilization. Isolation: Each VM operates independently, providing strong isolation for different workloads or tenants. Scalability: New VMs can be quickly created and deployed to scale applications. Flexibility: Different operating systems can be run on the same hardware platform. Disaster Recovery: Hypervisors can help in creating snapshots and backups of VMs, aiding in disaster recovery. 2.4 Disadvantages of Using Hypervisors Performance Overhead: Running multiple VMs on the same hardware can introduce performance overhead due to resource sharing and virtualization. Complexity: Managing a virtualized environment can be complex and may require specialized skills and tools. Security Risks: While hypervisors provide isolation, vulnerabilities in the hypervisor itself can potentially lead to security risks affecting all VMs. 2.5 Examples of Hypervisors Type 1 (Bare-Metal): VMware ESXi Microsoft Hyper-V Xen KVM (Kernel-based Virtual Machine) Type 2 (Hosted): VMware Workstation Oracle VirtualBox Parallels Desktop QEMU (Quick Emulator) IMPORTANT Hypervisors are fundamental to modern cloud computing, enabling the efficient use of hardware resources and the flexible deployment of virtualized workloads. 3. How is Docker Working ? Docker operates on a client-server architecture. Docker Client: The Docker client is the primary way users interact with Docker. It provides a command-line interface (CLI) that sends commands to the Docker daemon. The client can run on the same host as the daemon or connect to a remote daemon. Docker Daemon: The Docker daemon (dockerd) is a server that runs on the host machine. It is responsible for building, running, and managing Docker containers. The daemon listens for Docker API requests and manages Docker objects, such as images, containers, networks, and volumes. Docker REST API: The Docker API is used by the Docker client to communicate with the daemon. It can also be used by other applications to interact with the daemon. Docker Registry: This is a repository for Docker images. The Docker client can pull images from the registry to create containers and push images to the registry. Docker Hub is a popular public registry, but there are also private registries. In this architecture, the Docker client can interact with the Docker daemon over various protocols, such as UNIX sockets or network interfaces, enabling flexible deployment and management of containers. 4. Docker on Windows (Installation) Docker Desktop is a crucial tool for developers looking to containerize applications. The video from the Docker Mastery course offers a detailed walkthrough, now shared on YouTube for broader accessibility. Below are the steps and recommendations from the course for setting up Docker Desktop on Windows 10 or 11. 1. Downloading and Installing Docker Desktop Visit the Docker Desktop download page and download the installer. Run the executable to start the installation process. 2. Enabling WSL 2 Docker Desktop now uses WSL 2, a more efficient way to run Linux on Windows compared to the older Hyper-V setup. During installation, enable WSL 2 if it isn’t already enabled. The installer will guide you through this, including installing the necessary Linux kernel update. 3. Post-Installation Configuration After installation, launch Docker Desktop and follow the setup wizard. Agree to the End User License Agreement (EULA). Docker Desktop is free for learning and personal use, though some enterprise features may require a paid license. 4. Setting Up Visual Studio Code Download Visual Studio Code from its official website. Install Docker and Kubernetes extensions within Visual Studio Code for enhanced functionality. 5. Adjusting Docker Desktop Settings Access Docker Desktop settings by right-clicking the Docker icon in the system tray and selecting “Settings”. Configure your preferred settings, especially under the WSL integration section. Ensure your Linux distributions (e.g., Ubuntu) are enabled for Docker. 6. Creating a Docker ID Create a free Docker ID at hub.docker.com. Log in to Docker Desktop with your Docker ID to increase your pull rate limit from Docker Hub. 7. Cloning the Course Repository Clone the course repository into your WSL file system for better performance: git clone &lt;repository_url&gt; USING WINDOWS TERMINAL - NOT RECOMMENDED! Windows Terminal provides a modern interface for managing your command-line tools, supporting PowerShell, Command Prompt, and WSL distributions. Download it from the Microsoft Store if you’re on Windows 10, or use the pre-installed version on Windows 11. Customize the terminal to set your default profile to WSL for a seamless Docker experience. RECOMMENDED Use WSL2 distribution like Ubuntu to access docker instead of docker desktop or Windows terminal. Troubleshooting Tips Virtualization Errors: Ensure CPU virtualization features (VT-x) are enabled in your BIOS. Pull Rate Limits: Log in with your Docker ID to avoid hitting free-tier limits on Docker Hub. 5. Docker CLI Cheat Sheet Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Download Cheat Sheet"
  },"/dev/os/docker/3_docker_2_workflow/": {
    "title": "2. Workflow",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_2_workflow/",
    "body": "1. Creating a Docker Container Creating a Docker container that runs a Unix shell is straightforward and a common use case. Docker containers can run any process, including a shell, provided that the necessary binaries and libraries are included in the container image. Here’s a step-by-step guide on how to create and run a Docker container with a Unix shell: Step 1: Install Docker Ensure Docker is installed on your system. You can download and install Docker from the official Docker website. Step 2: Choose a Base Image Docker images are built from base images, which can include various Linux distributions. Common choices for Unix shell environments include: alpine: A minimal Docker image based on Alpine Linux. ubuntu: A more full-featured Docker image based on Ubuntu. Step 3: Create a Dockerfile A Dockerfile is a script that contains instructions on how to build a Docker image. Below are examples of Dockerfiles for both Alpine Linux and Ubuntu: Dockerfile for Alpine Linux # Use the official Alpine Linux image FROM alpine:latest # Install Bash shell RUN apk add --no-cache bash # Set the default command to run Bash CMD [\"bash\"] Dockerfile for Ubuntu # Use the official Ubuntu image FROM ubuntu:latest # Install Bash shell RUN apt-get update &amp;&amp; apt-get install -y bash # Set the default command to run Bash CMD [\"bash\"] Step 4: Build the Docker Image Navigate to the directory containing your Dockerfile and build the Docker image using the docker build command: # For Alpine Linux docker build -t alpine-bash . # For Ubuntu docker build -t ubuntu-bash . The -t flag tags the image with a name (alpine-bash or ubuntu-bash). Step 5: Run the Docker Container Once the image is built, you can run a container from the image using the docker run command: # For Alpine Linux docker run -it alpine-bash # For Ubuntu docker run -it ubuntu-bash The -it flags make the container interactive and allocate a pseudo-TTY, allowing you to interact with the shell. 1.1 Example: Running an Alpine Linux Shell Here’s a complete example of creating and running an Alpine Linux shell in a Docker container: Create a directory and a Dockerfile: mkdir alpine-shell cd alpine-shell nano Dockerfile Add the following content to the Dockerfile: FROM alpine:latest RUN apk add --no-cache bash CMD [\"bash\"] Build the Docker image: docker build -t alpine-bash . Run the Docker container: docker run -it alpine-bash TIP When starting the container, it will run the startup command ‘bash’ as specified in the docker file above! You will now be inside a Bash shell running in an Alpine Linux container. Some of linux commands to observe what’s going on inside the container. root@&lt;container-id&gt;# ls -al (observe the file system) root@&lt;container-id&gt;# ps -elf (running processes) If one explores the filesystem, there are no other shell or GUI in the file system (docker container linux is lightweight unlike VM). The bash process is the main process with PID=1. IMPORTANT Docker makes it easy to package and distribute environments, including Unix shells, ensuring consistency and isolation from the host system. 1.2 Additional Tips Networking You can connect the container to a network using the --network flag: docker run -it --network=my-network alpine-bash TIP Docker makes it easy to package and distribute environments, including Unix shells, ensuring consistency and isolation from the host system. 1.3 Dokerize an App A guide to put your application to docker and distribute. 1.4 Working with Existing Docker images 1.4.1 Creating / Running / Starting Container Docker hub provides docker images to start with. docker pull ubuntu # download the Ubuntu image docker run -it ubuntu # The command `run` # 1. creates a container with a random name # 2. starts the Ubuntu container with the default startup command specified in docker file (i.e bash) #- `-i`: Keeps STDIN open even if not attached. #- `-t`: Allocates a pseudo-TTY, which allows you to interact with the container. can run a custom startup command when running a container. docker run -it ubuntu sh # run Ubuntu container with a custom startup command that you want to run inside the container (i.e `sh`) docker run -it ubuntu top -b # The command that you want to run inside the container here is `top -b` docker run -it alpine-bash sh -c \"echo 'Hello, World!'\" # You can override the default command to run custom scripts or commands Start an existing container If you want to start an already existing container in interactive mode, you can use: docker start -i &lt;container_name_or_id&gt; This will attach to the container and allow you to interact with it. IMPORTANT Cannot run a command (like in run ... &lt;command to run inside container&gt;) when starting a container Start an existing container in detach mode To start an existing Docker container in detached mode: docker start &lt;container_name_or_id&gt; By default, Docker containers will run in the background unless they are designed to be interactive (e.g., by running a shell). If you want to reattach or interact with the running container, you can use: docker attach &lt;container_name_or_id&gt; # This will attach you to the PID=1 main process Run a command in a running container The docker exec -it command is used to run a command in a running Docker container interactively. This is often used to open a new shell session inside a container that is already running. To start a bash session inside a running container: docker exec -it &lt;container_name_or_id&gt; /bin/bash # Creates a new shell with bash and attach a pseudo terminal in the interactive mode docker exec: This part of the command is used to execute a command inside a running container. -i: This flag keeps STDIN open, allowing you to interact with the container. -t: This flag allocates a pseudo-TTY, which makes the command interactive (usually used for commands like /bin/bash or /bin/sh). IMPORTANT This command is useful when you need to inspect the container, troubleshoot, or manually run commands inside it. Enter into a running container You can enter a running container with: docker exec -it my_new_container /bin/bash you can replace bash with sh if bash is not available in the container. To attach to a running container later, use -a / –attach option: docker start -a my_new_container If you need to explicitly use a UID , like root = UID 0, you can specify this: docker exec -it -u 0 my_new_container /bin/bash # will log you as root 1.4.2 Exiting Container Lets run a container docker run -it ubuntu bash Exit without Stopping the container To exit the bash shell in the container one can, use the Ctrl+PQ option which will return the host system prompt but keeps the container running in the background. This operation detaches the container and allows you to return to your system’s shell without exiting from the only process (i.e bash) which is running inside the container. Hence the container will stay Running. Alternative Once inside the container, if you exit from the only process running inside the container (in this example, bash process), it will return to your system’s shell, and the container will be stopped. root@container_id# exit IMPORTANT A container must have a running process to stay alive! A container cannot exist if the main process is terminated. 2. Docker with Ubuntu vs Ubuntu VM? Docker provides the functionality of different operating systems, like Ubuntu, without running a full guest OS by leveraging a few key concepts and technologies. Here’s how Docker manages to provide the same functionalities using an Ubuntu image without installing the entire Ubuntu operating system: 2.1 Key Concepts and Technologies Containers vs. Virtual Machines: Virtual Machines (VMs): Each VM includes a full operating system along with the application and its dependencies, running on a hypervisor. This means each VM has its own kernel and OS resources. Containers: Containers, on the other hand, share the host system’s kernel and only include the application and its dependencies. Containers run as isolated processes on the host OS, using its kernel but maintaining their own filesystem, network, and process space. Union File Systems and Docker Layers: Docker images are built in layers. Each instruction in a Dockerfile creates a new layer. These layers are stacked and form a union filesystem, which means that the image only contains the necessary parts of the OS to run the application. For an Ubuntu image, this includes the necessary binaries, libraries, and tools that are part of the Ubuntu userland, but not the kernel. Namespaces and Cgroups: Namespaces: Provide isolated environments within the same OS instance. This includes process isolation (PID namespace), user isolation (user namespace), file system isolation (mount namespace), etc. Control Groups (Cgroups): Manage and limit resource usage (CPU, memory, disk I/O, etc.) of the containerized applications. Docker Images: When you pull an Ubuntu Docker image from Docker Hub, it includes the Ubuntu filesystem (binaries, libraries, etc.) but without the Ubuntu kernel. It relies on the host’s kernel to function. This means the image has everything needed to provide the “Ubuntu experience” (such as the apt package manager, bash shell, common utilities, etc.) without the overhead of a full OS. 2.2 Example of How an Ubuntu Image Works in Docker When you run an Ubuntu container, Docker uses the host OS kernel and provides the container with the Ubuntu filesystem: Download the Ubuntu Image: docker pull ubuntu Run the Ubuntu Container: docker run -it ubuntu Inside the container, you can run commands just like you would on a full Ubuntu system: root@container_id:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var You can use apt-get to install software, navigate the filesystem, and perform other tasks as if you were on an Ubuntu machine. How Docker Achieves This File System: The Ubuntu Docker image contains a minimal root filesystem that replicates the Ubuntu environment. This includes directories like /bin, /etc, /lib, /usr, etc., filled with the usual tools and libraries. Process Management: Docker containers run as isolated processes on the host system. The host’s kernel manages these processes, providing the necessary system calls and resource management. User Space Tools: The tools and applications you use within the container are from the Ubuntu user space. These tools are packaged in the Docker image, enabling you to interact with the container as if it were a standalone Ubuntu system. IMPORTANT Docker containers simulate a full OS environment by packaging the userland components (binaries, libraries, tools) of that OS while relying on the host’s kernel. This approach allows Docker to provide lightweight, efficient, and isolated environments that mimic the functionalities of a complete OS without the overhead of a full VM. 3. Docker Basics 3.1 Docker Crash Course Pulled docker images are stored in? Here is a list of the storage locations of the docker images on different operating systems: Ubuntu: /var/lib/docker/ Fedora: /var/lib/docker/ Debian: /var/lib/docker/ Windows: C:\\ProgramData\\DockerDesktop. MacOS: ~/Library/Containers/com. docker. docker/Data/vms/0/ 3.3 Docker Advance Docker Basic Commands 0. Docker Related docker ps running containers docker run –name debian-container-always -it –restart always debian:latest container restarts automatically if stopped due to exit from the main process. To end the session, clean up all the containers and images. docker container rm -f (sudo docker ps -aq) docker image rmi -f $(sudo docker images – aq) 1. Docker run -dit flag In Docker, the -dit flags are used in combination to run a container in the background (detached mode), while still providing interactive terminal capabilities and allocating a pseudo-TTY. Let’s break down the purpose of each flag and why they might be used together: 1. -d (detached mode): This flag runs the container in the background, returning control of the terminal to the user. The container continues to run even after the user logs out or closes the terminal session. 2. -i (interactive mode): This flag keeps the STDIN stream open, allowing the user to provide input to the container interactively. It is useful for cases where the container process requires user input or when running a process that reads from the terminal. 3. -t (pseudo-TTY): This flag allocates a pseudo-TTY, which provides an interface for terminal input and output. It makes the command prompt more user-friendly, with features like line editing. Why Use -dit Together? While it may seem counterintuitive to use -d (detached mode) and -it (interactive and TTY) together, there are specific scenarios where this combination is useful: 1. Starting Interactive Services in the Background: Sometimes, you want to start a service or an interactive application (like a shell or a REPL) in the background, but still be able to attach to it later if needed. For example, running a debugging tool or an interactive shell session in a container that you can later attach to using docker attach or docker exec. 2. Log and Debug: By using -it, you can ensure that the container has a proper terminal interface, which can be useful for logging and debugging. Even if the container is running in detached mode (-d), you can inspect logs or attach to the container to interact with it. 3. Daemon Processes with Interactive Control: Some daemon processes may provide an interactive control interface that can be accessed through a TTY. By using -dit, the container runs in the background, but you can still interact with the daemon if necessary. Example Scenario Suppose you’re running a container with a shell that needs to start and continue running in the background, but you also want the ability to interact with it later: docker run -dit --name my-shell-container ubuntu bash In this case: -d allows the container to run in the background. -i keeps the input open, which is necessary for interactive shells. -t provides a terminal interface, making the shell prompt user-friendly. You can later attach to this container using docker attach my-shell-container or execute a command interactively using docker exec -it my-shell-container bash. IMPORTANT Using -dit is not a common scenario, but it offers flexibility when you want a containerized process to run in the background while retaining interactive capabilities. 2. Get to docker container shell once detached Ref: https://stackoverflow.com/questions/30172605/how-do-i-get-into-a-docker-containers-shell There are at least 2 options depending on the target. 3. Docker commit The docker commit command is used to create a new Docker image from an existing container. This allows you to capture the current state of a container, including any changes made to it, and save it as a new image that can be used to create other containers. docker commit [OPTIONS] &lt;container_id_or_name&gt; &lt;new_image_name&gt; &lt;container_id_or_name&gt;: The ID or name of the container you want to commit. &lt;new_image_name&gt;: The name you want to give to the new image. Common Options: -m \"message\": Adds a commit message, similar to a Git commit message, describing what changes were made. -a \"author\": Specifies the author of the image (e.g., -a \"John Doe\"). -p: Pauses the container during the commit to ensure that the filesystem is in a consistent state. Example: Assume you have a running container with ID abc123 that you’ve made changes to, and you want to save those changes as a new image called my_custom_httpd: docker commit -m \"Customized Apache HTTPD configuration\" -a \"John Doe\" abc123 my_custom_httpd This command will create a new image named my_custom_httpd based on the current state of the abc123 container. You can then use this image to run new containers with the changes preserved. Verifying the New Image: To verify that the new image has been created, you can list your Docker images: docker images You should see my_custom_httpd in the list of available images. 3.1 Docker commit vs dockerfiles docker commit and Dockerfiles serve different purposes in Docker workflows, and each has its own advantages and use cases. Here’s a comparison of the advantages of using docker commit versus Dockerfiles: Advantages of docker commit 1. Quick Prototyping and Experimentation: Immediate Changes: docker commit allows you to quickly capture the current state of a running container, including changes made interactively (e.g., installing packages, modifying files). This can be useful for rapid prototyping or experimentation without needing to create a Dockerfile. Ease of Use: For developers who are experimenting with a new setup or configuration, docker commit can be a fast way to save the state of a container without writing a Dockerfile. 2. Snapshot of Current Container State: Capture Complex States: It’s useful when you have manually configured a container in ways that are hard to describe in a Dockerfile, such as complex runtime configurations or custom setup scripts. Preserve Manual Changes: If you’ve made manual changes inside a container (e.g., modifications to files or configurations) and want to save those changes as an image, docker commit captures those changes. 3. No Need for Dockerfile Syntax Knowledge: Simplicity for Non-Developers: Users who are not familiar with Dockerfile syntax or who prefer not to write Dockerfiles can use docker commit to create images from their containers without needing to learn Dockerfile commands. Advantages of Dockerfiles 1. Reproducibility: Consistent Builds: Dockerfiles provide a clear and repeatable method to build Docker images. This ensures that anyone building the image from the Dockerfile will get the same result, which is crucial for debugging and maintaining consistency across different environments. Version Control: Dockerfiles can be stored in version control systems (e.g., Git), allowing you to track changes over time and collaborate with others. 2. Documentation: Readable and Maintainable: Dockerfiles serve as documentation of how an image is built. They provide a human-readable and maintainable record of the installation and configuration steps. Best Practices: Dockerfiles encourage best practices by using well-defined instructions and following conventions that ensure better image layering and efficiency. 3. Automation and CI/CD Integration: Automated Builds: Dockerfiles can be used in continuous integration and deployment pipelines to automate image builds and deployments. Consistent Environments: Automated builds using Dockerfiles help maintain consistent environments across development, staging, and production. 4. Flexibility and Control: Custom Builds: Dockerfiles offer fine-grained control over the build process, allowing for optimization and customization that docker commit cannot provide. Complex Configurations: They enable the creation of complex images with multi-stage builds, conditional logic, and advanced configuration that docker commit does not support. In Summary Use docker commit when you need a quick snapshot of a running container’s state or when you are making interactive changes and need to preserve them without initially defining a Dockerfile. Use Dockerfiles for reproducibility, automation, documentation, and when you require a structured, maintainable way to build Docker images. IMPORTANT In most production scenarios, Dockerfiles are preferred due to their reproducibility and automation capabilities, while docker commit is more suited for ad-hoc tasks and experimentation. 4. Docker inspect The docker inspect command is used to obtain detailed information about Docker objects, such as containers, images, networks, or volumes. However, docker inspect doesn’t directly show the commit history of a Docker image or container in the way that, for example, Git shows commit history. Inspecting a Docker Image: If you want to see detailed information about a Docker image, including its layers and configuration, you can use: docker inspect &lt;image_name_or_id&gt; This command will display a JSON-formatted output with various details, such as the image’s ID, creation date, environment variables, command history, and more. Example: docker inspect my_custom_httpd Inspecting a Container: Similarly, to inspect a container, you can run: docker inspect &lt;container_name_or_id&gt; Example: docker inspect my_running_container 5. Docker history Viewing the Layers (Image History): If you want to see the layer-by-layer history of an image (which can give you insight into changes made via commits), you can use the docker history command: docker history &lt;image_name_or_id&gt; This command shows a list of layers, including the command that created each layer, the size of each layer, and when it was created. Example: docker history my_custom_httpd The output will show each layer of the image, which can help you understand what commands were run and what changes were made over time. However, it won’t show detailed commit messages like a version control system. To summarize, while docker inspect is useful for viewing detailed metadata and configuration, docker history is the command you would use to inspect the “commits” or layers that make up a Docker image. 4. Docker Volumes 4.1 Persisting Data: Following methods are available to persist data in docker 1. Mount a folder If you need to persist data, you can mount a volume using the -v flag: You can mount a folder in a host PC to the container as a volume docker run -it -v /host/path:/container/path alpine-bash 2. Named Volume First create the volume and then mount it. docker volume create named-volume docker run -dit --name ubuntu-named-vol --volume named-volume:/volume-mounted-from-host ubuntu:latest # or docker run -dit --name ubuntu-named-vol -v named-volume:/volume-mounted-from-host ubuntu:latest IMPORTANT The named volume created will stay in docker (not on the host PC). You can import content to the name volume created via docker and access it inside the container (vice versa). 3. Create a named volume on the fly Docker creates the named volume on the fly and mount it. How sudo docker run -dit --name ubuntu-named-vol –mount source=myvolume target=/volume-mounted-from-host ubuntu:latest # or docker run -dit --name ubuntu-named-vol -v named-volume:/volume-mounted-from-host ubuntu:latest 4. Additional Tips Mount a volume onto an existing directory You can also mount a volume for an existing data volumn / folder inside the container. Following will create a container and add the data volume at /var, a directory that is found in the base image. docker run -dit --name ubuntu-named-vol -v named-volume:/var ubuntu:latest 4.2 Share Volumes To share a volume between two Docker containers, you can create a Docker volume and then mount it to both containers. This allows the containers to read from and write to the same storage location, enabling them to share data. 4.2.1 Method 01 1. Create a Docker Volume: First, create a Docker volume that will be shared between the containers. docker volume create shared_volume This creates a named volume called shared_volume. 2. Run the First Container with the Volume: Run the first container and mount the volume to a specific directory inside the container. docker run -d --name container1 -v shared_volume:/shared_data my_image -d: Runs the container in detached mode. --name container1: Names the container container1. -v shared_volume:/shared_data: Mounts the shared_volume to the /shared_data directory inside the container. my_image: The name of the Docker image to run. 3. Run the Second Container with the Same Volume: Now, run the second container and mount the same volume to a directory inside it. docker run -d --name container2 -v shared_volume:/shared_data my_image This mounts the same shared_volume to the /shared_data directory inside the second container. Explanation: Both containers (container1 and container2) now share the same shared_volume mounted to the /shared_data directory. Any data written to /shared_data by one container will be accessible to the other container, allowing them to share files and data. Example Scenario: If container1 writes a file to /shared_data/myfile.txt, container2 will be able to read that file from the same location. This is useful in various scenarios, such as when you want two services running in separate containers to share configuration files, logs, or any other data. 4.2.2 Method 02 # Create a volume on the fly docker run -it --name=Container1 -v named-volume:/shared-data-vol ubuntu # Now, create another container that mounts the same volume from Container1 (using --volumes-from) docker run -it --name=Container2 --volumes-from Container1 ubuntu # This will show the volume `named-volume` (from Container1) inside Container2 --volumes-from &lt;source_container&gt;: Specifies the container from which volumes should be mounted. This is the container that has the volumes you want to share. 4.2.3 Additional Tips: You can list all your Docker volumes using: docker volume ls To get more details about the shared volume: docker volume inspect shared_volume This will show you where the volume is stored on your host system and other details. 4.3 Detach / Remove a Volume To detach a volume from a Docker container, you generally need to stop the container first and then remove the container. If you just want to detach the volume without removing it, you can do this by removing the container without deleting the associated volume. Steps are described below: 1. Stop the Container (Optional): If the container is running, stop it first: docker stop &lt;container_name_or_id&gt; 2. Remove the Container Without Removing the Volume: When removing the container, you can use the --volumes or -v flag to delete the associated volumes. To detach the volume and keep it intact, you should omit this flag: docker rm &lt;container_name_or_id&gt; This command will remove the container but will leave the volume intact. 3. Re-Attach the Volume to a New Container (Optional): If you want to use the same volume with another container, you can re-attach it when running a new container: docker run -d --name new_container -v shared_volume:/shared_data my_image Example: Let’s say you have a container named container1 using a volume called shared_volume: Stop the Container: docker stop container1 Remove the Container (Without Removing the Volume): docker rm container1 Re-Attach the Volume to Another Container: docker run -d --name container2 -v shared_volume:/shared_data my_image Note: If you use the docker rm -v &lt;container_name_or_id&gt; command, it will remove the container and any anonymous volumes associated with it, but named volumes like shared_volume will not be deleted unless explicitly removed with docker volume rm. The volume itself persists until you explicitly delete it with: docker volume rm shared_volume This approach allows you to detach a volume from one container and re-attach it to another, keeping the data intact and available. 4.4 Attach a Volume IMPORTANT Attaching a volume to an existing Docker container directly is not possible. Docker containers have a fixed configuration once they are created, which includes their volumes. However, you can achieve the desired result by creating a new container with the same configuration as the old one, including the new volume. 1. Create a New Container with the Additional Volume To simulate adding a volume to an existing container, you can create a new container based on the old one’s configuration and include the new volume mount. 1. Retrieve the Configuration of the Existing Container: First, get the details of the existing containe**r. You’ll need the image name and any necessary configuration details like environment variables, port mappings, etc. Use the following command to get the configuration: docker inspect &lt;existing_container_name_or_id&gt; Look for relevant details such as environment variables, ports, and volume mounts in the output. 2. Run a New Container with the Same Configuration and the New Volume: Use the information obtained from the inspect c**ommd to create a new container with the additional volume. docker run -d \\ --name &lt;new_container_name&gt; \\ --env &lt;env_vars&gt; \\ -p &lt;host_port&gt;:&lt;container_port&gt; \\ -v &lt;new_host_volume_path&gt;:&lt;new_container_volume_path&gt; \\ &lt;image_name&gt; Replace: &lt;new_container_name&gt;: a name for the new container. &lt;env_vars&gt;: environment variables if needed (format: VAR=value). &lt;host_port&gt;:&lt;container_port&gt;: port mappings if needed. &lt;new_host_volume_path&gt;: the new volume path on the host machine. &lt;new_container_volume_path&gt;: the path inside the container where the new volume will be mounted. &lt;image_name&gt;: the Docker image used by the existing container. Make sure to include any other configurations that were present in the old container. 3. Copy Data (if necessary): If you need to transfer data from the old container to the new one, you might need to manually cop**y fes. You can use docker cp to copy files between containers or between the host and containers. docker cp &lt;existing_container_name_or_id&gt;:&lt;path_in_container&gt; &lt;path_on_host&gt; docker cp &lt;path_on_host&gt; &lt;new_container_name&gt;:&lt;path_in_container&gt; 4. Verify the New Container: Check if the new container is running correctly and verify the volume mount: docker inspect &lt;new_container_name&gt; | grep Mounts -A 10 5. Remove the Old Container (optional): Once you’ve confirmed that the new container is working as expected, you can remove the old container if it’s no longer needed: docker rm &lt;existing_container_name_or_id&gt; This method essentially involves replicating the existing container with the addition of the new volume. While it’s not a direct modification of the existing container, it achieves the same result by creating a new container with the desired configuration. URL for images Ubuntu image - https://mega.nz/file/sSEDhZjJ#rLq-lkgW9dXwo1ca6oKwiQhKC_BvBi8hNd3EOr8g4U8 Comp6017 VM (credentials comp6017:comp6017) - https://mega.nz/file/Yel1gCLQ#5TvJjG3Gn1nceXadPLCK9dXR2onmxfjrMqO7fu3Bypg Redis with Docker https://www.docker.com/blog/how-to-use-the-redis-docker-official-image/ Docker Compose https://youtu.be/DM65_JyGxCo What is docker-compose? Let’s come back to docker-compose. Docker Compose is a tool you can use to define and share multi-container applications. This means you can run a project with multiple containers using a single source. For example, assume you’re building a project with NodeJS and MongoDB together. You can create a single image that starts both containers as a service – you don’t need to start each separately. Interesting right? And this solves the problem which I called out at the very beginning of this article. To achieve this we need to define a docker-compose.yml."
  },"/dev/os/docker/3_docker_3_networking/": {
    "title": "3. Networking",
    "keywords": "docker",
    "url": "/dev/os/docker/3_docker_3_networking/",
    "body": "1. Fundamentals of Networking 2. Linking Containers 2.1 Using a Bridge Network To link two Redis containers in Docker, where one acts as the database and the other as a command-line interface (CLI) client, you can use Docker’s network features to allow the containers to communicate with each other. Here’s a step-by-step guide: 1. Create a Docker Network: This will allow the containers to communicate with each other. 2. Run the Redis Database Container: Start the Redis database container and attach it to the network. 3. Run the Redis CLI Container: Start another container with the Redis CLI, also attached to the same network, and connect it to the Redis database. Step 1: Create a Docker Network First, create a custom bridge network: docker network create redis-net Step 2: Run the Redis Database Container Run the Redis database container on the created network: docker run -d --name redis-db --network redis-net redis Here, -d runs the container in detached mode, --name redis-db names the container, and --network redis-net attaches the container to the redis-net network. Step 3: Run the Redis CLI Container Run another container for the Redis CLI, connecting it to the database container: docker run -it --rm --network redis-net redis redis-cli -h redis-db In this command: -it makes the container interactive and attaches a terminal. --rm removes the container once it exits. --network redis-net attaches the container to the redis-net network. redis is the image name. redis-cli is the command to run the Redis CLI. -h redis-db specifies the hostname of the Redis server to connect to, which is the name of the database container. With this setup, the Redis CLI container will connect to the Redis database container. You can then run Redis commands interactively. Example Command Once inside the Redis CLI, you can run commands like: SET mykey \"Hello\" GET mykey This setup isolates the containers and ensures they can communicate securely within the same network. 2.2 Link without Creating Docker Network To link two Docker containers without using a Docker network, you can use the --link option. Following steps summarizes steps to link a Redis database container and a Redis CLI container using the --link option: DEPRECATED --link This feature has been deprecated in favor of using Docker networks, but it can still be used in some cases. Step 1: Run the Redis Database Container First, run the Redis database container: docker run -d --name redis-db redis Step 2: Run the Redis CLI Container with Link Next, run the Redis CLI container and link it to the Redis database container: docker run -it --rm --link redis-db:redis redis redis-cli -h redis In this command: -it makes the container interactive and attaches a terminal. --rm removes the container once it exits. --link redis-db:redis creates a link from the current container to the redis-db container and gives the alias redis for connecting to it. redis is the image name. redis-cli is the command to run the Redis CLI. -h redis specifies the hostname (alias) of the Redis server to connect to, which corresponds to the alias defined in the --link option. This setup will allow the Redis CLI container to connect to the Redis database container using the alias redis without needing to specify an IP address or custom network. Step 2 (Alternative) First create a redis container (for the CLI) with the link to redis-db, run bash command at the start. docker run --rm -it –-link redis-db:redis redis /bin/bash at the bash prompt (inside the container), run the command redis-cli to start the redis cli, connecting to the database running at port 6379 at the server host redis (alias which is given with the --link option) root@309-402356254:/data# redis-cli -h redis -p 6379 TIP During server container creation, you can connect to the Redis server using the specified Redis port (6379 is the usual default if otherwise specified)."
  },"/dev/os/docker/3_kube_1_fundamentals/": {
    "title": "1. Fundamentals",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_1_fundamentals/",
    "body": "1. Kubernetes and Docker Kubernetes and Docker are both crucial tools in the world of containerization, but they serve different purposes and are often used together. 1.1 Docker: A Containerization Platform Docker is a platform designed to simplify the process of building, deploying, and running applications using containers. Containers are lightweight, portable units that package an application and all of its dependencies, ensuring that it runs consistently across different environments—whether it’s on a developer’s local machine, in a testing environment, or in production. Key features of Docker: Containerization: Docker packages an application and its dependencies into a single container, which can run on any system with Docker installed. Portability: Since containers include everything needed to run an application, they can be easily moved across different environments. Isolation: Docker containers are isolated from each other, which helps in maintaining consistency and preventing conflicts between different applications or services. 1.2 Kubernetes: A Container Orchestration Tool Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate the deployment, scaling, and management of containerized applications. While Docker handles the creation and running of individual containers, Kubernetes takes things a step further by managing large numbers of containers across multiple hosts, ensuring that they work together effectively. Key features of Kubernetes: Orchestration: Kubernetes coordinates the scheduling and running of containers on a cluster of machines, making sure the right containers are running at the right time. Scaling: Kubernetes can automatically scale up or down the number of running containers based on demand. Load balancing: It distributes network traffic to ensure that no single container is overwhelmed with too many requests. Self-healing: Kubernetes can automatically restart containers that fail, replace them, and kill those that don’t respond to user-defined health checks. Rolling updates and rollbacks: Kubernetes allows you to update applications without downtime and roll back to previous versions if something goes wrong. 1.3 How Are Docker and Kubernetes Related? Docker and Kubernetes are closely related, but they serve different roles within the container ecosystem. Docker is used to create and manage individual containers, while Kubernetes is used to manage and orchestrate those containers at scale. Docker creates containers. Kubernetes manages those containers across a cluster of machines, handling tasks like load balancing, scaling, and ensuring high availability. Kubernetes often uses Docker as the container runtime. This means Docker is the tool that actually runs the containers, while Kubernetes orchestrates how and where those containers are deployed. In essence, while Docker is the engine that runs the containers, Kubernetes is the control system that manages many containers across multiple environments, ensuring they work together efficiently. 2. Getting Started In the world of containerization, Kubernetes and Docker are powerful tools that work together to simplify the process of deploying and managing applications. This guide walks through the steps required to set up Kubernetes on Docker for Windows, covering everything from prerequisites to initial configuration. 2.1 Prerequisites and Setup Before diving into Kubernetes, it’s essential to ensure that your system meets the necessary prerequisites. The first crucial step is to enable Hyper-V on your Windows machine. Hyper-V is the built-in hypervisor for Windows, and it’s required because Docker and Kubernetes, being Linux-based technologies, need to run within a Linux virtual machine on Windows. To enable Hyper-V: Open the Control Panel. Navigate to Programs and select Turn Windows Features on or off. Locate the Hyper-V section, check all related boxes, and click Apply and OK. If Hyper-V cannot be enabled through the control panel, it may be disabled in your system’s BIOS. This is particularly common in laptops, so you might need to enable it from the BIOS settings before proceeding. 2.2 Installing Docker for Windows Once Hyper-V is enabled, the next step is to install Docker for Windows. Begin by searching for “Docker for Windows” on Google, and follow the link to the official Docker website. The installation process involves creating a Docker Hub account, which is necessary for downloading Docker and managing containers. After the installation is complete, you can confirm that Docker is running by checking the system tray for the Docker whale icon. This icon provides quick access to Docker’s settings and status. One important setting to configure is the Shared Drive option, which allows Docker to access files on your local machine. 2.3 Enabling Kubernetes With Docker up and running, enabling Kubernetes is a straightforward process: Open Docker’s settings menu. Navigate to the Kubernetes tab. Check the box to enable Kubernetes and click Apply. This action will initiate the download of Kubernetes cluster components, which might take a few minutes depending on your internet connection. Once completed, Kubernetes will start running in the background, ready for use. 2.4 Installing and Configuring kubectl The next step involves installing kubectl, the command-line tool used for interacting with Kubernetes clusters. To do this: Search for “kubectl download” and follow the link to the official Kubernetes documentation. Download the kubectl executable and place it in a directory of your choice, such as C:\\kubectl. Add the path to this directory to your system’s Environment Variables so that you can run kubectl commands from any command prompt. After setting the path, restart any open command-line windows and type kubectl to verify that the installation was successful. You should see a list of commands available within kubectl. 2.5 Connecting kubectl to Your Kubernetes Cluster With kubectl installed, it’s time to connect it to your Kubernetes cluster. By default, kubectl is configured to point to the Kubernetes cluster created by Docker for Windows. You can verify this by running the command: kubectl config current-context This command should return “docker-for-desktop,” indicating that kubectl is configured to interact with your local Kubernetes cluster. To further verify the connection, you can check the status of the nodes and pods in your cluster with the following commands: kubectl get nodes kubectl get pods At this point, your Kubernetes cluster is fully operational, and you’re ready to start deploying applications."
  },"/dev/os/docker/3_kube_2_kubectl/": {
    "title": "2. kubectl",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_2_kubectl/",
    "body": "Deployments https://youtu.be/DMpEZEakYVc Configuration management https://youtu.be/o-gXx7r7Rz4 To make the Year 10 maths paper more challenging, I will incorporate more complex algebraic manipulations, higher-level trigonometry, and multi-step problem-solving questions. The questions will require deeper understanding and application of mathematical concepts. \\begin{enumerate} \\item \\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt, leftmargin=10pt] \\setlength\\itemsep{1em} \\item \\setstretch{1.5}\\$20,000 is invested for 3 years. Calculate interest earned on each of the following to find which would give the best return. \\begin{enumerate} \\item 5\\% p.a compounded quarterly \\item 4.5\\% p.a compounded every 2 months \\item 4\\% p.a compounded monthly \\end{enumerate} \\item Do you think the size of the investment would alter which option in part (a) would give the best return \\item Prove your answer to part (b) algebraically by redoing part (a) with \\$P in place of \\$20,000, and expressing your answer as a percentage of \\$P \\end{enumerate} \\item \\setstretch{1.5} 'The more frequent the compounding, the greater the interest' Explain what this statement means. Use a principal of \\$10,000 invested at 8\\% p.a for 5 years and three different compounding periods (such as weekly, quarterly or monthly) to help demonstrate your answer. Express the percentage increase correct to 1 decimal place. \\end{enumerate} \\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt, leftmargin=10pt] \\setlength\\itemsep{1em} \\item The number of earthworms in a compost heap after \\eq{d} days is given by \\eq[1]{100 - 4d - d^2}. \\begin{enumerate} \\item How many earthworms were there originally? \\item Factorize the expression for the number of earthworms. \\item Use your factorization to show that when \\eq[1]{d = 2(\\sqrt{26} - 1)}, no earthworms remain. \\item \\setstretch{1.5} Evaluate \\eq[1]{2(\\sqrt{26} - 1)} and round to 2 decimal places to find on which day the number of earthworms is reduced to zero. \\item A second colony of earthworms is now in another compost heap. Their population is given by \\eq[1]{64 - 2n - n^2}. \\begin{enumerate} \\item Factorize the expression. \\item \\setstretch{1.5} Use this factorization to show that no earthworms remain when \\eq[1]{n = \\sqrt{65} - 1} \\end{enumerate} \\item Use your answer in (c) and (e)(ii) to decide which colony stayed longer in their respective compost heaps. \\end{enumerate} \\end{enumerate} To make the Year 10 maths paper more challenging, I will incorporate more complex algebraic manipulations, higher-level trigonometry, and multi-step problem-solving questions. The questions will require deeper understanding and application of mathematical concepts. Year 10 Advanced Mathematics Exam Time Allowed: 2 Hours Total Marks: 100 Instructions: Write all your answers in the space provided. Show all working for full marks. Calculators are allowed. Check your answers carefully. Section A: Multiple Choice (20 marks) Simplify the expression: [ \\frac{3x^2 - 12}{x - 2} + \\frac{2x^2 - 8x + 8}{x - 2} ] a) ( 5x + 2 ) b) ( x + 2 ) c) ( 5x - 2 ) d) ( 5x - 4 ) (1 mark) Solve the equation for ( x ): [ \\sqrt{2x + 5} = x - 3 ] a) ( x = 4 ) b) ( x = 5 ) c) ( x = 1 ) d) ( x = -1 ) (1 mark) The area of a sector of a circle with radius 10 cm and angle ( 60^\\circ ) is: a) ( \\frac{100\\pi}{3} ) cm² b) ( \\frac{50\\pi}{3} ) cm² c) ( \\frac{25\\pi}{3} ) cm² d) ( \\frac{100\\pi}{6} ) cm² (1 mark) If ( f(x) = 3x^2 - 2x + 1 ), what is the value of ( f’(x) ) at ( x = 2 )? a) 10 b) 4 c) 12 d) 8 (1 mark) The probability of rolling a sum of 8 with two dice is: a) ( \\frac{5}{36} ) b) ( \\frac{7}{36} ) c) ( \\frac{1}{6} ) d) ( \\frac{1}{8} ) (1 mark) Section B: Short Answer (40 marks) Simplify the expression and factor completely: [ x^4 - 16 ] (4 marks) Solve the simultaneous equations: [ \\begin{aligned} 3x + 4y &amp;= 18 5x - 2y &amp;= 4 \\end{aligned} ] (6 marks) A triangle has sides of lengths 7 cm, 24 cm, and 25 cm. Prove whether it is a right-angled triangle, and find the area. (5 marks) Calculate the exact value of ( \\sin(75^\\circ) ) using trigonometric identities. (4 marks) The sum of the roots of the quadratic equation ( ax^2 + bx + c = 0 ) is 7, and the product of the roots is 12. Find the values of ( a ), ( b ), and ( c ). (6 marks) In a geometric sequence, the 3rd term is 24, and the 6th term is 192. Find the first term and the common ratio. (5 marks) Section C: Extended Response (40 marks) A rectangle is inscribed in a circle of radius 10 cm. a) Express the area of the rectangle as a function of the length of one side, ( x ). b) If the perimeter of the rectangle is 40 cm, find the dimensions of the rectangle that maximize its area. (12 marks) A student invested $1000 at an interest rate of 5% per annum, compounded annually. a) Write the formula for the amount ( A ) after ( n ) years. b) Calculate the time taken for the investment to double in value. c) How long would it take to triple the investment? (10 marks) The following data represents the scores of students in a test: 56, 63, 72, 80, 85, 90, 92, 96, 98, 100. a) Calculate the standard deviation of the data. b) If two students who scored 110 and 115 are added to the data, how does the mean and standard deviation change? (10 marks) A water tank is in the shape of a cone with a height of 12 m and a base radius of 5 m. a) Calculate the volume of the tank. b) If water is being pumped into the tank at a rate of 2 cubic meters per minute, how long will it take to fill the tank? c) If the tank is only partially filled to a depth of 6 m, calculate the volume of water inside. (8 marks) End of Paper This advanced version of the Year 10 maths paper includes more challenging algebra, trigonometry, and problem-solving questions that require a deeper understanding and the ability to apply mathematical concepts in a more complex and integrated manner. Here’s a challenging Year 9 mathematics exam designed to test students’ understanding of higher-level concepts while still aligning with the curriculum expectations: Year 9 Advanced Mathematics Exam Time Allowed: 2 Hours Total Marks: 100 Instructions: Write all your answers in the space provided. Show all working for full marks. Calculators are allowed. Check your answers carefully. Section A: Multiple Choice (20 marks) Simplify the expression: [ \\frac{2x^2 - 8}{x - 2} + \\frac{3x^2 - 18}{x - 3} ] a) ( 5x + 1 ) b) ( 5x - 1 ) c) ( 5x + 2 ) d) ( 5x - 2 ) (1 mark) If ( f(x) = 2x^2 - 3x + 5 ), what is the value of ( f(2) )? a) 11 b) 13 c) 9 d) 7 (1 mark) (1 mark) The probability of drawing a red ball from a bag containing 5 red balls, 7 blue balls, and 8 green balls is: a) ( \\frac{1}{4} ) b) ( \\frac{1}{5} ) c) ( \\frac{1}{3} ) d) ( \\frac{5}{20} ) (1 mark) Section B: Short Answer (40 marks) Expand and simplify the expression: [ (2x - 3)(x^2 + x - 2) ] (6 marks) Solve the simultaneous equations: [ \\begin{aligned} 2x + 3y &amp;= 7 5x - 2y &amp;= -1 \\end{aligned} ] (6 marks) A triangle has sides of lengths 9 cm, 12 cm, and 15 cm. Prove that the triangle is right-angled and find its area. (5 marks) Calculate the exact value of ( \\cos(45^\\circ) \\times \\tan(30^\\circ) ). (4 marks) A quadratic equation has roots ( x = 2 ) and ( x = -3 ). Form the quadratic equation in standard form. (4 marks) In a geometric sequence, the first term is 5 and the common ratio is 3. What is the 5th term of the sequence? (4 marks) The sum of the first ( n ) terms of an arithmetic sequence is given by ( S_n = 3n^2 + 2n ). Find the 10th term of the sequence. (5 marks) A rectangular garden has a length of ( (3x + 4) ) meters and a width of ( (2x - 1) ) meters. If the area of the garden is 150 m², find the value of ( x ). (6 marks) Section C: Extended Response (40 marks) A water tank is in the shape of a cylinder with a height of 10 m and a radius of 3 m. a) Calculate the volume of the tank. b) If the tank is filled to 60% of its capacity, how much water is in the tank? c) If water is being added to the tank at a rate of 2 cubic meters per minute, how long will it take to fill the tank completely from its current level? (12 marks) The following data shows the number of hours students spent studying for a test: 2, 4, 6, 6, 8, 10, 12. a) Calculate the mean and median of the data. b) If a student who studied for 14 hours is added to the data, how does the mean change? c) Calculate the standard deviation of the original data set. (10 marks) A rectangle is inscribed in a circle of radius 7 cm. a) Express the area of the rectangle as a function of the length of one side, ( x ). b) If the perimeter of the rectangle is 40 cm, find the dimensions of the rectangle that maximize its area. (10 marks) A box contains 3 red, 4 blue, and 5 green marbles. a) If two marbles are drawn at random without replacement, what is the probability that both marbles are red? b) What is the probability that one marble is red and the other is green? c) If three marbles are drawn at random without replacement, what is the probability that all three are of different colors? (8 marks) End of Paper This Year 9 maths paper is designed to challenge students with complex problem-solving and higher-order thinking questions, while still being accessible to those who have mastered the Year 9 curriculum."
  },"/dev/os/docker/3_kube_3_deployments/": {
    "title": "2. kubectl",
    "keywords": "docker",
    "url": "/dev/os/docker/3_kube_3_deployments/",
    "body": "1. config we’ll explore how to use Kubectl to manage your Kubernetes clusters, focusing on configuration, retrieving resources, working with namespaces, describing resources, and checking versions. 1.1 Configuring Kubectl for Multiple Clusters Kubectl can point to multiple Kubernetes clusters, making it easy to switch between development, staging, and production environments. The configuration is managed through contexts, which are stored in a kubeconfig file. Here’s how you can configure and switch contexts: 1. Checking Your Current Context To find out which Kubernetes cluster your Kubectl is currently pointing to, use the following command: kubectl config current-context This command outputs the name of the current context, indicating the active cluster. 2. Listing All Available Contexts To see all the contexts available in your kubeconfig file, run: kubectl config get-contexts This command displays a table of all contexts, clusters, and users, with a * next to the currently active context. 3. Switching to a Different Context To switch to another context, such as a production environment, use: kubectl config use-context [context-name] Replace [context-name] with the name of the context you want to switch to. This command changes the active cluster to the specified context. 4. Using Multiple Kubeconfig Files If you manage multiple clusters with different configuration files, you can temporarily set the KUBECONFIG environment variable: export KUBECONFIG=$HOME/.kube/config:$HOME/.kube/config2 This command merges the configurations from both files, allowing Kubectl to use them simultaneously. 1.2 Retrieving Kubernetes Resources One of the primary tasks in managing Kubernetes clusters is retrieving information about the resources. Kubectl provides several commands for this purpose: 1. Listing Pods To list all pods in the current namespace, use: kubectl get pods This command outputs a table with pod names, statuses, and other details. 2. Listing Deployments To list all deployments in the current namespace, run: kubectl get deployments This command provides information about all deployments, including the number of replicas and their current status. 3. Listing Services To see all services running in the current namespace, use: kubectl get services This command displays a table listing each service’s name, type, cluster IP, external IP, and ports. 4. Listing ConfigMaps To list all config maps, which store non-confidential data in key-value pairs, use: kubectl get configmaps This command shows a table with all config maps in the current namespace. 5. Listing Secrets To view all secrets, which store sensitive information like passwords and tokens, run: kubectl get secrets This command outputs a table of all secrets, showing their names and types. 1.3 Working with Namespaces Namespaces in Kubernetes allow you to create virtual clusters within a physical cluster, enabling better resource management and isolation. 1. Listing All Namespaces To see all namespaces in your cluster, use: kubectl get namespaces This command lists all namespaces, showing their names and statuses. 2. Creating a New Namespace To create a new namespace called test, run: kubectl create namespace test This command creates a new namespace that can be used to isolate resources. 3. Listing Pods in a Specific Namespace To list all pods within a specific namespace, use: kubectl get pods -n [namespace-name] Replace [namespace-name] with the name of the namespace you’re interested in. This command outputs a table of pod details for the specified namespace. 1.4 Describing Resources For a more in-depth look at the state and events related to specific resources, Kubectl’s describe command is invaluable. 1. Describing a Pod To get detailed information about a specific pod, including its status, events, and resource usage, use: kubectl describe pod [pod-name] -n [namespace-name] Replace [pod-name] and [namespace-name] with the appropriate pod and namespace names. This command provides a wealth of information useful for debugging and monitoring. 2. Describing a Node To investigate the details of a specific node, especially when facing infrastructure issues, run: kubectl describe node [node-name] This command provides comprehensive information about the node, including its conditions, capacity, and recent events. 1.5 Checking Versions Knowing the versions of both your Kubectl client and Kubernetes server is crucial, especially when managing clusters with different versions. 1. Checking Kubectl and Kubernetes Versions To see the version of your Kubectl client and the Kubernetes server, use: kubectl version This command outputs the client version (Kubectl) and the server version (Kubernetes cluster). Deployments https://youtu.be/DMpEZEakYVc"
  },"/dev/os/iotplatform/1_tensorboard/": {
    "title": "1. Home Assistant",
    "keywords": "iotplatform",
    "url": "/dev/os/iotplatform/1_tensorboard/",
    "body": "Content is Coming Soon…"
  }}
