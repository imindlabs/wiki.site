{"/ds/aml/image_analysis/1_ima_1_basics/": {
    "title": "1. Introduction",
    "keywords": "image_analysis",
    "url": "/ds/aml/image_analysis/1_ima_1_basics/",
    "body": "Guass and Median Filters https://medium.com/jun94-devpblog/cv-2-gaussian-and-median-filter-separable-2d-filter-2d11ee022c66#:~:text=Gaussian%20kernel%2C%20as%20its%20name,(pixels)%20in%20an%20image."
  },"/ds/aml/image_analysis/1_ima_2_filters/": {
    "title": "2. Filters",
    "keywords": "image_analysis",
    "url": "/ds/aml/image_analysis/1_ima_2_filters/",
    "body": "Guass and Median Filters https://medium.com/jun94-devpblog/cv-2-gaussian-and-median-filter-separable-2d-filter-2d11ee022c66#:~:text=Gaussian%20kernel%2C%20as%20its%20name,(pixels)%20in%20an%20image. 1. Image Gradient An image gradient is a directional change in the intensity or color in an image. It is a fundamental concept in image processing and computer vision, often used for edge detection, texture analysis, and object recognition. The gradient of an image measures how much and in which direction the intensity of the image is changing. 1.1 Key Concepts 1. Gradient at a Pixel: At a given pixel in an image, the gradient is a vector that points in the direction of the greatest rate of increase of intensity. The magnitude of this vector indicates the strength of the change, while the direction indicates the orientation of the edge or transition in the image. 2. Gradient Components: The image gradient is typically described by its components in the \\(x\\) (horizontal) and \\(y\\) (vertical) directions: \\(\\nabla I = \\left[ \\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y} \\right]\\) \\(\\frac{\\partial I}{\\partial x}\\) (denoted as \\(I_x\\)) is the gradient in the \\(x\\) direction. \\(\\frac{\\partial I}{\\partial y}\\) (denoted as \\(I_y\\)) is the gradient in the \\(y\\) direction. 3. Magnitude and Direction of Gradient: The magnitude of the gradient is given by: \\(\\text{Magnitude} = \\sqrt{I_x^2 + I_y^2}\\) The direction (angle) of the gradient is given by: \\(\\text{Direction} = \\theta = \\arctan\\left(\\frac{I_y}{I_x}\\right)\\) 4. Edge Detection: Gradients are commonly used in corner and edge detection algorithms because edges in an image are typically locations of high intensity change. The magnitude of the gradient will be high at edges and low in flat regions. 1.2 How to Compute the Gradient: The image gradient is usually computed using convolution with derivative kernels such as the Sobel operator, Prewitt operator, or Scharr operator. Sobel Operator: The Sobel operator is one of the most widely used methods to compute the gradient in an image. It uses convolution with the following kernels to calculate \\(I_x\\) and \\(I_y\\): Gradient in the \\(x\\) direction (horizontal): \\(I_x = \\begin{bmatrix} -1 &amp; 0 &amp; +1 \\\\ -2 &amp; 0 &amp; +2 \\\\ -1 &amp; 0 &amp; +1 \\end{bmatrix}\\) Gradient in the \\(y\\) direction (vertical): \\(I_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ +1 &amp; +2 &amp; +1 \\end{bmatrix}\\) 1.3 Implementation Here’s how you can compute the gradient of an image using the Sobel operator in OpenCV: import cv2 import numpy as np import matplotlib.pyplot as plt # Load the image in grayscale img = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE) # Compute the gradient in the x direction (horizontal edges) I_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) # Compute the gradient in the y direction (vertical edges) I_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) # Compute the gradient magnitude magnitude = np.sqrt(I_x**2 + I_y**2) # Compute the gradient direction direction = np.arctan2(I_y, I_x) # Display the results plt.subplot(1, 3, 1), plt.imshow(I_x, cmap='gray'), plt.title('Gradient X') plt.subplot(1, 3, 2), plt.imshow(I_y, cmap='gray'), plt.title('Gradient Y') plt.subplot(1, 3, 3), plt.imshow(magnitude, cmap='gray'), plt.title('Gradient Magnitude') plt.show() 2. Image Gradient Corelation Ix.Iy In the Harris Corner Detection algorithm, \\(I_{xy}\\) represents the product of the gradients in the \\(x\\) and \\(y\\) directions at each pixel. The gradients \\(I_x\\) and \\(I_y\\) are computed using derivative operations on the image, typically with the Sobel operator. The steps to calculate \\(I_{xy}\\) are as follows: 2.1 Steps to Calculate \\(I_{xy}\\) 1. Compute Image Gradients \\(I_x\\) and \\(I_y\\): The image gradients represent the rate of change in pixel intensity in the \\(x\\) and \\(y\\) directions. They can be calculated using the Sobel operator, which is a common method to approximate the derivative. Gradient in the \\(x\\) direction (\\(I_x\\)): \\(I_x = \\frac{\\partial I}{\\partial x}\\) The Sobel filter for \\(I_x\\) is: \\(\\text{Sobel}_x = \\begin{bmatrix} -1 &amp; 0 &amp; +1 \\\\ -2 &amp; 0 &amp; +2 \\\\ -1 &amp; 0 &amp; +1 \\end{bmatrix}\\) Gradient in the \\(y\\) direction (\\(I_y\\)): \\(I_y = \\frac{\\partial I}{\\partial y}\\) The Sobel filter for \\(I_y\\) is: \\(\\text{Sobel}_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ +1 &amp; +2 &amp; +1 \\end{bmatrix}\\) 2. Calculate \\(I_{xy}\\): Once you have the gradients \\(I_x\\) and \\(I_y\\), you calculate \\(I_{xy}\\) as the product of these two gradients at each pixel: \\(I_{xy} = I_x \\cdot I_y\\) 2.2 Implementation In OpenCV, you can compute \\(I_{xy}\\) using the following steps: import cv2 import numpy as np # Load the image and convert it to grayscale img = cv2.imread('path_to_your_image.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Compute gradients in the x and y direction using Sobel operator I_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3) I_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3) # Compute the product of the gradients (Ixy) I_xy = I_x * I_y # I_{xy} captures the correlation between the gradients # in the x and y directions, which is a crucial component in # forming the structure tensor M used in the # Harris Corner Detection algorithm. Explanation: cv2.Sobel: This function computes the first derivative of the image in the specified direction. The arguments 1, 0 and 0, 1 in cv2.Sobel specify the direction in which the gradient is computed. ksize=3 specifies the size of the Sobel kernel (3x3 in this case). The gradients \\(I_x\\) and \\(I_y\\) are then multiplied to get \\(I_{xy}\\)."
  },"/ds/aml/image_analysis/1_ima_3_edge_detectors/": {
    "title": "3. Edge Detectors",
    "keywords": "image_analysis",
    "url": "/ds/aml/image_analysis/1_ima_3_edge_detectors/",
    "body": "1. Prewitt and Sobel Kernels Edge detection is a fundamental operation in image processing that identifies significant intensity changes in an image, which typically correspond to object boundaries. Two popular methods for edge detection are the Prewitt and Sobel operators, which are both based on convolution with specific kernels designed to highlight edges in different directions. 1.1 Prewitt Operator The Prewitt operator is a simpler method for detecting edges. It uses two 3x3 convolution kernels to approximate the first derivative of the image intensity in the horizontal and vertical directions. Prewitt Kernel for Horizontal Edges (\\(G_x\\)): \\(G_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\) Prewitt Kernel for Vertical Edges (\\(G_y\\)): \\(G_y = \\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\) 1.2 Sobel Operator The Sobel operator is similar to the Prewitt operator but includes a smoothing effect by incorporating a weight of 2 in the center row/column. This makes it more robust to noise and gives it a better edge response. Sobel Kernel for Horizontal Edges (\\(G_x\\)): \\(G_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\) Sobel Kernel for Vertical Edges (\\(G_y\\)): \\(G_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\) 1.3 Edge Detection Process Apply Convolution: Convolve the image with \\(G_x\\) to detect horizontal edges. Convolve the image with \\(G_y\\) to detect vertical edges. Compute Gradient Magnitude: After obtaining \\(G_x\\) and \\(G_y\\), the gradient magnitude at each pixel is calculated as: \\(G = \\sqrt{G_x^2 + G_y^2}\\) Alternatively, a faster but less precise approximation can be used: \\(G = |G_x| + |G_y|\\) The magnitude \\(G\\) represents the strength of the edge. Thresholding (Optional): To highlight significant edges, a threshold can be applied to the gradient magnitude. Pixels with \\(G\\) above a certain threshold are considered part of an edge. 1.4 Usage Here’s how you can implement edge detection using both the Prewitt and Sobel operators in Python: import cv2 import numpy as np import matplotlib.pyplot as plt # Load the image in grayscale img = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE) # Define Prewitt kernels prewitt_kernel_x = np.array([[ -1, 0, 1], [ -1, 0, 1], [ -1, 0, 1]]) prewitt_kernel_y = np.array([[ -1, -1, -1], [ 0, 0, 0], [ 1, 1, 1]]) # Apply Prewitt operator prewitt_x = cv2.filter2D(img, -1, prewitt_kernel_x) prewitt_y = cv2.filter2D(img, -1, prewitt_kernel_y) prewitt = cv2.magnitude(prewitt_x, prewitt_y) # Apply Sobel operator using OpenCV built-in functions sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) sobel = cv2.magnitude(sobel_x, sobel_y) # Display the results plt.figure(figsize=(12, 6)) plt.subplot(1, 3, 1), plt.imshow(img, cmap='gray'), plt.title('Original Image') plt.subplot(1, 3, 2), plt.imshow(prewitt, cmap='gray'), plt.title('Prewitt Edge Detection') plt.subplot(1, 3, 3), plt.imshow(sobel, cmap='gray'), plt.title('Sobel Edge Detection') plt.show() 1.5 Key Points Prewitt Operator: Simpler and computationally less expensive. Less robust to noise compared to the Sobel operator. Sobel Operator: Adds smoothing, making it more effective at detecting edges in noisy images. Widely used due to its balance between simplicity and performance. IMPORTANT Both Prewitt and Sobel operators are fundamental tools for detecting edges in an image. The choice between them often depends on the specific requirements of the task, with Sobel being preferred in most practical applications due to its enhanced noise resistance and edge-detection capability. 2. Canny Edge Detection Canny Edge Detection is a popular and widely used algorithm for detecting edges in an image. Developed by John F. Canny in 1986, the algorithm is designed to be an optimal edge detector, providing good detection, accurate localization, and minimal response to noise. The Canny edge detection algorithm is more complex than simpler edge detectors like Prewitt and Sobel, but it produces more accurate and reliable results. 2.1 Steps in the Canny Edge Detection Algorithm: The Canny Edge Detection algorithm consists of the following steps: 1. Noise Reduction: The first step is to reduce noise in the image, as noise can cause false edge detection. This is typically done using a Gaussian filter, which smooths the image by averaging pixel values with their neighbors. The Gaussian kernel is applied to the image using convolution: \\(G(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)\\) The parameter \\(\\sigma\\) controls the amount of smoothing. 2. Gradient Calculation: The algorithm computes the intensity gradient of the smoothed image using a method such as the Sobel operator. This produces two gradient images, \\(G_x\\) and \\(G_y\\), representing the gradient in the \\(x\\) and \\(y\\) directions, respectively. The gradient magnitude and direction are then calculated: \\(G = \\sqrt{G_x^2 + G_y^2}\\) \\(\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)\\) 3. Non-Maximum Suppression: Non-maximum suppression is applied to thin out the edges. It suppresses any pixel that is not considered to be part of an edge, leaving only the local maxima in the gradient direction. For each pixel, the algorithm checks whether it is a local maximum by comparing it with its neighbors in the gradient direction. If it is not the maximum, it is set to zero. 4. Double Thresholding: After non-maximum suppression, the algorithm applies a double threshold to classify pixels as strong, weak, or non-relevant edges: Strong edges: Pixels with a gradient magnitude greater than the high threshold. Weak edges: Pixels with a gradient magnitude between the low and high thresholds. Non-relevant: Pixels with a gradient magnitude below the low threshold are suppressed. This step helps to distinguish between true edges and noise. 5. Edge Tracking by Hysteresis: In the final step, weak edges are either included in the final edge map or discarded based on their connectivity to strong edges. A weak edge pixel is retained if it is connected to a strong edge pixel; otherwise, it is suppressed. This helps to ensure that only valid edges are preserved. 2.2 Usage import cv2 import matplotlib.pyplot as plt # Load the image in grayscale img = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE) # Apply Canny edge detection edges = cv2.Canny(img, threshold1=100, threshold2=200) # Display the results plt.figure(figsize=(8, 6)) plt.subplot(1, 2, 1), plt.imshow(img, cmap='gray'), plt.title('Original Image') plt.subplot(1, 2, 2), plt.imshow(edges, cmap='gray'), plt.title('Canny Edge Detection') plt.show() threshold1 and threshold2: These are the low and high thresholds used in the double thresholding step. Edges with gradient values above threshold2 are considered strong edges, while those between threshold1 and threshold2 are considered weak edges. The choice of these thresholds significantly affects the output; a lower threshold results in more detected edges, while a higher threshold reduces the number of detected edges. 2.3 Keypoints Canny Edge Detection: A multi-step process that provides a more accurate and reliable method for detecting edges in images. Steps: Includes noise reduction, gradient calculation, non-maximum suppression, double thresholding, and edge tracking by hysteresis. Applications: Widely used in various computer vision tasks, such as object detection, image segmentation, and feature extraction. Canny Edge Detection is known for its robustness and is commonly used in real-world applications due to its ability to detect edges even in noisy images while minimizing false detections. 3 Hough Transform Line Detector The Hough Transform is a powerful technique used in computer vision and image processing to detect lines, circles, or other parametric shapes in an image. The method is particularly robust for detecting features in noisy images or where the shapes are only partially visible. 3.1 Concept of Hough Transform for Line Detection: The basic idea behind the Hough Transform for line detection is to transform the points in the image space into a parameter space, where a line can be represented by a point. The transform accumulates evidence for all possible lines that could pass through each point in the image. 1. Equation of a Line In the image space (Cartesian coordinates), a line can be represented as: \\(y = mx + c\\) where \\(m\\) is the slope and \\(c\\) is the y-intercept. However, this representation is not ideal for the Hough Transform because vertical lines would require infinite slope. Instead, the Hough Transform uses the polar representation of a line: \\(\\rho = x \\cos \\theta + y \\sin \\theta\\) where: \\(\\rho\\) is the perpendicular distance from the origin to the line. \\(\\theta\\) is the angle of the perpendicular from the origin to the line. 2. Hough Space (Accumulator Array) In the Hough Transform, each point \\((x, y)\\) in the image corresponds to a sinusoidal curve in the \\((\\rho, \\theta)\\) parameter space. Every point on this curve represents a potential line passing through \\((x, y)\\). Voting in Hough Space: Each point in the image votes for all the possible lines (\\(\\rho, \\theta\\)) that could pass through it. The votes are accumulated in an array known as the accumulator. The dimension of the accumulator array corresponds to the range of possible values of \\(\\rho\\) and \\(\\theta\\). 3. Detecting Lines After all points in the image have voted, peaks in the accumulator array indicate the presence of lines. These peaks correspond to the \\(\\rho\\) and \\(\\theta\\) values of the lines in the image. 3.2 Usage import cv2 import numpy as np import matplotlib.pyplot as plt # Load the image in grayscale img = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE) # Apply Canny Edge Detection edges = cv2.Canny(img, 50, 150, apertureSize=3) # Perform Hough Line Transform lines = cv2.HoughLines(edges, 1, np.pi/180, 200) # Draw the lines on the image output_img = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR) for line in lines: rho, theta = line[0] a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho x1 = int(x0 + 1000 * (-b)) y1 = int(y0 + 1000 * (a)) x2 = int(x0 - 1000 * (-b)) y2 = int(y0 - 1000 * (a)) cv2.line(output_img, (x1, y1), (x2, y2), (0, 0, 255), 2) # Display the results plt.figure(figsize=(8, 6)) plt.subplot(1, 2, 1), plt.imshow(img, cmap='gray'), plt.title('Original Image') plt.subplot(1, 2, 2), plt.imshow(output_img), plt.title('Hough Line Detection') plt.show() cv2.HoughLines(): This function performs the Hough Line Transform. The parameters are: edges: The output from the Canny edge detector. 1: The resolution of the parameter \\(\\rho\\) in pixels. np.pi/180: The resolution of the parameter \\(\\theta\\) in radians. 200: The threshold parameter. Only lines with votes greater than this value are considered. Drawing Lines: The detected lines are drawn on the image using the calculated \\(\\rho\\) and \\(\\theta\\) values. 3.3 Variations of the Hough Transform: Standard Hough Transform: Works well for detecting lines but can be computationally expensive due to the size of the accumulator array. Probabilistic Hough Transform: A more efficient variant that randomly samples points to reduce computational load. Implemented in OpenCV as cv2.HoughLinesP(). It returns the start and end points of the detected lines, making it faster and more suitable for real-time applications. Example of Probabilistic Hough Transform: # Perform Probabilistic Hough Line Transform lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10) # Draw the lines on the image output_img = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR) for line in lines: x1, y1, x2, y2 = line[0] cv2.line(output_img, (x1, y1), (x2, y2), (0, 255, 0), 2) # Display the results plt.figure(figsize=(8, 6)) plt.imshow(output_img) plt.title('Probabilistic Hough Line Detection') plt.show() cv2.HoughLinesP(): The probabilistic variant, which returns the endpoints of line segments. minLineLength: The minimum length of a line segment to be detected. maxLineGap: The maximum allowed gap between points on the same line to link them together. 3.4 Keypoints Hough Transform: A technique for detecting lines in an image by transforming points in the image space to the parameter space and finding peaks in the accumulator array. Standard Hough Transform: Detects all lines in an image but is computationally intensive. Probabilistic Hough Transform: A more efficient variant that detects line segments and is faster. Applications: Widely used in applications like lane detection in autonomous vehicles, detecting lines in documents, and more. The Hough Transform is robust and versatile, making it a fundamental tool in many computer vision and image processing tasks."
  },"/ds/aml/image_analysis/1_ima_4_corner_detectors/": {
    "title": "4. Corner Detectors",
    "keywords": "image_analysis",
    "url": "/ds/aml/image_analysis/1_ima_4_corner_detectors/",
    "body": "1. CornerHarris Corner Detection The CornerHarris algorithm is a popular method used in computer vision for detecting corners in an image. Corners are points in the image where the intensity changes significantly in all directions. This algorithm is often used in image processing for feature detection, which is essential in tasks such as object recognition, image matching, and tracking. The CornerHarris algorithm works by computing the gradients of the image, forming a matrix that captures changes in intensity, and then calculating a response function that identifies corners based on these changes. It is effective in detecting corners that are invariant to rotations and scale changes, making it a robust tool in feature detection for various image processing tasks. 1.1 How It Works: 1. Gradient Computation First, the algorithm computes the gradient of the image in the \\(x\\) and \\(y\\) directions. This can be done using Sobel filters or any other method to find the derivatives \\(I_x\\) and \\(I_y\\) of the image. 2. Structure Tensor (Auto-correlation Matrix) The algorithm then computes the structure tensor, also known as the second-moment matrix, at each pixel. This matrix is defined as: \\(M = \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix}\\) where: \\(I_x^2\\) is the squared gradient in the \\(x\\) direction. \\(I_y^2\\) is the squared gradient in the \\(y\\) direction. \\(I_x I_y\\) is the product of the gradients in the \\(x\\) and \\(y\\) directions. The elements of this matrix are then smoothed (often with a Gaussian filter) to reduce noise. 3. Corner Response Calculation The CornerHarris algorithm then computes the corner response function \\(R\\) at each pixel using the structure tensor \\(M\\). The response function is given by: \\(R = \\text{det}(M) - k \\cdot (\\text{trace}(M))^2\\) where: \\(\\text{det}(M) = I_x^2 \\cdot I_y^2 - (I_x I_y)^2\\) is the determinant of the matrix \\(M\\). \\(\\text{trace}(M) = I_x^2 + I_y^2\\) is the trace of the matrix \\(M\\). \\(k\\) is a sensitivity parameter (typically a small constant, around 0.04 to 0.06). \\(R\\) is a scalar value that indicates the likelihood of a pixel being a corner: If \\(R\\) is large and positive, it indicates a corner. If \\(R\\) is negative, it indicates an edge. If \\(R\\) is close to zero, it indicates a flat region. Purpose of Calculating the Determinant and Trace: The determinant of \\(M\\) captures the “intensity variation” around a pixel in two perpendicular directions. Here’s why it is important: Measuring the Response to Corners: The determinant \\(\\text{det}(M)\\) reflects how much the image intensity varies in the local neighborhood of a pixel. A large determinant indicates a strong variation in both directions, which is characteristic of a corner. Specifically: Corners: If \\(\\text{det}(M)\\) is large and positive, it indicates that the pixel has significant intensity variations in all directions, characteristic of a corner. Edges: If the intensity varies significantly in one direction but not in the perpendicular direction, the determinant will be small or close to zero, indicating an edge rather than a corner. Flat Regions: If there is little or no variation in any direction, the determinant will be close to zero, indicating a flat region. Corner Response Function \\(R\\): The determinant of \\(M\\) is used in the corner response function \\(R\\), which is calculated as: \\(R = \\text{det}(M) - k \\cdot (\\text{trace}(M))^2\\) where: \\(\\text{trace}(M) = I_x^2 + I_y^2\\) is the trace of matrix \\(M\\) and represents the sum of the eigenvalues of \\(M\\). \\(k\\) is a constant (typically around 0.04 to 0.06) that controls the sensitivity of the detector. The purpose of this response function \\(R\\) is to combine the determinant (which measures variation) with the trace (which provides a measure of the average intensity change). The combination helps to distinguish corners from edges and flat regions. 4. Thresholding and Non-Maximum Suppression The algorithm then applies a threshold to the \\(R\\) values to detect strong corners. Non-maximum suppression is performed to ensure that only the most prominent corners are retained. This involves comparing each corner’s \\(R\\) value to its neighbors and keeping only local maxima. 1.2 Usage: You can use the CornerHarris algorithm in OpenCV using the cv2.cornerHarris function. Below is a step-by-step guide and example of how to use this function in Python with OpenCV. 1. Load an Image: Load the image on which you want to detect corners. Grayscale images are preferred for corner detection. import cv2 import numpy as np img = cv2.imread('path_to_your_image.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 2. Convert Image to Float32: The cornerHarris function requires the input image to be in the float32 format. gray = np.float32(gray) 3. Apply the CornerHarris Function: Use cv2.cornerHarris to detect corners. You can adjust the parameters to fit your needs. dst = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04) gray: The input image in float32 format. blockSize: The size of the neighborhood considered for corner detection. A larger blockSize makes the algorithm consider more surrounding pixels, which can help detect larger features but may miss smaller details. ksize: Aperture parameter for the Sobel operator (used for gradient computation). This affects the computation of image gradients. A larger ksize will make the gradients more smooth. k: Harris detector free parameter, typically between 0.04 and 0.06. A sensitivity parameter that adjusts the detection threshold. Lower values tend to be more sensitive to edges, while higher values make the detection stricter. 4. Dilate the Result (Optional): Dilate the corner image to enhance the corners (this is often done for better visualization). dst = cv2.dilate(dst, None) 7. Threshold and Mark the Corners: Threshold the image to highlight the corners. For visualization, you can mark these corners on the original image. img[dst &gt; 0.01 * dst.max()] = [0, 0, 255] Here, the corners are marked in red on the original image. 8. Display the Image with Detected Corners: Finally, show the image with detected corners using OpenCV’s imshow function. cv2.imshow('Corners Detected', img) cv2.waitKey(0) cv2.destroyAllWindows() 9. Full Example Code: import cv2 import numpy as np # Load the image and convert it to grayscale img = cv2.imread('path_to_your_image.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Convert to float32 format gray = np.float32(gray) # Detect corners using the Harris Corner Detection method dst = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04) # Dilate the detected corners for better visibility dst = cv2.dilate(dst, None) # Threshold to mark the corners on the original image img[dst &gt; 0.01 * dst.max()] = [0, 0, 255] # Display the image with corners cv2.imshow('Corners Detected', img) cv2.waitKey(0) cv2.destroyAllWindows() 10. Additional Tips: Experiment with the blockSize, ksize, and k parameters to fine-tune the corner detection for your specific image. Consider using cv2.cornerSubPix to refine the corner locations for sub-pixel accuracy if needed. 2. Shi-Tomasi Corner Detection The Shi-Tomasi corner detection algorithm is an improvement over the Harris Corner Detection method. It is also known as the “Good Features to Track” algorithm. Shi-Tomasi refines the Harris method by providing a more reliable way to detect corners, especially in scenarios involving image tracking and matching. 2.1 How It Works: 1. Structure Tensor (Matrix \\(M\\)): Similar to the Harris algorithm, the Shi-Tomasi method begins by computing the structure tensor (also called the second-moment matrix) at each pixel: \\(M = \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix}\\) where \\(I_x\\) and \\(I_y\\) are the image gradients in the \\(x\\) and \\(y\\) directions, respectively. 2. Eigenvalue Calculation: The key difference between Harris and Shi-Tomasi lies in how corners are detected from the matrix \\(M\\). For each pixel, the algorithm computes the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) of the matrix \\(M\\). These eigenvalues represent the intensity changes in two perpendicular directions. Understanding Eigenvalues in Image Gradients: Eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\): The eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) of the matrix \\(M\\) represent the amount of variation (or change in intensity) in the image along two orthogonal directions. These directions are determined by the eigenvectors corresponding to the eigenvalues. Largest Eigenvalue: The largest eigenvalue \\(\\lambda_1\\) (assuming \\(\\lambda_1 \\geq \\lambda_2\\)) represents the direction in which the image intensity changes the most. This direction is not necessarily aligned with the \\(x\\) or \\(y\\) axes; it could be any orientation depending on the local image structure. A large \\(\\lambda_1\\) indicates significant variation in intensity along this principal direction, which could be a combination of changes in both \\(x\\) and \\(y\\). Smallest Eigenvalue: The smallest eigenvalue \\(\\lambda_2\\) represents the intensity variation in the direction orthogonal to the direction of \\(\\lambda_1\\). If \\(\\lambda_2\\) is also large, it indicates significant variation in the perpendicular direction as well, which is characteristic of a corner. IMPORTANT In corner detection: A corner is identified when both \\(\\lambda_1\\) and \\(\\lambda_2\\) are large, meaning there is significant variation in all directions around that pixel. An edge is characterized by a large \\(\\lambda_1\\) and a small \\(\\lambda_2\\), meaning the intensity varies strongly in one direction but not much in the perpendicular direction. A flat region has both \\(\\lambda_1\\) and \\(\\lambda_2\\) small, indicating little to no variation in any direction. 3. Corner Response Criterion: Instead of using a response function like Harris, the Shi-Tomasi algorithm directly uses the smallest eigenvalue as the corner response: \\(R = \\min(\\lambda_1 and \\lambda_2)\\) A large \\(R\\) indicates that both eigenvalues are large, suggesting significant intensity changes in both directions, which is characteristic of a corner. If \\(R\\) is small or close to zero, it indicates an edge or a flat region. 4. Corner Selection: Corners are selected as the pixels with the highest \\(R\\) values, subject to a threshold. The threshold is typically chosen to select the strongest corners. Non-maximum suppression is applied to ensure that only the most prominent corners are retained. 2.2 Advantages of Shi-Tomasi Over Harris Shi-Tomasi is more stable in detecting corners because it avoids the need to combine eigenvalues into a single response function. It directly considers the smallest eigenvalue, which better represents the quality of a corner, especially when one direction has a much larger intensity change than the other. 2.3 Usage Here’s how you can use the Shi-Tomasi algorithm in Python with OpenCV: import cv2 import numpy as np # Load the image in grayscale img = cv2.imread('path_to_your_image.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Shi-Tomasi corner detection corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10) # Convert the corners to integer values corners = np.int0(corners) # Draw circles around the detected corners for i in corners: x, y = i.ravel() cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Display the image with detected corners cv2.imshow('Shi-Tomasi Corners', img) cv2.waitKey(0) cv2.destroyAllWindows() gray: The grayscale input image. maxCorners: The maximum number of corners to detect. If more corners are found, the algorithm retains only the strongest ones. qualityLevel: A value between 0 and 1, which determines the minimum acceptable quality of corners. A lower value means more corners might be detected. minDistance: The minimum Euclidean distance between detected corners to avoid detecting too many closely located corners. 2.4 Keypoints Shi-Tomasi Corner Detection: An algorithm that improves upon the Harris method by directly using the smallest eigenvalue of the structure tensor as the corner response. More Robust: It tends to be more reliable in detecting corners, especially in real-world applications like feature tracking. Implementation: OpenCV provides a built-in function cv2.goodFeaturesToTrack to perform Shi-Tomasi corner detection easily. The Shi-Tomasi algorithm is particularly useful in applications like object tracking, where reliable corner detection is crucial."
  },"/ds/aml/image_analysis/1_ima_5_region_detectors/": {
    "title": "5. Region Detectors",
    "keywords": "image_analysis",
    "url": "/ds/aml/image_analysis/1_ima_5_region_detectors/",
    "body": "1. MSER The MSER (Maximally Stable Extremal Regions) algorithm is a popular method for blob detection in images. It is particularly effective for detecting regions that are stable over a wide range of thresholds, making it useful in tasks like text detection, object recognition, and image matching. 1.1 What are Extremal Regions? An extremal region is a connected component in an image where all pixels have intensity values either higher (bright regions) or lower (dark regions) than the pixels on its outer boundary. MSER focuses on finding these regions that remain stable across a range of intensity thresholds. 1.2 Working Principle of MSER: The MSER algorithm detects regions in an image that are maximally stable with respect to changes in the intensity threshold. The main steps of the MSER algorithm are as follows: 1. Image Thresholding: The algorithm begins by thresholding the image at various intensity levels. Thresholding means converting the image to binary form, where pixels above a certain intensity are set to one value (e.g., white) and those below are set to another (e.g., black). The algorithm examines the image at every possible threshold, incrementing the threshold level progressively from 0 to 255 (for an 8-bit grayscale image). 2. Connected Component Analysis: For each threshold level, the algorithm identifies connected components, which are groups of contiguous pixels that have the same intensity value. These connected components are the extremal regions. As the threshold changes, some regions will grow, merge, or disappear. 3. Maximal Stability: The key idea of MSER is to identify regions that are “maximally stable” over a range of thresholds. Stability is measured by how little the area of a region changes as the threshold changes. A region is considered maximally stable if its area remains nearly constant across several thresholds. Formally, the stability of a region ( R_i ) is measured as: [ S(R_i) = \\frac{|R_{i+\\Delta} - R_i|}{|R_i|} ] where ( |R_i| ) is the area (number of pixels) of the region at threshold ( i ), and ( \\Delta ) is a small increment in the threshold. 4. Region Selection: Regions with the smallest stability values are selected as MSERs. These regions represent blobs in the image that are resistant to changes in thresholding and are therefore likely to be meaningful structures. 5. Post-Processing: After detecting MSERs, various post-processing steps may be applied to filter out noise, merge overlapping regions, or refine the detected regions. 1.3 Key Properties of MSER: Affine Invariance: MSERs are invariant to affine transformations, which means that they are robust to changes in scale, rotation, and translation. This makes the algorithm particularly useful in object recognition tasks where objects may appear in different orientations or sizes. Scale Invariance: The algorithm is also capable of detecting regions of different sizes, making it scale-invariant. Contrast Invariance: MSER is robust to changes in lighting or contrast, as it focuses on regions that remain stable across varying intensity thresholds. 1.4 Usage import cv2 import matplotlib.pyplot as plt # Load the image img = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE) # Initialize MSER detector mser = cv2.MSER_create() # Detect MSER regions regions, _ = mser.detectRegions(img) # Draw MSER regions output_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for region in regions: hull = cv2.convexHull(region.reshape(-1, 1, 2)) cv2.polylines(output_img, [hull], 1, (0, 255, 0), 2) # Display the results plt.figure(figsize=(8, 6)) plt.imshow(output_img) plt.title('MSER Detection') plt.show() Explanation of the Code: cv2.MSER_create(): Creates an MSER object using OpenCV’s built-in MSER implementation. detectRegions: Detects MSER regions in the image and returns a list of points for each detected region. cv2.convexHull: Computes the convex hull of the detected region, which is the smallest polygon that can enclose all the points in the region. cv2.polylines: Draws the convex hulls on the original image. 1.5 Applications of MSER: Text Detection: MSER is often used in text detection in natural scenes, as text regions tend to form stable extremal regions due to their high contrast with the background. Object Recognition: In object recognition tasks, MSER can be used to detect key regions that are invariant to affine transformations, making it easier to recognize objects under different viewpoints. Image Matching: MSER regions are used as keypoints for matching images, especially in applications like panorama stitching or 3D reconstruction. 1.6 Keypoints MSER (Maximally Stable Extremal Regions): An algorithm for detecting blob-like regions in images that are stable over a range of intensity thresholds. Key Properties: Affine invariance, scale invariance, and contrast invariance make MSER a powerful tool in various computer vision tasks. Implementation: The algorithm is available in OpenCV and can be easily applied to detect regions of interest in images. MSER is particularly useful in scenarios where the goal is to detect regions that are robust to changes in lighting, scale, and orientation, making it ideal for tasks like text detection and object recognition."
  }}
