<!DOCTYPE HTML>
<html lang="en" >
    <head><meta charset="UTF-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"><title>6. Keypoint Detectors · Intelligent Mind Labs</title><meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="description" content="Wikis for Data Science, Computing, Software Development, Math, etc.
"><!-- <meta name="generator" content="Jekyll (using style of GitBook 3.2.3)"> --><meta name="author" content="Intelligent Mind Labs"><!-- <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script> -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script> -->

<!-- For logo animation -->
<!-- <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"> -->

<link rel="stylesheet" href="/ds/aml/assets/fonts/fa5.15/css/all.min.css">

<link id="gitbook_style" rel="stylesheet" href="/ds/aml/assets/gitbook/style.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/gitbook-plugin-back-to-top-button/plugin.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/gitbook-plugin-expandable-chapters-small2/expandable-chapters-small.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/gitbook-plugin-fontsettings/website.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/gitbook-plugin-search-pro/search.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/gitbook-plugin-splitter/splitter.css">



<!-- <link rel="stylesheet" href="/ds/aml/assets/gitbook/rouge/colorful.css"> -->

<style>
@import url('https://fonts.googleapis.com/css2?family=Exo:ital,wght@0,100..900;1,100..900&display=swap');
</style>
    

<link rel="stylesheet" href="/ds/aml/assets/gitbook/custom.css">
<link rel="stylesheet" href="/ds/aml/assets/gitbook/custom-local.css"><style>
.img_url {
    content: url("/ds/aml/assets/images/wiki.imindlabs.grey.logo.png");
}

.book.color-theme-2 .img_url {
    content: url("/ds/aml/assets/images/wiki.imindlabs.logo.png");
}
</style><link rel="stylesheet" href="/ds/aml/assets/gitbook/custom-local-child.css">

<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<!-- <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/ds/aml/assets/gitbook/images/apple-touch-icon-precomposed-152.png"> -->
<!-- <link rel="shortcut icon" href="/ds/aml/wiki.imindlabs.favicon.png" type="image/x-icon"> -->
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/ds/aml/wiki.imindlabs.favicon.png">
<link rel="icon" href="/ds/aml/wiki.imindlabs.favicon.png" sizes="32x32" type="image/png">
<link rel="icon" href="/ds/aml/wiki.imindlabs.favicon.png" sizes="16x16" type="image/png">
<link rel="shortcut icon" href="/ds/aml/wiki.imindlabs.favicon.png" type="image/png">



<!-- trackers -->
            <link rel="prev" href="/ds/aml/image_analysis/1_ima_5_region_detectors/" />
        

        
            <link rel="next" href="/ds/aml/basics/1_regression_1_cv/" />
        
    </head>
    <body>         
        <div class="book"><div class="book-summary">
    <script type="text/javascript">
        // Fixes the page links scroll problem on both desktop and mobile browsers
        function pageScrollToTop(element) {
            // both mobile and non-mobile
            $('div.body-inner').animate({ scrollTop: 0 });
            $(element).parent().find('li>ul>li').removeClass('active');
            return true;  // propagate
        }
        // Fixes the anchor links scroll problem on mobile browsers
        function mobilePageScrollToAnchor(element) {
            $(element).closest('li.chapter').find('ul>li').removeClass('active');
            $(element).parent().addClass('active');
            
            // ! BUG: Enable the following functionality on large screen sizes
            // On sub menu item click the page did not navigate. Now it does.
            //if ($(document).width() <= 1240) {
                $('div.body-inner').animate({ scrollTop: $($(element).attr('href')).get(0).offsetTop });
            // }
            return true;
        }
    </script>

    <!-- <div style="text-align:center;" class="animate__animated animate__pulse animate__delay-1s">
        <a href="/ds/aml/">
            <img class="logo zoom" src="/ds/aml/assets/images/wiki.imindlabs.logo.png" alt="Intelligent Mind Labs wiki logo" />
        </a>
    </div> -->
    <div style="text-align:center;" class="logo-wrap">
        <a href="/ds/aml/">
            <img class="logo zoom" src="/ds/aml/assets/images/wiki.imindlabs.logo_153x200.webp" alt="Intelligent Mind Labs wiki logo" width="153" height="200"/>
        </a>
    </div>

    <nav role="navigation">
        <div id="book-search-input" role="search">
            <input type="text" placeholder="Type to search" />
        </div>
        <div id="book-search-input-link" role="search">
            <a href="/ds/aml/assets/search.html">Click to Search</a>
        </div>
        <ul class="summary">
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml">
                
                <a href="/ds/aml/" onclick="pageScrollToTop(this)">
                    Wiki | Advance Machine Learning
                </a>
            </li>

            <!-- <li class="divider"></li> -->

            
            <!-- <p>1_learning_domains</p> -->
            
                <p class="collection-title">Learning Domains</p>
            
            
            

            

            
            
            
            <!-- <p>2_dataset_preparation</p> -->
            
                <p class="collection-title">Dataset Preparation</p>
            
            
            

            

            
            
            
            <!-- <p>3_feature_analysis</p> -->
            
                <p class="collection-title">Feature Analysis</p>
            
            
            

            

            
            
            
            <!-- <p>4_problem_domains</p> -->
            
                <p class="collection-title">Problem Domains</p>
            
            
            

            
            
            
            <p class="separator-title">Image Processing</p>
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/basics/1_ima_1_basics/">
                
                <a href="/ds/aml/basics/1_ima_1_basics/" onclick="pageScrollToTop(this)">
                    1. Introduction
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/image_analysis/1_ima_2_filters/">
                
                <a href="/ds/aml/image_analysis/1_ima_2_filters/" onclick="pageScrollToTop(this)">
                    2. Filters
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/image_analysis/1_ima_3_edge_detectors/">
                
                <a href="/ds/aml/image_analysis/1_ima_3_edge_detectors/" onclick="pageScrollToTop(this)">
                    3. Edge Detectors
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/image_analysis/1_ima_4_corner_detectors/">
                
                <a href="/ds/aml/image_analysis/1_ima_4_corner_detectors/" onclick="pageScrollToTop(this)">
                    4. Corner Detectors
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/image_analysis/1_ima_5_region_detectors/">
                
                <a href="/ds/aml/image_analysis/1_ima_5_region_detectors/" onclick="pageScrollToTop(this)">
                    5. Region Detectors
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter active" data-level="1.2" data-path="/ds/aml/image_analysis/1_ima_6_keypoints/">
                
                <a href="/ds/aml/image_analysis/1_ima_6_keypoints/" onclick="pageScrollToTop(this)">
                    6. Keypoint Detectors
                </a>
                
                
                <ul><li><a href="#1-sift" onclick="mobilePageScrollToAnchor(this)" >1. SIFT</a><ul><li><a href="#steps" onclick="mobilePageScrollToAnchor(this)" >Steps:</a></li><li><a href="#objective" onclick="mobilePageScrollToAnchor(this)" >Objective:</a></li></ul></li><li><a href="#sift-scale-invariant-feature-transform" onclick="mobilePageScrollToAnchor(this)" >SIFT (Scale-Invariant Feature Transform)</a><ul><li><a href="#keypoint-detection" onclick="mobilePageScrollToAnchor(this)" >Keypoint Detection</a></li><li><a href="#sift-descriptor" onclick="mobilePageScrollToAnchor(this)" >SIFT Descriptor</a></li><li><a href="#summary" onclick="mobilePageScrollToAnchor(this)" >Summary</a></li></ul></li><li><a href="#sift-feature-extraction-on-checkerboard-image" onclick="mobilePageScrollToAnchor(this)" >SIFT Feature Extraction on Checkerboard Image</a><ul><li><a href="#step-by-step-explanation" onclick="mobilePageScrollToAnchor(this)" >Step-by-Step Explanation:</a></li></ul></li><li><a href="#opencvs-sift-function-explanation" onclick="mobilePageScrollToAnchor(this)" >OpenCV’s SIFT Function Explanation:</a><ul><li><a href="#initialization" onclick="mobilePageScrollToAnchor(this)" >Initialization:</a></li><li><a href="#key-point-detection" onclick="mobilePageScrollToAnchor(this)" >Key Point Detection:</a></li><li><a href="#descriptor-extraction" onclick="mobilePageScrollToAnchor(this)" >Descriptor Extraction:</a></li></ul></li><li><a href="#explanation-of-cv2drawkeypoints" onclick="mobilePageScrollToAnchor(this)" >Explanation of <code class="language-plaintext highlighter-rouge">cv2.drawKeypoints()</code></a></li><li><a href="#observations" onclick="mobilePageScrollToAnchor(this)" >Observations</a><ul><li><a href="#1-sift-keypoints-on-original-image" onclick="mobilePageScrollToAnchor(this)" >1. SIFT Keypoints on Original Image:</a></li><li><a href="#2-sift-descriptors" onclick="mobilePageScrollToAnchor(this)" >2. SIFT Descriptors:</a></li><li><a href="#3-sift-keypoints-on-rotated-and-scaled-image" onclick="mobilePageScrollToAnchor(this)" >3. SIFT Keypoints on Rotated and Scaled Image:</a></li><li><a href="#4-sift-descriptors-on-rotated-and-scaled-image" onclick="mobilePageScrollToAnchor(this)" >4. SIFT Descriptors on Rotated and Scaled Image:</a></li><li><a href="#note" onclick="mobilePageScrollToAnchor(this)" >Note</a></li></ul></li></ul></li><li><a href="#binary-shape-analyais" onclick="mobilePageScrollToAnchor(this)" >Binary Shape Analyais</a><ul><li><a href="#httpspyimagesearchcom20210222opencv-connected-component-labeling-and-analysis" onclick="mobilePageScrollToAnchor(this)" >https://pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/</a></li><li><a href="#steps-1" onclick="mobilePageScrollToAnchor(this)" >Steps:</a></li></ul></li><li><a href="#observations-from-the-binary-shape-analysis-task" onclick="mobilePageScrollToAnchor(this)" >Observations from the Binary Shape Analysis Task</a><ul><li><a href="#1-image-thresholding" onclick="mobilePageScrollToAnchor(this)" >1. Image Thresholding:</a></li><li><a href="#2-connected-component-analysis" onclick="mobilePageScrollToAnchor(this)" >2. Connected Component Analysis:</a></li><li><a href="#3-blob-statistics" onclick="mobilePageScrollToAnchor(this)" >3. Blob Statistics:</a></li><li><a href="#before-diving-into-the-main-steps-lets-understand-the-utility-functions-that-will-be-used-throughout-the-process" onclick="mobilePageScrollToAnchor(this)" >Before diving into the main steps, let’s understand the utility functions that will be used throughout the process.</a></li><li><a href="#function-display_imageimg-title" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">display_image(img, title="")</code></a></li><li><a href="#function-imshow_componentslabels" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">imshow_components(labels)</code></a></li><li><a href="#function-extract_featureslabels_im-stats" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">extract_features(labels_im, stats)</code></a></li></ul></li><li><a href="#image-loading-and-preprocessing" onclick="mobilePageScrollToAnchor(this)" >Image Loading and Preprocessing</a><ul><li><a href="#step-by-step-explanation-1" onclick="mobilePageScrollToAnchor(this)" >Step-by-Step Explanation:</a></li></ul></li><li><a href="#connected-component-analysis-and-feature-extraction" onclick="mobilePageScrollToAnchor(this)" >Connected Component Analysis and Feature Extraction</a><ul><li><a href="#step-by-step-explanation-2" onclick="mobilePageScrollToAnchor(this)" >Step-by-Step Explanation:</a></li><li><a href="#cv2connectedcomponentswithstats-explanation" onclick="mobilePageScrollToAnchor(this)" ><code class="language-plaintext highlighter-rouge">cv2.connectedComponentsWithStats</code> Explanation</a></li></ul></li></ul></li><li><a href="#exercise-3---histogram-feature-extraction" onclick="mobilePageScrollToAnchor(this)" >Exercise 3 - Histogram Feature Extraction</a><ul><li><a href="#objective-1" onclick="mobilePageScrollToAnchor(this)" >Objective:</a></li><li><a href="#steps-2" onclick="mobilePageScrollToAnchor(this)" >Steps:</a></li><li><a href="#deliverables" onclick="mobilePageScrollToAnchor(this)" >Deliverables:</a></li></ul></li><li><a href="#utility-functions-explanation" onclick="mobilePageScrollToAnchor(this)" >Utility Functions Explanation</a><ul><li><a href="#function-compute_histogramimage-bins256" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">compute_histogram(image, bins=256)</code></a></li><li><a href="#function-display_histogramhist-titlehistogram" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">display_histogram(hist, title="Histogram")</code></a></li><li><a href="#function-display_imageimg-title-1" onclick="mobilePageScrollToAnchor(this)" >Function: <code class="language-plaintext highlighter-rouge">display_image(img, title="")</code></a></li><li><a href="#histogram-computation-and-visualization-for-each-character" onclick="mobilePageScrollToAnchor(this)" >Histogram Computation and Visualization for Each Character</a></li><li><a href="#loop-for-k-in-range1-num_labels" onclick="mobilePageScrollToAnchor(this)" >Loop: <code class="language-plaintext highlighter-rouge">for k in range(1, num_labels)</code></a></li><li><a href="#post-loop" onclick="mobilePageScrollToAnchor(this)" >Post Loop:</a></li><li><a href="#comparing-histograms-of-characters" onclick="mobilePageScrollToAnchor(this)" >Comparing Histograms of Characters:</a></li><li><a href="#insights-on-the-histogram-feature" onclick="mobilePageScrollToAnchor(this)" >Insights on the Histogram Feature:</a></li><li><a href="#resolution-of-the-histogram" onclick="mobilePageScrollToAnchor(this)" >Resolution of the Histogram:</a></li></ul></li></ul></li></ul>

                
                
            </li>
            
            
            
            <p class="separator-title">Regression</p>
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/basics/1_regression_1_cv/">
                
                <a href="/ds/aml/basics/1_regression_1_cv/" onclick="pageScrollToTop(this)">
                    1. Computer Vision
                </a>
                
                
                
            </li>
            
            
            
            
            
            <li class="chapter" data-level="1.1" data-path="/ds/aml/basics/1_regression_1_nlp/">
                
                <a href="/ds/aml/basics/1_regression_1_nlp/" onclick="pageScrollToTop(this)">
                    2. Natural Language Processing
                </a>
                
                
                
            </li>
            

            
            <!-- <li class="divider"></li> -->
            
            
            
            <!-- <p>posts</p> -->
            
            
            

            

            
            
            
        </ul>
    </nav>
</div><div class="book-body">
                <div class="book-header" role="navigation">
                    <!-- Title -->
                    <h1>
                        <i class="fa fa-circle-o-notch fa-spin"></i>
                        
                            <a href="." >6. Keypoint Detectors</a>
                        
                    </h1>
                </div>

                <div class="body-inner"><div class="page-wrapper" tabindex="-1" role="main">
    

    <!-- <div class="spinner-container">
        <div class="spinner-border text-info" role="status">
            <span class="visually-hidden">Loading...</span>
        </div>
    </div> -->

    <div class="page-inner">
        <div id="book-search-results"> <!-- class="hide_element" -->            
            <div class="search-noresults">
                <section class="normal markdown-section">
                    
                        <h1 id="/image_analysis/1_ima_6_keypoints">6. Keypoint Detectors</h1>
                    

                    <h1 id="1-sift">1. SIFT</h1>

<p>In this exercise, we will delve into the properties of the <strong>Scale-Invariant Feature Transform (SIFT)</strong>, particularly its invariance characteristics.</p>

<h2 id="steps">Steps:</h2>

<ol>
  <li><strong>Keypoint Detection on Checkerboard Image</strong>:
    <ul>
      <li>Use the SIFT algorithm to detect keypoints on a natural image.</li>
      <li>Count the number of keypoints identified.</li>
      <li>Visualize these keypoints using the <code class="language-plaintext highlighter-rouge">drawKeypoints</code> method.</li>
      <li><strong>Observation</strong>: Pay attention to the locations of the keypoints. How do they compare to the Harris corners from Practical 3? What can you infer about their main orientation?</li>
    </ul>
  </li>
  <li><strong>SIFT Descriptors</strong>:
    <ul>
      <li>Extract SIFT descriptors for the detected keypoints.</li>
      <li>Analyze the number of descriptors and the dimensionality of each.</li>
      <li>Compare your findings with your lecture notes on SIFT. Are they consistent?</li>
      <li>Convert the descriptors to an intensity image and inspect it.</li>
      <li><strong>Observation</strong>: Look for any discernible patterns in the intensity image. What do the descriptors convey?</li>
    </ul>
  </li>
  <li><strong>Invariance Test with Rotated and Scaled Image</strong>:
    <ul>
      <li>Rotate (10 deg) and slightly scale (1.2) the input image.</li>
      <li>Repeat the SIFT keypoint detection and descriptor extraction on this transformed image.</li>
      <li>Convert the new set of descriptors to an intensity image.</li>
      <li><strong>Observation</strong>: Compare the intensity images of the original and transformed checkerboard. Can you validate the scale invariance of the SIFT descriptors?</li>
      <li>Choose a few keypoint pairs from both images and delve deeper into their descriptors. What similarities or differences can you spot?</li>
      <li>Extra: perform correspondence between the original image and its rotated and scaled version. Draw a line between any two matched keypoints in the two images.</li>
    </ul>
  </li>
</ol>

<h2 id="objective">Objective:</h2>

<p>Through this exercise, you’ll gain hands-on experience with the SIFT algorithm, understanding its robustness against transformations and its ability to capture distinctive features in images. By comparing it with other methods like Harris corners, you’ll appreciate the nuances and
—strengths of each approach.</p>

<hr />
<h1 id="sift-scale-invariant-feature-transform">SIFT (Scale-Invariant Feature Transform)</h1>

<p>SIFT is a method in computer vision to detect and describe local features in images. The algorithm provides key advantages when it comes to scale, rotation, and translation invariances.</p>

<h3 id="keypoint-detection">Keypoint Detection</h3>

<h4 id="scale-space-extrema-detection">Scale-space Extrema Detection</h4>

<ul>
  <li>The image is progressively blurred using Gaussian filters, creating a series of scaled images.</li>
  <li>The Difference of Gaussians (DoG) is found between successive Gaussian blurred images.</li>
  <li>Extrema (maxima and minima) in the DoG images are potential keypoints.</li>
</ul>

<h4 id="keypoint-localization">Keypoint Localization</h4>

<ul>
  <li>Refines keypoints to eliminate less stable ones.</li>
  <li>Uses a method similar to the Harris corner detection to discard keypoints that have low contrast or lie along an edge.</li>
</ul>

<h4 id="orientation-assignment">Orientation Assignment</h4>

<ul>
  <li>Each keypoint is given one or more orientations based on local image gradient directions.</li>
  <li>This ensures the keypoint descriptor is rotation invariant.</li>
</ul>

<h3 id="sift-descriptor">SIFT Descriptor</h3>

<h4 id="descriptor-representation">Descriptor Representation</h4>

<ul>
  <li>For each keypoint, a descriptor is computed.</li>
  <li>A 16x16 neighborhood around the keypoint is considered, divided into 4x4 sub-blocks.</li>
  <li>An 8-bin orientation histogram is computed for each sub-block, resulting in a 128-bin descriptor for each keypoint.</li>
</ul>

<h4 id="descriptor-normalization">Descriptor Normalization</h4>

<ul>
  <li>The descriptor is normalized to ensure invariance to illumination changes.</li>
</ul>

<h3 id="summary">Summary</h3>

<p>SIFT is powerful for detecting and describing local features in images. It’s invariant to image scale, rotation, and partially invariant to affine transformations and illumination changes. This makes it suitable for tasks like object recognition, panorama stitching, and 3D scene reconstruction.</p>

<hr />
<h2 id="sift-feature-extraction-on-checkerboard-image">SIFT Feature Extraction on Checkerboard Image</h2>

<p>This code demonstrates the process of detecting and extracting SIFT (Scale-Invariant Feature Transform) keypoints and descriptors from a checkerboard image and its rotated and scaled version.</p>

<h3 id="step-by-step-explanation">Step-by-Step Explanation:</h3>

<ol>
  <li><strong>Loading Image Files</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">filenames</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="sh">'</span><span class="s">*.png</span><span class="sh">'</span><span class="p">))</span>
 <span class="n">filename</span> <span class="o">=</span> <span class="n">filenames</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>    </div>
    <ul>
      <li>The code first fetches all the image filenames with <code class="language-plaintext highlighter-rouge">.png</code> extension from the specified directory.</li>
      <li>It then selects the first image from this list for further processing.</li>
    </ul>
  </li>
  <li><strong>Reading the Image and Initializing SIFT</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">IMREAD_GRAYSCALE</span><span class="p">)</span>
 <span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">SIFT_create</span><span class="p">()</span>
</code></pre></div>    </div>
    <ul>
      <li>The selected image is read in grayscale mode.</li>
      <li>A SIFT detector object is initialized.</li>
    </ul>
  </li>
  <li><strong>Detecting SIFT Keypoints</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">keypoints</span> <span class="o">=</span> <span class="n">sift</span><span class="p">.</span><span class="nf">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li>The SIFT keypoints are detected for the grayscale image.</li>
    </ul>
  </li>
  <li><strong>Extracting SIFT Descriptors</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">sift</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">descriptors</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">SIFT Descriptors</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div>    </div>
    <ul>
      <li>SIFT descriptors are computed for the detected keypoints.</li>
      <li>The descriptors are visualized as an intensity image.</li>
    </ul>
  </li>
  <li><strong>Rotating and Scaling the Image</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>
 <span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">getRotationMatrix2D</span><span class="p">((</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">rows</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
 <span class="n">rotated_scaled_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="n">rows</span><span class="p">))</span>
</code></pre></div>    </div>
    <ul>
      <li>The original image is rotated by 2 degrees and scaled by a factor of 1.2.</li>
      <li>The transformed image is stored in <code class="language-plaintext highlighter-rouge">rotated_scaled_img</code>.</li>
    </ul>
  </li>
</ol>

<hr />
<h2 id="opencvs-sift-function-explanation">OpenCV’s SIFT Function Explanation:</h2>

<h3 id="initialization">Initialization:</h3>
<p>To initialize a SIFT detector object, use the following code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">SIFT_create</span><span class="p">()</span>
</code></pre></div></div>
<p>SIFT stands for Scale-Invariant Feature Transform. It’s an algorithm in computer vision to detect and describe local features in images.</p>

<h3 id="key-point-detection">Key Point Detection:</h3>
<p>To detect the SIFT keypoints of an image, use the following code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keypoints</span> <span class="o">=</span> <span class="n">sift</span><span class="p">.</span><span class="nf">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p><strong>Parameters</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">img</code>: The input image where keypoints are to be detected.</li>
  <li><code class="language-plaintext highlighter-rouge">None</code>: Mask of the image. It’s an optional parameter. If provided, the function will look for keypoints only in the specified region.</li>
</ul>

<h3 id="descriptor-extraction">Descriptor Extraction:</h3>
<p>To compute the descriptors from the detected keypoints, use the following code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">sift</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">)</span>
</code></pre></div></div>
<p><strong>Parameters</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">img</code>: The input image.</li>
  <li><code class="language-plaintext highlighter-rouge">keypoints</code>: The detected keypoints for which descriptors are to be computed.</li>
</ul>

<p><strong>Return</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">keypoints</code>: List of keypoints.</li>
  <li><code class="language-plaintext highlighter-rouge">descriptors</code>: The SIFT descriptors of the keypoints. Each keypoint is represented by a vector of 128 values.</li>
</ul>

<hr />
<h2 id="explanation-of-cv2drawkeypoints">Explanation of <code class="language-plaintext highlighter-rouge">cv2.drawKeypoints()</code></h2>

<p>The <code class="language-plaintext highlighter-rouge">cv2.drawKeypoints()</code> function is a utility provided by OpenCV to visualize the keypoints detected in an image.</p>

<h4 id="parameters">Parameters:</h4>

<ul>
  <li><strong>input_image</strong>:
    <ul>
      <li>Description: The original image on which keypoints were detected.</li>
      <li>Type: 2D or 3D array (typically a grayscale or color image).</li>
    </ul>
  </li>
  <li><strong>keypoints</strong>:
    <ul>
      <li>Description: A list of detected keypoints on the <code class="language-plaintext highlighter-rouge">input_image</code>. These keypoints encapsulate information about the location, scale, orientation, and other characteristics of local features in the image.</li>
      <li>Type: List of <code class="language-plaintext highlighter-rouge">cv2.KeyPoint</code> objects. These objects are typically obtained from feature detection methods like SIFT, SURF, etc.</li>
    </ul>
  </li>
  <li><strong>output_image</strong> (optional):
    <ul>
      <li>Description: Image on which the keypoints will be drawn. If not provided, a new image is created to draw the keypoints. In most scenarios, this is the same as the <code class="language-plaintext highlighter-rouge">input_image</code>.</li>
      <li>Type: 2D or 3D array.</li>
    </ul>
  </li>
  <li><strong>flags</strong> (optional):
    <ul>
      <li>Description: Determines the drawing characteristics of the keypoints.</li>
      <li>Type: Integer or combination of flag values.</li>
      <li>Notable Flag:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</code>: Ensures that the size of the keypoint is visualized along with its orientation, providing a richer representatin of the keypoint.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="output">Output:</h4>

<ul>
  <li><strong>img_keypoints</strong>:
    <ul>
      <li>Description: Image with the keypoints drawn on it.</li>
      <li>Type: 2D or 3D array, sthe <code class="language-plaintext highlighter-rouge">img_keypoints</code> variable.</li>
    </ul>
  </li>
</ul>

<hr />
<h2 id="observations">Observations</h2>

<h3 id="1-sift-keypoints-on-original-image">1. SIFT Keypoints on Original Image:</h3>

<p>When we detect SIFT keypoints on the original checkerboard image, we observe that the keypoints are not just located at the corners of the squares (as we might expect with Harris corners). Instead, SIFT keypoints are distributed across the image, capturing more intricate details and patterns.</p>

<p><strong>Why is this the case?</strong></p>

<p>SIFT keypoints are scale-invariant and rotation-invariant. This means that they are designed to capture features that remain consistent across different scales and orientations. The keypoints are detected based on the difference of Gaussian functions applied at different scales, allowing them to capture features at various levels of detail.</p>

<h3 id="2-sift-descriptors">2. SIFT Descriptors:</h3>

<p>The SIFT descriptors provide a unique fingerprint for each keypoint, capturing the local image gradient information around the keypoint. When visualized as an intensity image, the descriptors might not show a clear pattern to the human eye, but they contain rich information about the local image gradients.</p>

<p><strong>What can we infer from the descriptors?</strong></p>

<p>The intensity image of the descriptors might seem like random patterns, but these patterns encode the gradient orientations in the keypoint’s neighborhood. This makes the descriptors robust to transformations like rotation and scaling.</p>

<h3 id="3-sift-keypoints-on-rotated-and-scaled-image">3. SIFT Keypoints on Rotated and Scaled Image:</h3>

<p>Upon rotating and scaling the checkerboard image, and then detecting SIFT keypoints, we observe that some of the keypoints are still consistently detected in similar regions as in the original image. This demonstrates the scale and rotation invariance of SIFT keypoints.</p>

<h3 id="4-sift-descriptors-on-rotated-and-scaled-image">4. SIFT Descriptors on Rotated and Scaled Image:</h3>

<p>When we extract the SIFT descriptors for the keypoints detected on the rotated and scaled image and visualize them as an intensity image, we notice that the patterns are quite similar to the descriptors of the original image. This is because the descriptors capture the local gradient information, which remains consistent even after transformations.</p>

<p><strong>Scale Invariance Verification:</strong></p>

<p>By comparing the SIFT descriptors of the original image and the rotated and scaled image, we can verify the scale invariance property of SIFT. Even though the image underwent transformations, the descriptors remain consistent, proving the robustness of SIFT features.</p>

<h3 id="note">Note</h3>

<p>The performance of SIFT might not be optimal on chessboard images. This is primarily because chessboard patterns lack the intricate textures and features that SIFT excels at detecting. For more pronounced results, consider applying SIFT on images with richer details, such as car license plates.</p>

<h1 id="binary-shape-analyais">Binary Shape Analyais</h1>

<h3 id="httpspyimagesearchcom20210222opencv-connected-component-labeling-and-analysis">https://pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/</h3>
<p>In this exercise, you’ll be working with binary shape analysis to extract blob features from a gray-scale input image. The main goal is to separate individual characters from the image and then extract several binary features from them.</p>

<h3 id="steps-1">Steps:</h3>

<ol>
  <li><strong>Image Thresholding</strong>:
    <ul>
      <li>Convert the input gray-scale image into a binary image.</li>
      <li>Use Otsu’s thresholding method (available in OpenCV) to achieve this.</li>
      <li>Examine the resulting binary image to ensure the thresholding is effective.</li>
    </ul>
  </li>
  <li><strong>Connected Component Labeling (CCL)</strong>:
    <ul>
      <li>Implement a CCL algorithm to separate the blobs, which in this context refers to the characters in the image.</li>
      <li>Refer to lecture notes or documentation for guidance on implementing this algorithm.</li>
    </ul>
  </li>
  <li><strong>Blob Extraction</strong>:
    <ul>
      <li>Apply the CCL algorithm to the binary image.</li>
      <li>For each detected blob, identify the tightest bounding box.</li>
      <li>Extract the character within each bounding box.</li>
      <li>Verify the accuracy of the bounding boxes by either:
        <ul>
          <li>Saving individual characters as image files, or</li>
          <li>Drawing the bounding boxes on the original image.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Feature Computation</strong>:
 For each extracted character, compute the following features:
    <ul>
      <li><strong>Area</strong>: Total number of foreground pixels.</li>
      <li><strong>Height</strong>: Height of the bounding box.</li>
      <li><strong>Width</strong>: Width of the bounding box.</li>
      <li><strong>Fraction of Foreground Pixels</strong>: Calculated as \(\frac{\text{Area}}{\text{Height} \times \text{Width}}\)</li>
      <li><strong>Distribution in X-direction</strong>: Analyze the spread of foreground pixels horizontally.</li>
      <li><strong>Distribution in Y-direction</strong>: Analyze the spread of foreground pixels vertically.</li>
    </ul>
  </li>
  <li><strong>Feature Analysis</strong>:
    <ul>
      <li>Compare the features obtained for different pairs of characters.</li>
      <li>Identify which features help in distinguishing different characters.</li>
      <li>Determine which features consistently describe characters that appear the same.</li>
    </ul>
  </li>
</ol>

<p>By the end of this exercise, you should be able to effectively separate characters from an image and analyze their binary features to understand their distinct characteristics.</p>

<hr />

<h2 id="observations-from-the-binary-shape-analysis-task">Observations from the Binary Shape Analysis Task</h2>

<h3 id="1-image-thresholding">1. Image Thresholding:</h3>
<p>After loading the image, we first converted it to grayscale. This simplifies the image and helps in thresholding. We applied three types of thresholding:</p>
<ul>
  <li><strong>Basic Binary Thresholding</strong>: Pixels with intensity above 127 are set to 255 (white), and those below are set to 0 (black).</li>
  <li><strong>Otsu’s Thresholding</strong>: This method automatically calculates an optimal threshold value based on the image histogram.</li>
  <li><strong>Gaussian Blur + Otsu’s Thresholding</strong>: Before applying Otsu’s thresholding, we smoothed the image using Gaussian blur. This can help in removing noise and improving the thresholding result.</li>
</ul>

<p>From the displayed images, we can observe that the combination of Gaussian blur and Otsu’s thresholding provides a cleaner binary image, especially if the original image has noise.</p>

<h3 id="2-connected-component-analysis">2. Connected Component Analysis:</h3>
<p>After thresholding, we performed connected component labeling to identify individual blobs or regions in the binary image. Each unique blob is assigned a unique label.</p>

<ul>
  <li>The number of labels gives us the number of unique regions detected, including the background.</li>
  <li>The maximum label value provides an idea of the labeling range.</li>
</ul>

<p>The colored components image visually represents each unique region with a different color. This helps in understanding the separation of different blobs and verifying the accuracy of the connected component analysis.</p>

<h3 id="3-blob-statistics">3. Blob Statistics:</h3>
<p>The <code class="language-plaintext highlighter-rouge">connectedComponentsWithStats</code> function provides statistics for each detected blob:</p>
<ul>
  <li><strong>Leftmost coordinate</strong>: The x-coordinate of the top-left corner of the bounding box.</li>
  <li><strong>Topmost coordinate</strong>: The y-coordinate of the top-left corner of the bounding box.</li>
  <li><strong>Width</strong>: Width of the bounding box.</li>
  <li><strong>Height</strong>: Height of the bounding box.</li>
  <li><strong>Area</strong>: Total number of pixels in the blob.</li>
</ul>

<p>From the displayed statistics, we can analyze the size and position of each blob. This information can be crucial for further processing, such as feature extraction or classification tasks.</p>

<hr />
<h3 id="before-diving-into-the-main-steps-lets-understand-the-utility-functions-that-will-be-used-throughout-the-process">Before diving into the main steps, let’s understand the utility functions that will be used throughout the process.</h3>
<h3 id="function-display_imageimg-title">Function: <code class="language-plaintext highlighter-rouge">display_image(img, title="")</code></h3>
<p>This function displays an image using the <code class="language-plaintext highlighter-rouge">matplotlib</code> library.</p>
<ul>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">img</code>: The image to be displayed.</li>
      <li><code class="language-plaintext highlighter-rouge">title</code> (optional): A title for the image display.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>The function creates a figure with a specified size.</li>
      <li>It then displays the image in grayscale format.</li>
      <li>The title is set, and the axis is turned off for better visualization.
        <h3 id="function-imshow_componentslabels">Function: <code class="language-plaintext highlighter-rouge">imshow_components(labels)</code></h3>
        <p>This function visualizes the connected components (or blobs) in an image with different colors.</p>
      </li>
    </ul>
  </li>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">labels</code>: The labeled image obtained from the connected component analysis.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>Maps each label to a unique hue.</li>
      <li>Merges it with blank channels to create an HSV image.</li>
      <li>Converts the HSV image to BGR format for visualization.</li>
      <li>Sets the background label (usually 0) t
        <h3 id="function-extract_featureslabels_im-stats">Function: <code class="language-plaintext highlighter-rouge">extract_features(labels_im, stats)</code></h3>
        <p>This function extracts specific features from each detected blob or character in the image.</p>
      </li>
    </ul>
  </li>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">labels_im</code>: The labeled image from the connected component analysis.</li>
      <li><code class="language-plaintext highlighter-rouge">stats</code>: Statistics of each blob, usually obtained alongside the labeled image.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>For each blob, the function computes:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">Area</code>: Total number of foreground pixels.</li>
          <li><code class="language-plaintext highlighter-rouge">Height</code>: Height of the blob’s bounding box.</li>
          <li><code class="language-plaintext highlighter-rouge">Width</code>: Width of the blob’s bounding box.</li>
          <li><code class="language-plaintext highlighter-rouge">Fraction of Foreground Pixels</code>: Ratio of the area to the bounding box’s area.</li>
          <li><code class="language-plaintext highlighter-rouge">X Distribution</code>: Distribution of foreground pixels along the x-axis.</li>
          <li><code class="language-plaintext highlighter-rouge">Y Distribution</code>: Distribution of foreground pixels along the y-axis.</li>
        </ul>
      </li>
      <li>The function returns a list of dictionaries, where each dictionary contains the features for a specific blob.
o black.</li>
    </ul>
  </li>
</ul>

<h2 id="image-loading-and-preprocessing">Image Loading and Preprocessing</h2>

<p>In this section, we load an image and convert it to grayscale, and apply different thresholding techniques to binarize the image.</p>

<h3 id="step-by-step-explanation-1">Step-by-Step Explanation:</h3>

<ol>
  <li><strong>Loading Image Files</strong>:
 We first fetch all the image filenames with <code class="language-plaintext highlighter-rouge">.png</code> extension from the specified directory and then select one of the images for further processing.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">images</span><span class="sh">"</span>  <span class="c1"># Replace with your path
</span> <span class="n">filenames</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="sh">'</span><span class="s">*.png</span><span class="sh">'</span><span class="p">))</span>
 <span class="n">filename</span> <span class="o">=</span> <span class="n">filenames</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
 <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">IMREAD_COLOR</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Displaying the Original Image</strong>:
 We use the <code class="language-plaintext highlighter-rouge">display_image</code> function to visualize the original image.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nf">display_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="sh">"</span><span class="s">Original Image</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Grayscale Conversion</strong>:
 The color image is converted to grayscale. This is done to simplify the image and to prepare it for thresholding.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Thresholding</strong>:
 Thresholding is a technique to segment an image by setting a pixel to a foreground value if it’s above a certain threshold and to a background value if it’s below that threshold.</p>

    <ul>
      <li><strong>Basic Binary Thresholding</strong>:
  Pixels with intensity above 127 are set to 255 (white), and those below are set to 0 (black).
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">_</span><span class="p">,</span> <span class="n">th1</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">threshold</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>
  <span class="nf">display_image</span><span class="p">(</span><span class="n">th1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Basic Binary Thresholding</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Otsu’s Thresholding</strong>:
  Otsu’s method calculates an “optimal” threshold by maximizing the variance between two classes of pixels (foreground and background). It’s more adaptive than basic thresholding.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">_</span><span class="p">,</span> <span class="n">th2</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">threshold</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span> <span class="o">+</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
  <span class="nf">display_image</span><span class="p">(</span><span class="n">th2</span><span class="p">,</span> <span class="sh">"</span><span class="s">Otsu</span><span class="sh">'</span><span class="s">s Thresholding</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Gaussian Blur + Otsu’s Thresholding</strong>:
  Before applying Otsu’s thresholding, we smooth the image using a Gaussian blur. This helps in removing noise and can lead to better thresholding results.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">GaussianBlur</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">th3</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">threshold</span><span class="p">(</span><span class="n">blur</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span> <span class="o">+</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
  <span class="nf">display_image</span><span class="p">(</span><span class="n">th3</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gaussian Blur + Otsu</span><span class="sh">'</span><span class="s">s Thresholding</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p>By the end of these steps, we have three binarized versions of the original image using different thresholding techniques. This allows us to compare and choose the best method for further processing.</p>

<hr />
<h2 id="connected-component-analysis-and-feature-extraction">Connected Component Analysis and Feature Extraction</h2>

<p>In this section, we perform connected component analysis on the thresholded image to identify and label individual blobs (connected components). After labeling, we will extract specific features for each blob and compare them.</p>

<h3 id="step-by-step-explanation-2">Step-by-Step Explanation:</h3>

<ol>
  <li><strong>Inverting the Image</strong>:
 The connected component function in OpenCV treats white as the foreground and black as the background. Since our image has black characters on a white background, we invert the image colors.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">th</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">bitwise_not</span><span class="p">(</span><span class="n">th3</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Connected Component Analysis</strong>:
 We perform connected component labeling on the inverted image. This function labels each connected component with a unique label.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">num_labels</span><span class="p">,</span> <span class="n">labels_im</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">centroids</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">connectedComponentsWithStats</span><span class="p">(</span><span class="n">th</span><span class="p">,</span> <span class="n">connectivity</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Visualizing the Components</strong>:
 Using the <code class="language-plaintext highlighter-rouge">imshow_components</code> function, we color each connected component differently to visualize them distinctly.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">colored_components_img</span> <span class="o">=</span> <span class="nf">imshow_components</span><span class="p">(</span><span class="n">labels_im</span><span class="p">)</span>
 <span class="nf">display_image</span><span class="p">(</span><span class="n">colored_components_img</span><span class="p">,</span> <span class="sh">"</span><span class="s">Colored Components</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Extracting Features for Each Blob</strong>:
 For each labeled component (blob), we extract specific features like area, height, width, and distributions using the <code class="language-plaintext highlighter-rouge">extract_features</code> function.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">blob_features</span> <span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">labels_im</span><span class="p">,</span> <span class="n">stats</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Comparing Features of Blobs</strong>:
 As a demonstration, we compare the features of the first two blobs.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">blob1_features</span> <span class="o">=</span> <span class="n">blob_features</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
 <span class="n">blob2_features</span> <span class="o">=</span> <span class="n">blob_features</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div>    </div>

    <ul>
      <li>Displaying features for the first blob:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Features for Blob 1:</span><span class="sh">"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">blob1_features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">X Distribution</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y Distribution</span><span class="sh">"</span><span class="p">]:</span>
          <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li>Displaying features for the second blob:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Features for Blob 2:</span><span class="sh">"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">blob2_features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">X Distribution</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y Distribution</span><span class="sh">"</span><span class="p">]:</span>
          <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Demonstrating Feature Comparison</strong>:
 As an example, we demonstrate how to compare the area of the first two blobs.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">difference_in_area</span> <span class="o">=</span> <span class="n">blob1_features</span><span class="p">[</span><span class="sh">"</span><span class="s">Area</span><span class="sh">"</span><span class="p">]</span> <span class="o">-</span> <span class="n">blob2_features</span><span class="p">[</span><span class="sh">"</span><span class="s">Area</span><span class="sh">"</span><span class="p">]</span>
 <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Difference in Area between Blob 1 and Blob 2: </span><span class="si">{</span><span class="n">difference_in_area</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>By the end of these steps, we have labeled each connected component in the image, extracted features for each blob, and demonstrated how to compare these features. This process can be useful in various image processing tasks, such as character recognition, where distinguishing between different characters based on their features is essential.</p>

<h3 id="cv2connectedcomponentswithstats-explanation"><code class="language-plaintext highlighter-rouge">cv2.connectedComponentsWithStats</code> Explanation</h3>

<h4 id="input">Input:</h4>
<ul>
  <li><strong>th</strong>: This is the binary image on which connected component labeling is performed. In our case, it’s the inverted thresholded image.</li>
  <li><strong>connectivity</strong>: This parameter determines how a pixel is connected to its neighbors. A value of <code class="language-plaintext highlighter-rouge">4</code> means a pixel is connected to its top, bottom, left, and right neighbors. A value of <code class="language-plaintext highlighter-rouge">8</code> would also include diagonal neighbors.</li>
</ul>

<h4 id="outputs">Outputs:</h4>
<ul>
  <li><strong>num_labels</strong>: This is the total number of unique labels assigned. It includes the background as one label.</li>
  <li><strong>labels_im</strong>: This is an image of the same size as the input where each pixel’s value is its label. Pixels belonging to the same connected component have the same label.</li>
  <li><strong>stats</strong>: This is a matrix where each row corresponds to a label and contains statistics related to that label. The columns represent:
    <ul>
      <li>The x-coordinate of the top-left point of the bounding box.</li>
      <li>The y-coordinate of the top-left point of the bounding box.</li>
      <li>The width of the bounding box.</li>
      <li>The height of the bounding box.</li>
      <li>The total area (in pixels) of the connected component.</li>
    </ul>
  </li>
  <li><strong>centroids</strong>: This is a matrix where each row corresponds to a label, and the columns represent the x and y coordinates of the centroid of the connurther processing.</li>
</ul>

<hr />
<h1 id="exercise-3---histogram-feature-extraction">Exercise 3 - Histogram Feature Extraction</h1>

<h3 id="objective-1">Objective:</h3>
<p>Develop a program to compute the histogram of a given input gray-scale image patch. Utilize this program to analyze the characters segmented in Exercise 2</p>

<h3 id="steps-2">Steps:</h3>

<ol>
  <li><strong>Histogram Computation</strong>:
    <ul>
      <li>Write a function or program that calculates the histogram of an input gray-scale image patch.</li>
      <li>Decide on the number of bins for the histogram. This choice will affect the resolution and the details captured by the histogram.</li>
    </ul>
  </li>
  <li><strong>Application on Exercise 2</strong>:
    <ul>
      <li>Revisit the results from Exercise 2 where individual characters were segmented using bounding boxes.</li>
      <li>For each segmented character, compute its histogram using the program developed in the first step.</li>
    </ul>
  </li>
  <li><strong>Analysis</strong>:
    <ul>
      <li>Compare the histograms of different characters. Observe the differences and similarities.</li>
      <li>Compare the histograms of characters that are identical. Note the variations, if any, and the consistencies.</li>
      <li>Discuss the effectiveness of the histogram as a feature for character differentiation.</li>
    </ul>
  </li>
  <li><strong>Resolution Dependency</strong>:
    <ul>
      <li>Analyze how the histogram feature’s effectiveness changes with the resolution (i.e., the number of bins).</li>
      <li>Does increasing the number of bins provide more discriminative power, or does it introduce noise? Conversely, does reducing the number of bins oversiplify the feature?</li>
    </ul>
  </li>
</ol>

<h3 id="deliverables">Deliverables:</h3>
<ul>
  <li>A program or function for histogram computation.</li>
  <li>Histograms of segmented characters from Exercise 2.</li>
  <li>Analysis and comments on the utility of the histogram feature for character differentiation and its dependency on resolution.</li>
</ul>

<hr />
<h2 id="utility-functions-explanation">Utility Functions Explanation</h2>

<h3 id="function-compute_histogramimage-bins256">Function: <code class="language-plaintext highlighter-rouge">compute_histogram(image, bins=256)</code></h3>
<p>This function calculates the histogram of a given grayscale image.</p>
<ul>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">image</code>: The input grayscale image for which the histogram is to be computed.</li>
      <li><code class="language-plaintext highlighter-rouge">bins</code> (optional): The number of bins for the histogram. Default is set to 256, which represents each pixel intensity value from 0 to 255.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>The function uses OpenCV’s <code class="language-plaintext highlighter-rouge">calcHist</code> method to compute the histogram of the input image.</li>
      <li>It returns the histogram as a list of pixel intensities.</li>
    </ul>
  </li>
</ul>

<h3 id="function-display_histogramhist-titlehistogram">Function: <code class="language-plaintext highlighter-rouge">display_histogram(hist, title="Histogram")</code></h3>
<p>This function visualizes the computed histogram using the <code class="language-plaintext highlighter-rouge">matplotlib</code> library.</p>
<ul>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">hist</code>: The histogram values that need to be plotted.</li>
      <li><code class="language-plaintext highlighter-rouge">title</code> (optional): The title for the histogram plot. Default is set to “Histogram”.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>The function creates a figure with a specified size.</li>
      <li>It then plots the histogram values with pixel values on the x-axis and their frequencies on the y-axis.</li>
      <li>A grid is added for better visualization.</li>
    </ul>
  </li>
</ul>

<h3 id="function-display_imageimg-title-1">Function: <code class="language-plaintext highlighter-rouge">display_image(img, title="")</code></h3>
<p>This function displays a grayscale image using the <code class="language-plaintext highlighter-rouge">matplotlib</code> library.</p>
<ul>
  <li><strong>Parameters</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">img</code>: The grayscale image that needs to be displayed.</li>
      <li><code class="language-plaintext highlighter-rouge">title</code> (optional): A title for the image display.</li>
    </ul>
  </li>
  <li><strong>Functionality</strong>:
    <ul>
      <li>The function creates a figure with a specified size.</li>
      <li>It then displays the image in grayscale format.</li>
      <li>The title is set, and the axis is turned on for better visualization.</li>
    </ul>
  </li>
</ul>

<hr />
<h3 id="histogram-computation-and-visualization-for-each-character">Histogram Computation and Visualization for Each Character</h3>

<p>In the provided code, we’re iterating through each detected character (or blob) in the image, extracting its bounding box, computing its histogram, and then visualizing both the character with its bounding box and its histogram.</p>

<h3 id="loop-for-k-in-range1-num_labels">Loop: <code class="language-plaintext highlighter-rouge">for k in range(1, num_labels)</code></h3>
<p>This loop iterates over each detected character. We start from 1 because the label 0 is reserved for the background.</p>

<ul>
  <li>
    <p><strong>Inside the Loop</strong>:</p>

    <ul>
      <li><strong>Bounding Box Extraction</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">x, y, w, h, area = stats[k]</code>: For each character, we extract its bounding box’s top-left coordinates <code class="language-plaintext highlighter-rouge">(x, y)</code>, its width <code class="language-plaintext highlighter-rouge">w</code>, height <code class="language-plaintext highlighter-rouge">h</code>, and the total area <code class="language-plaintext highlighter-rouge">area</code> from the <code class="language-plaintext highlighter-rouge">stats</code> array.</li>
          <li><code class="language-plaintext highlighter-rouge">character_patch = gray[y:y+h, x:x+w]</code>: Using the bounding box coordinates and dimensions, we extract the character’s region from the grayscale image.</li>
        </ul>
      </li>
      <li><strong>Histogram Computation</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">hist = compute_histogram(character_patch, bins=32)</code>: We compute the histogram of the extracted character patch using 256 bins. The number of bins can be adjusted based on the desired resolution.</li>
        </ul>
      </li>
      <li><strong>Histogram Visualization</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">display_histogram(hist, title=f"Histogram for Character {k}")</code>: We visualize the computed histogram using the <code class="language-plaintext highlighter-rouge">display_histogram</code> function.</li>
        </ul>
      </li>
      <li><strong>Character Visualization with Bounding Box</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">output = img.copy()</code>: We create a copy of the original image to draw the bounding box.</li>
          <li><code class="language-plaintext highlighter-rouge">cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 3)</code>: We draw a green bounding box around the detected character.</li>
          <li><code class="language-plaintext highlighter-rouge">display_image(output, title=f"Character {k} with Bounding Box")</code>: We display the character with its bounding box using the <code class="language-plaintext highlighter-rouge">display_image</code> function.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="post-loop">Post Loop:</h3>
<p>After processing all characters, you can compare the histograms of different characters using various methods like correlation, chi-square, etc. This step is essential to understand the similarity or difference between characters based on their histograms.</p>

<hr />

<hr />
<h3 id="comparing-histograms-of-characters">Comparing Histograms of Characters:</h3>

<p>When we analyze the histograms of characters, several observations can be made:</p>

<ol>
  <li><strong>Same Characters</strong>:
    <ul>
      <li>Histograms of the same characters tend to be very similar. This is because the distribution of pixel intensities for the same character will closely match.</li>
      <li>When comparing the histograms of the same characters, the correlation value will be close to 1, indicating a high degree of similarity.</li>
    </ul>
  </li>
  <li><strong>Different Characters</strong>:
    <ul>
      <li>For different characters, the histograms might vary significantly. This is especially true if the characters have distinct shapes or structures.</li>
      <li>The correlation between the histograms of different characters will be lower. In some cases, if the intensity distributions are contrasting sharply, the correlation might even be negative.</li>
    </ul>
  </li>
</ol>

<h3 id="insights-on-the-histogram-feature">Insights on the Histogram Feature:</h3>

<p>The histogram, which represents the distribution of pixel intensities in an image, provides both advantages and limitations when used for character recognition:</p>

<ol>
  <li><strong>Advantages</strong>:
    <ul>
      <li><strong>Simplicity</strong>: Histograms are straightforward to compute and understand.</li>
      <li><strong>Robustness</strong>: They can be robust against minor variations or noise in the image.</li>
      <li><strong>Profile Capture</strong>: Histograms can capture the general profile of a character, such as whether it’s generally bright or dark, which can be useful in differentiating certain characters.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li><strong>Loss of Spatial Information</strong>: While histograms capture the intensity distribution, they lose the spatial arrangement of these intensities. This means two very different characters might have the same histogram if they have a similar count of dark and light pixels.</li>
      <li><strong>Discrimination</strong>: For characters that have similar pixel intensity distributions, histograms might not be able to distinguish them effectively.</li>
    </ul>
  </li>
</ol>

<h3 id="resolution-of-the-histogram">Resolution of the Histogram:</h3>

<p>The number of bins in the histogram, which determines its resolution, plays a crucial role in its effectiveness:</p>

<ol>
  <li><strong>High Resolution (Many Bins)</strong>:
    <ul>
      <li>Can capture intricate details of the pixel intensity distribution.</li>
      <li>Might be overly sensitive to minor variations or noise in the image.</li>
      <li>Requires more memory and might be slower when used for comparisons.</li>
    </ul>
  </li>
  <li><strong>Low Resolution (Fewer Bins)</strong>:
    <ul>
      <li>Provides a more generalized view of the pixel intensity distribution, which can make it more robust against noise.</li>
      <li>However, it might miss out on capturing subtle differences between characters.</li>
      <li>Computationally more efficient due to fewer bins.</li>
    </ul>
  </li>
</ol>

<p><strong>In Conclusion</strong>:
The resolution of the histogram is a crucial parameter. A very high-resolution histogram might be too sensitive to noise, while a very low-resolution histogram might miss out on important character details. It’s always a good idea to experiment with different resolutions to find the one that works best for the specific dataset and task at hand.</p>
</section>
            </div><div class="search-results">
    <div class="has-results">
        <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
        <ul class="search-results-list"></ul>
    </div>
    <div class="no-results">
        <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
    </div>
</div></div>
    </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>

<!-- introduce js_includes_ex support -->

<script>

    // Libraries to add dynamic tags below using .html files
    
    
    window.sciptsLib = new Map();
    window.cssLib = new Map();
    window.scipts = new Map();
    window.links = new Map();
    var jsIdIdx = 0;
    var cssIdIdx = 0;
    
    window.sciptsLib.set('bootstrap', [
        {uri: '//cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js'},
        {uri: '//cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js'}
    ]);
    window.cssLib.set('bootstrap', {uri: '//cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css'});
    
    window.sciptsLib.set('prism', {uri: '/ds/aml/assets/libs/prism/prism.js', defer: false});
    window.cssLib.set('prism', {uri: '/ds/aml/assets/libs/prism/prism.css', defer: false});

    //window.sciptsLib.set('xmind', {uri: '/ds/aml/assets/xmind/umd/xmind-embed-viewer.js', defer: false});
    
    // animate is included in head.html to support logo animation in navbar
    // window.cssLib.set('animate', {uri: '//cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css'});
    
    // Add the script tags before the following element
    var getbookStyleElement = document.getElementById("gitbook_style")

    // Adding script tags to externally link js
        keyName = `prism_js`

        if (!window.scipts.get(keyName)) {

            // Get the links from sciptsLib
            jsLinks = window.sciptsLib.get( 'prism');

            if (jsLinks instanceof Array) {
            
                let scriptTags = []            
                
                for (i in jsLinks) {
                    scriptTag = document.createElement("script")
                    scriptTag.src = jsLinks[i]['uri'];                    
                    scriptTag.defer = ('defer' in jsLinks[i]) ? jsLinks[i]['defer'] : true;
                    scriptTag.id =  `prism_js_${jsIdIdx++}`;
                    document.head.insertBefore(scriptTag, getbookStyleElement)
                    
                    scriptTags.push(scriptTag);
                    console.log(scriptTag)
                }

                // Add script tags to window.scripts map object
                window.scipts.set(keyName, scriptTags)
            
            } else if (window.sciptsLib.has( 'prism')) {

                scriptTag = document.createElement("script");
                // console.log("sadasd", prism, jsLinks);
                scriptTag.src = jsLinks['uri'];
                scriptTag.defer = ('defer' in jsLinks) ? jsLinks['defer'] : true;
                scriptTag.id =  `prism_js_${jsIdIdx++}`;
                document.head.insertBefore(scriptTag, getbookStyleElement)
                console.log(scriptTag)

                // Add script tag to window.scripts map object
                window.scipts.set(keyName, scriptTag);
            }
        }


        // Adding link tags to externally link css
        keyName = `prism_css`

        if (!window.scipts.get(keyName)) {

            // Get the links from cssLib
            cssLinks = window.cssLib.get( 'prism');            
            
            if (cssLinks instanceof Array) {
            
                let cssLinkTags = []            
                
                for (i in cssLinks) {
                    cssLinkTag = document.createElement("script")
                    cssLinkTag.href = cssLinks[i]['uri'];
                    cssLinkTag.defer = ('defer' in cssLinks[i]) ? cssLinks[i]['defer'] : true;
                    cssLinkTag.id =  `prism_css_${cssIdIdx++}`;
                    cssLinkTag.rel = 'stylesheet';
                    document.head.insertBefore(cssLinkTag, getbookStyleElement)
                    
                    cssLinkTags.push(cssLinkTag);
                    console.log(cssLinkTag)
                }

                // Add script tags to window.scripts map object
                window.scipts.set(keyName, cssLinkTags)
            
            } else if (window.cssLib.has( 'prism')) {

                cssLinkTag = document.createElement("link")
                cssLinkTag.href = cssLinks['uri'];
                cssLinkTag.defer = ('defer' in cssLinks) ? cssLinks['defer'] : true;
                cssLinkTag.id =  `prism_css_${cssIdIdx++}`;
                cssLinkTag.rel = 'stylesheet';
                document.head.insertBefore(cssLinkTag, getbookStyleElement)

                console.log(cssLinkTag)

                // Add script tag to window.scripts map object
                window.scipts.set(keyName, cssLinkTag);
            }
        }    
        
    // Adding script tags to externally link js
        keyName = `mathjax_js`

        if (!window.scipts.get(keyName)) {

            // Get the links from sciptsLib
            jsLinks = window.sciptsLib.get( 'mathjax');

            if (jsLinks instanceof Array) {
            
                let scriptTags = []            
                
                for (i in jsLinks) {
                    scriptTag = document.createElement("script")
                    scriptTag.src = jsLinks[i]['uri'];                    
                    scriptTag.defer = ('defer' in jsLinks[i]) ? jsLinks[i]['defer'] : true;
                    scriptTag.id =  `mathjax_js_${jsIdIdx++}`;
                    document.head.insertBefore(scriptTag, getbookStyleElement)
                    
                    scriptTags.push(scriptTag);
                    console.log(scriptTag)
                }

                // Add script tags to window.scripts map object
                window.scipts.set(keyName, scriptTags)
            
            } else if (window.sciptsLib.has( 'mathjax')) {

                scriptTag = document.createElement("script");
                // console.log("sadasd", mathjax, jsLinks);
                scriptTag.src = jsLinks['uri'];
                scriptTag.defer = ('defer' in jsLinks) ? jsLinks['defer'] : true;
                scriptTag.id =  `mathjax_js_${jsIdIdx++}`;
                document.head.insertBefore(scriptTag, getbookStyleElement)
                console.log(scriptTag)

                // Add script tag to window.scripts map object
                window.scipts.set(keyName, scriptTag);
            }
        }


        // Adding link tags to externally link css
        keyName = `mathjax_css`

        if (!window.scipts.get(keyName)) {

            // Get the links from cssLib
            cssLinks = window.cssLib.get( 'mathjax');            
            
            if (cssLinks instanceof Array) {
            
                let cssLinkTags = []            
                
                for (i in cssLinks) {
                    cssLinkTag = document.createElement("script")
                    cssLinkTag.href = cssLinks[i]['uri'];
                    cssLinkTag.defer = ('defer' in cssLinks[i]) ? cssLinks[i]['defer'] : true;
                    cssLinkTag.id =  `mathjax_css_${cssIdIdx++}`;
                    cssLinkTag.rel = 'stylesheet';
                    document.head.insertBefore(cssLinkTag, getbookStyleElement)
                    
                    cssLinkTags.push(cssLinkTag);
                    console.log(cssLinkTag)
                }

                // Add script tags to window.scripts map object
                window.scipts.set(keyName, cssLinkTags)
            
            } else if (window.cssLib.has( 'mathjax')) {

                cssLinkTag = document.createElement("link")
                cssLinkTag.href = cssLinks['uri'];
                cssLinkTag.defer = ('defer' in cssLinks) ? cssLinks['defer'] : true;
                cssLinkTag.id =  `mathjax_css_${cssIdIdx++}`;
                cssLinkTag.rel = 'stylesheet';
                document.head.insertBefore(cssLinkTag, getbookStyleElement)

                console.log(cssLinkTag)

                // Add script tag to window.scripts map object
                window.scipts.set(keyName, cssLinkTag);
            }
        }    
        
    

</script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script
  type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>





<!-- introduce mathjax support -->
<script>
    function fixes_chrome_anchors() {
        let chrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor);
        if (window.location.hash && chrome) {
            setTimeout(function () {
                var hash = window.location.hash;
                window.location.hash = "";
                window.location.hash = hash;
            }, 300);
        }
    }

    if (document.readyState === "loading") {
        // Loading hasn't finished yet
        document.addEventListener("DOMContentLoaded", fixes_chrome_anchors);
    } else {
        // `DOMContentLoaded` has already fired
        fixes_chrome_anchors();
    }
</script>


                        <a href="/ds/aml/image_analysis/1_ima_5_region_detectors/" class="navigation navigation-prev navigation-unique hide_element" aria-label="Previous page: 5. Region Detectors"> <!-- class="hide_element" -->
                            <i class="fa fa-angle-left"></i>
                        </a>
                    

                    
                        <a href="/ds/aml/basics/1_regression_1_cv/" class="navigation navigation-next navigation-unique" aria-label="Next page: 1. Computer Vision"> <!-- class="hide_element" -->
                            <i class="fa fa-angle-right"></i>
                        </a>
                    
                    <div class="copyright"><span>&copy; 2024 Intelligent Mind Labs, Inc. All rights reserved.</span>.</div>
                </div>
            </div>

            <script>
                // div "book" element's content is the only content refreshed when nav link is pressed.
                // This functionality is implemented in "gitbook.js" and "theme.js"
                // head.html and footer.html will not be called again and again when a new nav link is clicked
                // Gitbook js is optimized to dynamically change content inside div "book" element.
                // Due to this: script put in the footer will only be called once,
                // which is the very first time the page is loaded by entering
                // the web address in the address bar in the browser
                // 
                $ = jQuery;
                $(function() {
                    // spinnerHide();
                    // setTimeout(spinnerHide, 200);
           
                    $('.childBtn').on('click', function(event) {
                        event.stopImmediatePropagation(); // To prevent following the link (optional)
                        window.location = $(this).attr('link');
                    });

                    $('.parentBtn').on('click', function(event) {
                        window.location = $(this).attr('link');
                    });
                });
            </script>

            <script>
            var gitbook = gitbook || [];
            gitbook.push(function() {
                gitbook.page.hasChanged({
    "page": {
        "title": "Introduction",
        "level": "1.1",
        "depth": 1,
        
        "next": {
            "title": "1. Computer Vision",
            "level": "1.2",
            "depth": 1,
            "path": "_4_problem_domains/1_regression_1_cv.md",
            "ref": "_4_problem_domains/1_regression_1_cv.md",
            "articles": []
        },
        
        "dir": "ltr"
    },    "config": {
        "plugins": ["fontsettings", "highlight", "livereload", "lunr", "search", "sharing", "theme-default", "livereload"],
        "styles": {
            "ebook": "styles/ebook.css",
            "epub": "styles/epub.css",
            "mobi": "styles/mobi.css",
            "pdf": "styles/pdf.css",
            "print": "styles/print.css",
            "website": "styles/website.css"
        },
        "pluginsConfig": {
            "expandable-chapter-small2": {
                "articlesExpand": true,
            },
            "fontsettings": {
                "family": "sans",
                "size": 2,
                "theme": "white"
            },
            "highlight": {},
            "livereload": {},
            "lunr": {
                "ignoreSpecialCharacters": false,
                "maxIndexSize": 1000000
            },
            "search": {},            "sharing": {
                "facebook": true,

                "google": false,

                "github": true,
              
                "github_link": "https://github.com",
              

                "telegram": false,
                "telegram_link": "https://t.me",

                "instapaper": false,

                "twitter": true,
              

                "vk": false,

                "weibo": false,

                "all": ["facebook", "google", "twitter", "weibo", "instapaper", "github", "telegram"]
            },
"theme-default": {
                "showLevel": false,
                "styles": {
                    "ebook": "styles/ebook.css",
                    "epub": "styles/epub.css",
                    "mobi": "styles/mobi.css",
                    "pdf": "styles/pdf.css",
                    "print": "styles/print.css",
                    "website": "styles/website.css"
                }
            },
        },
        "theme": "default",
        "author": "Intelligent Mind Labs",
        "pdf": {
            "pageNumbers": true,
            "fontSize": 12,
            "fontFamily": "Arial",
            "paperSize": "a4",
            "chapterMark": "pagebreak",
            "pageBreaksBefore": "/",
            "margin": {
                "right": 62,
                "left": 62,
                "top": 56,
                "bottom": 56
            }
        },
        "structure": {
            "langs": "LANGS.md",
            "readme": "README.md",
        },
        "variables": {},
        "title": "Wiki | Advance Machine Learning",
        "language": "en",
        "gitbook": "*"
    },
    "file": {
        "path": "_4_problem_domains/1_ima_6_keypoints.md",
        "mtime": "2019-04-27 00:00:00 +0800",
        "type": "markdown"
    },
    "gitbook": {
        "version": "3.2.3",
        "time": "2024-08-20 15:29:14 +0800"
    },
    "basePath": "/ds/aml",
    "book": {
        "language": ""
    }
});
            });

            </script>
        </div><script src="/ds/aml/assets/gitbook/gitbook.js"></script>
<script src="/ds/aml/assets/gitbook/theme.js"></script>

<script src="/ds/aml/assets/gitbook/gitbook-plugin-back-to-top-button/plugin.js" async=""></script>
<!-- <script src="/ds/aml/assets/gitbook/gitbook-plugin-copy-code-button/toggle.js"></script> -->
<script src="/ds/aml/assets/gitbook/gitbook-plugin-expandable-chapters-small2/expandable-chapters-small.js"></script>
<script src="/ds/aml/assets/gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="/ds/aml/assets/gitbook/gitbook-plugin-search-pro/jquery.mark.min.js" defer=""></script>
<script src="/ds/aml/assets/gitbook/gitbook-plugin-search-pro/search.js" defer=""></script>
<script src="/ds/aml/assets/gitbook/gitbook-plugin-sharing/buttons.js" defer=""></script>
<script src="/ds/aml/assets/gitbook/gitbook-plugin-splitter/splitter.js" defer=""></script>




<!--
<script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
<script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
<script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
<script src="../gitbook/gitbook-plugin-search/search.js"></script>
-->



<script>
function spinnerHide() {
    $(".spinner-container").removeClass("show_element");
    $(".spinner-container").addClass("hide_element");
    $(".spinner-container").addClass("display_none");
    

    $("#book-search-results").removeClass("hide_element");
    $("#book-search-results").addClass("show_element");

    $(".navigation.navigation-next").removeClass("hide_element");
    $(".navigation.navigation-next").addClass("show_element");

    $(".navigation.navigation-prev").removeClass("hide_element");
    $(".navigation.navigation-prev").addClass("show_element");
}        
</script><!-- trackers --><!-- google analytics --><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V3WBTFTQWG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V3WBTFTQWG');
</script>
</body>
</html>